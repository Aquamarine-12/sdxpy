\documentclass{scrbook}

% ----------------------------------------
% Character encodings.
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% ----------------------------------------
% Bibliography.
\usepackage[backend=biber,style=alphabetic,sorting=nyt,maxbibnames=99]{biblatex}
\addbibresource{bibliography.bib}

% ----------------------------------------
% Callout boxes.
\usepackage{tcolorbox}
\newtcolorbox{callout}{colback=black!5!white,colframe=black!75!white}

% ----------------------------------------
% Cross-references.
\newcommand{\appref}[1]{Appendix~\ref{#1}}
\newcommand{\chapref}[1]{Chapter~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\tblref}[1]{Table~\ref{#1}}

% ----------------------------------------
% Figures
\newcommand{\figpdf}[4]{\begin{figure}%
\centering%
\includegraphics[scale={#4}]{#2}%
\caption{#3}%
\label{#1}%
\end{figure}}

% ----------------------------------------
% Glossary items and references.
\newcommand{\glossref}[1]{\textbf{#1}}
\newcommand{\glosskey}[1]{\textbf{#1}}

% ----------------------------------------
% Index.
\usepackage{makeidx}
\makeindex

% ----------------------------------------
% Format code listings.
% https://tex.stackexchange.com/questions/263032/why-is-listings-frame-width-a-little-larger-then-textwidth
% https://tex.stackexchange.com/questions/378600/unicode-characters-in-lstlisting
\usepackage{listings}
\lstset{
  basicstyle=\footnotesize\ttfamily,
  upquote=true,
  xleftmargin=3.4pt,
  xrightmargin=3.4pt,
  literate={✓}{{\checkmark}}1 {ô}{{\^o}}1 {ü}{{\"u}}1 {…}{{\ldots}}1
}

% Failed to find package that defines
% {├}{{\bdbvrh}}1
% {└}{{\bduvrh}}1
% for 'tree' output

% ----------------------------------------
% URLs as footnotes.
% Always load 'hyperref' last (see link below for explanation).
% https://tex.stackexchange.com/questions/16268/warning-with-footnotes-namehfootnote-xx-has-been-referenced-but-does-not-exi
\newcommand{\hreffoot}[2]{{#1}\footnote{\href{#2}{#2}}}
\usepackage[hidelinks]{hyperref}

% ----------------------------------------
% Start of document.
\begin{document}
\include{config.tex}
\date{}
\maketitle
\frontmatter
\tableofcontents
\mainmatter

\chapter{Chapter 1: Introduction}\label{introduction}


The best way to learn design is to study examples,
and some of the best examples of software design come from
the tools programmers use in their own work.
These lessons build small versions of file backup systems,
testing frameworks,
and regular expression matchers
both to demystify them
and to give some insights into how experienced programmers think.

\begin{callout}


We shape our tools, and thereafter our tools shape us.


— Marshall McLuhan

\end{callout}

\section{Audience}\label{introduction-audience}


These \hreffoot{personas}{https://teachtogether.tech/en/index.html\#s:process-personas} describe who this book is meant to help \cite{Wilson2019}:

\begin{itemize}

\item 

Aïsha started writing VB macros for Excel in an accounting course and never looked back.
    After spending three years fixing her company's \hreffoot{Django}{https://www.djangoproject.com/} website
    she wants to learn how to build back-end applications properly.
    This material will fill in some gaps in her programming knowledge
    and teach her some common design patterns.



\item 

Rupinder is studying computer science at college.
    He has learned a lot about the theory of algorithms,
    and while he uses Git and unit testing tools in his assignments,
    he doesn't feel he understands how they work.
    This material will give him a better understanding of those tools
    and of how to design new ones.



\item 

Yim builds mobile apps for a living
    but also teaches two college courses.
    They are frustrated that so many books talk about algorithms but not about design
    and use examples that their students can't relate to.
    This material will fill those gaps
    and give them starting points for a wide variety of course assignments.



\end{itemize}


Like these three personas, readers should be able to:

\begin{itemize}

\item 

Write Python programs using loops, arrays, functions, and classes.



\item 

Create static web pages using HTML and CSS.



\item 

Use \hreffoot{Git}{https://git-scm.com/} to save and share files.
    (It's OK not to know \hreffoot{the more obscure commands}{https://git-man-page-generator.lokaltog.net/}.)



\item 

Explain what a tree is and how to process one recursively.
    (This is the most complicated data structure and algorithm we \emph{don't} explain.)



\end{itemize}


This book can be read on its own or used as a classroom resource.
If you are looking for a project to do in a software design course,
adding a tool to those covered here would be fun as well as educational.
Please see \chapref{conclusion} for more details.

\section{Topics}\label{introduction-contents}


Programmers have invented \hreffoot{a lot of tools}{https://en.wikipedia.org/wiki/Programming\_tool} to make their lives easier.
This volume focuses on a few that individual developers use while writing software;
we hope future volumes
will explore those used in the applications that programmers build.


\appref{glossary} defines the terms we introduce in these lessons,
which in turn define their scope:

\begin{itemize}

\item 

How to process a program like any other piece of text.



\item 

How to turn a program into a data structure that can be analyzed and modified.



\item 

What design patterns are and which ones are used most often.



\item 

How programs are executed and how we can control and inspect their execution.



\item 

How we can analyze programs' performance in order to make sensible design tradeoffs.



\item 

How to find and run code modules on the fly.



\end{itemize}

\section{Layout}\label{introduction-layout}


We display Python source code like this:


\begin{lstlisting}[frame=single,frameround=tttt]
for thing in collection:
    print(thing)
\end{lstlisting}



\noindent and Unix shell commands like this:


\begin{lstlisting}[frame=single,frameround=tttt]
for filename in *.dat
do
    cut -d , -f 10 $filename
done
\end{lstlisting}



\noindent Data files and program output are shown in italics:


\begin{lstlisting}[frame=single,frameround=tttt]
Package,Releases
0,1
0-0,0
0-0-1,1
00print-lol,2
00smalinux,0
01changer,0
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
- name: read
  params:
  - sample_data.csv
\end{lstlisting}



We occasionally wrap lines in source code in unnatural ways to make listings fit the printed page,
and sometimes use \texttt{...} to show where lines have been omitted.
Where we need to break lines of output for the same reason,
we end all but the last line with a single backslash \texttt{{\textbackslash}}.
The full listings are all available in \hreffoot{our Git repository}{https://github.com/gvwilson/sdpy}
and \hreffoot{on our website}{https://gvwilson.github.io/sdpy}.


Finally,
we write functions as \texttt{function\_name} rather than \texttt{function\_name()};
the latter is more common,
but people don't use \texttt{array\_name[]} for arrays,
and the empty parentheses makes it hard to tell
whether we're talking about "the function itself" or "a call to the function with no parameters".

\section{Contributing}\label{introduction-use}


All of the written material in this book
is made available under the \hreffoot{Creative Commons - Attribution - NonCommercial 4.0 International license}{https://creativecommons.org/licenses/by-nc/4.0/}
(CC-BY-NC-4.0),
while the software is made available under the \hreffoot{Hippocratic License}{https://firstdonoharm.dev/}.
The first allows you to use and remix this material for non-commercial purposes,
as-is or in adapted form,
provided you cite its original source.
The second allows you to use and remix the software on this site
provided you do not violate international agreements governing human rights.
Please see \appref{license} for details.


If you would like to improve what we have,
add new material,
or have questions,
please file an issue in our GitHub repository or send us email.
Please note that all contributors are required to abide by our Code of Conduct
(\appref{conduct}).

\section{Acknowledgments}\label{introduction-acknowledgments}


This book was inspired by
classics like \cite{Kernighan1979,Kernighan1981,Kernighan1983,Kernighan1988,Petre2016},
and by:

\begin{itemize}

\item the entries in the \hreffoot{\emph{Architecture of Open Source Applications}}{https://aosabook.org/} series \cite{Brown2011,Brown2012,Armstrong2013,Brown2016};

\item \hreffoot{Mary Rose Cook}{https://maryrosecook.com/}'s \hreffoot{Gitlet}{http://gitlet.maryrosecook.com/};

\item \hreffoot{Matt Brubeck}{https://limpet.net/mbrubeck/}'s \hreffoot{browser engine tutorial}{https://limpet.net/mbrubeck/2014/08/08/toy-layout-engine-1.html};

\item \hreffoot{Connor Stack}{https://connorstack.com/}'s \hreffoot{database tutorial}{https://cstack.github.io/db\_tutorial/};

\item \hreffoot{Maël Nison}{https://arcanis.github.io/}'s \hreffoot{package manager tutorial}{https://classic.yarnpkg.com/blog/2017/07/11/lets-dev-a-package-manager/};

\item \hreffoot{Paige Ruten}{https://viewsourcecode.org/}'s \hreffoot{kilo text editor}{https://viewsourcecode.org/snaptoken/kilo/index.html};

\item \hreffoot{Bob Nystrom}{http://journal.stuffwithstuff.com/}'s book \hreffoot{\emph{Crafting Interpreters}}{https://craftinginterpreters.com/} \cite{Nystrom2021};

\item \hreffoot{Pavel Panchekha}{https://pavpanchekha.com/} and \hreffoot{Chris Harrelson}{https://twitter.com/chrishtr}'s \hreffoot{\emph{Web Browser Engineering}}{https://browser.engineering/}

\item \hreffoot{Julia Evans}{https://jvns.ca/}' posts and \hreffoot{zines}{https://wizardzines.com/}.

\end{itemize}


I am also grateful to the creators of
\hreffoot{Black}{https://black.readthedocs.io/},
\hreffoot{flake8}{https://flake8.pycqa.org/},
\hreffoot{Glosario}{https://github.com/carpentries/glosario},
\hreffoot{GNU Make}{https://www.gnu.org/software/make/},
\hreffoot{isort}{https://pycqa.github.io/isort/},
\hreffoot{ivy}{https://www.dmulholl.com/docs/ivy/main/},
\hreffoot{LaTeX}{https://www.latex-project.org/},
\hreffoot{pip}{https://pip.pypa.io/},
\hreffoot{Python}{https://www.python.org/},
\hreffoot{SVG Screenshot}{https://chrome.google.com/webstore/detail/svg-screenshot/nfakpcpmhhilkdpphcjgnokknpbpdllg},
\hreffoot{WAVE}{https://wave.webaim.org/},
and the other open source tools used to make this material.
If we all give a little,
we all get a lot.


\chapter{Chapter 2: A Testing Framework}\label{tester}

\begin{itemize}

\item Functions are objects that can be saved in data structures or passed as arguments to other functions.

\item Python stores variables in a structure like a dictionary.

\item A unit test is a function that takes a fixture, performs an operation, and passes, fails, or produces an error.

\item Use reflection to discover functions and other values in programs at runtime.

\item Replace actual functions with mock objects temporarily to simplify testing.

\end{itemize}


\noindent 
    Terms defined:
    \glossref{actual result (of test)}, \glossref{assertion}, \glossref{context manager}, \glossref{defensive programming}, \glossref{docstring}, \glossref{error (result of test)}, \glossref{exception handler}, \glossref{expected result (of test)}, \glossref{failure (result of test)}, \glossref{fixture}, \glossref{introspection}, \glossref{mock object}, \glossref{pass (result of test)}, \glossref{throw exception}, \glossref{unit test}



We are going to write many small programs in the coming chapters
but not a lot of re-runnable tests.
That's OK for exploratory programming,
but if our software is going to be used instead of just read,
we should try to make sure it works.


A tool for writing and running \glossref{unit tests} is a good first step.
Such a tool should:

\begin{itemize}

\item find files containing tests;

\item find the tests in those files;

\item run the tests;

\item capture their results; and

\item report each test's result and a summary of those results.

\end{itemize}


Our design is inspired by \hreffoot{pytest}{https://docs.pytest.org/},
which was in turn inspired by many tools built for other languages
from the 1980s onward \cite{Meszaros2007}.

\section{Functions as Objects}\label{tester-funcobj}


FIXME

\section{Finding Functions}\label{tester-reflection}


The first thing we need to understand is how Python stores variables.
The answer is, "In a dictionary."
Run the Python interpreter and call the \texttt{globals} function:

\begin{lstlisting}[frame=single,frameround=tttt]
>>> globals()
{
    '__name__': '__main__',
    '__doc__': None,
    '__package__': None,
    '__loader__': <class '_frozen_importlib.BuiltinImporter'>,
    '__spec__': None,
    '__annotations__': {},
    '__builtins__': <module 'builtins' (built-in)>
}
\end{lstlisting}


\texttt{globals} returns a copy of the dictionary that Python uses
to store all the variables at the top (global) level of your program.
Since we just started the interpreter,
what we get are the variables that Python defines automatically:
it uses double underscores like \texttt{\_\_name\_\_} for these variables,
but they're just string keys in a dictionary.


Let's define a variable of our own:

\begin{lstlisting}[frame=single,frameround=tttt]
>>> my_variable = 123
>>> globals()
{
    '__name__': '__main__',
    '__doc__': None,
    '__package__': None,
    '__loader__': <class '_frozen_importlib.BuiltinImporter'>,
    '__spec__': None,
    '__annotations__': {},
    '__builtins__': <module 'builtins' (built-in)>,
    'my_variable': 123
}
\end{lstlisting}


There's our variable \texttt{my\_variable} and its value.


There's another function called \texttt{locals} that returns a dictionary full of
all the variables defined in the current (local) scope.
Let's create a function that takes a parameter,
creates a local variable,
and then shows what's in scope:


\begin{lstlisting}[frame=single,frameround=tttt]
def show_off(some_parameter):
    some_variable = some_parameter * 2
    print("local values", locals())

show_off("hello")
\end{lstlisting}

\begin{lstlisting}[frame=single,frameround=tttt]
local values {'some_parameter': 'hello', 'some_variable': 'hellohello'}
\end{lstlisting}



The second thing we need to understand is that
a function is just another kind of object:
while a string object holds characters
and an image object holds pixels,
a function object holds instructions.
When we write:

\begin{lstlisting}[frame=single,frameround=tttt]
def example():
    print("in example")
\end{lstlisting}


what we're actually doing is saying,
"Please create an object containing the instructions to print a string
and assign it to the variable \texttt{example}."
Here's proof:


\begin{lstlisting}[frame=single,frameround=tttt]
alias = example
alias()
\end{lstlisting}

\begin{lstlisting}[frame=single,frameround=tttt]
in example
\end{lstlisting}



We can also assign to the original variable:


\begin{lstlisting}[frame=single,frameround=tttt]
def replacement():
    print("in replacement")

example = replacement
example()
\end{lstlisting}

\begin{lstlisting}[frame=single,frameround=tttt]
in replacement
\end{lstlisting}



Like other objects,
functions have attributes.
We can use \texttt{dir} (short for "directory") to get a list of their names:


\begin{lstlisting}[frame=single,frameround=tttt]
def example():
    "Docstring for example."
    print("in example")

print(dir(example))
\end{lstlisting}

\begin{lstlisting}[frame=single,frameround=tttt]
[
    '__annotations__', '__builtins__', '__call__', '__class__'
    '__closure__', '__code__', '__defaults__', '__delattr__'
    '__dict__', '__dir__', '__doc__', '__eq__', '__format__' '__ge__',
    '__get__', '__getattribute__', '__globals__' '__gt__', '__hash__',
    '__init__', '__init_subclass__' '__kwdefaults__', '__le__',
    '__lt__', '__module__', '__name__' '__ne__', '__new__',
    '__qualname__', '__reduce__', '__reduce_ex__' '__repr__',
    '__setattr__', '__sizeof__', '__str__', '__subclasshook__'
]
\end{lstlisting}



I don't know what all of these do,
but \texttt{\_\_doc\_\_} holds the documentation string (docstring) for the function
and \texttt{\_\_name\_\_} holds its original name:


\begin{lstlisting}[frame=single,frameround=tttt]
print("docstring:", example.__doc__)
print("name:", example.__name__)
\end{lstlisting}

\begin{lstlisting}[frame=single,frameround=tttt]
docstring: Docstring for example.
name: example
\end{lstlisting}



If a program's variables are stored in a dictionary,
we can iterate over them.
Let's do that to find all the functions whose names start with \texttt{test\_}:


\begin{lstlisting}[frame=single,frameround=tttt]
def test_addition():
    assert 2 + 2 == 4

def test_multiplication():
    assert 3 * 3 == 9

def test_remainder():
    assert 15 % 4 == 0 # this is wrong

def find_tests():
    result = []
    for (name, func) in globals().items():
        if name.startswith("test_"):
            result.append(func)
    return result

print("all the test functions", find_tests())
\end{lstlisting}

\begin{lstlisting}[frame=single,frameround=tttt]
all the test functions [
    <function test_addition at 0x1008d7d90>,
    <function test_multiplication at 0x1009fb010>,
    <function test_remainder at 0x1009fb0a0>
]
\end{lstlisting}



Remember, a function is just another kind of object in Python:
when we print the function out,
Python shows us its name and its address in memory.
If we have a function we can call it,
which means we can find all the \texttt{test\_} functions in this file
and call them one by one.


We can do more than just call functions:
we can check if they run to completion or raise an exception
and report that:


\begin{lstlisting}[frame=single,frameround=tttt]
def test_addition():
    assert 2 + 2 == 4

def test_multiplication():
    assert 3 * 3 == 9

def test_remainder():
    assert 15 % 4 == 0 # this is wrong

def run_tests():
    for (name, func) in globals().items():
        if name.startswith("test_"):
            try:
                func()
                print(func.__name__, "passed")
            except AssertionError:
                print(func.__name__, "failed")

run_tests()
\end{lstlisting}

\begin{lstlisting}[frame=single,frameround=tttt]
test_addition passed
test_multiplication passed
test_remainder failed
\end{lstlisting}



Notice that all the \texttt{test\_} functions can be called with no arguments.
If some of them required arguments,
we'd have to know what it expected and then call it with the right number of values.
On the other hand,
if we say that test functions all have the same signature (i.e., parameter list),
we can call them interchangeably.

\section{Structuring Tests}\label{tester-structure}


As in other unit testing frameworks,
each test will be a function of zero arguments
so that the framework can run them all in the same way.
Each test will create a \glossref{fixture} to be tested
and use \glossref{assertions}
to compare the \glossref{actual result}
against the \glossref{expected result}.
The outcome can be exactly one of:

\begin{itemize}

\item 

\glossref{Pass}:
    the test works as expected.



\item 

\glossref{Fail}:
    something is wrong with the test subject.



\item 

\glossref{Error}:
    something wrong in the test itself,
    which means we don't know whether the test subject is working properly or not.



\end{itemize}


To make this work,
we need some way to distinguish failing tests from broken ones.
Our solution relies on the fact that exceptions are objects
and that a program can use \glossref{introspection}
to determine the class of an object.
If a test \glossref{throws an exception} whose class is \texttt{assert.AssertionError},
then we will assume the exception came from
one of the assertions we put in the test as a check.
Any other kind of assertion indicates that the test itself contains an error.


To start,
let's record tests and what they mean.
We don't run tests immediately
because we want to wrap each one in our own \glossref{exception handler}.
Instead,
the function \texttt{hope\_that} saves a descriptive message and a function that implements a test
in an array:


\begin{lstlisting}[frame=single,frameround=tttt]
# Tests to run.
HOPE_TESTS = []

# Record a single test for running later.
def hope_that(message, func):
    HOPE_TESTS.append([message, func])
\end{lstlisting}


\begin{callout}


\subsubsection*{Independence}


Because we're appending tests to an array,
they will be run in the order in which they are registered,
but we shouldn't rely on that.
Every unit test should work independently of every other
so that an error or failure in an early test
doesn't affect the result of a later one.

\end{callout}


The function \texttt{main} runs all registered tests:


\begin{lstlisting}[frame=single,frameround=tttt]
# Run all of the tests that have been asked for and report summary.
def main():
    results = {
        "pass": 0,
        "fail": 0,
        "error": 0
    }
    for [message, test] in HOPE_TESTS:
        try:
            test()
            results["pass"] += 1
        except AssertionError as exc:
            results["fail"] += 1
        except Exception as exc:
            results["error"] += 1
    print(f"pass {results['pass']}")
    print(f"fail {results['fail']}")
    print(f"error {results['error']}")
\end{lstlisting}



\noindent If a test completes without an exception, it passes.
If any of the \texttt{assert} calls inside the test raises an \texttt{AssertionError},
the test fails,
and if it raises any other exception,
it's an error.
After all tests are run,
\texttt{main} reports the number of results of each kind.


Let's try it out:


\begin{lstlisting}[frame=single,frameround=tttt]
# Something to test (doesn't handle zero properly).
def sign(value):
    if value < 0:
        return -1
    else:
        return 1

# These two should pass.
def test_sign_negative():
    assert sign(-3) == -1

def test_sign_positive():
    assert sign(19) == 1

# This one should fail.
def test_sign_zero():
    assert sign(0) == 0

# This one is an error (misspelled function).
def test_sign_error():
    assert sgn(1) == 1

# Register functions and Call the main driver.
hope_that("Sign of negative is -1", test_sign_negative)
hope_that("Sign of positive is 1", test_sign_positive)
hope_that("Sign of zero is 0", test_sign_zero)
hope_that("Sign misspelled is error", test_sign_error)
main()
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
pass 2
fail 1
error 1
\end{lstlisting}


\section{Discovery}\label{tester-discovery}


This simple approach does what it's supposed to, but:

\begin{enumerate}

\item 

It doesn't tell us which tests have passed or failed.



\item 

The description of the test is separate from the test code.
    Some people argue that tests shouldn't need descriptions—that
    we should instead give them long names that describe what they're doing—but
    we should support string-style explanations for those who want them.



\item 

It doesn't discover tests on its own:
    we have to remember to register the test using \texttt{hope\_that},
    which means that sooner or later (probably sooner)
    some of our tests won't be run.



\item 

We don't have a way to test things that are supposed to raise \texttt{AssertionError}.
    Putting assertions into code to check that it is behaving correctly
    is called \glossref{defensive programming};
    it's a good practice,
    but we should make sure those assertions are failing when they're supposed to,
    just as we should test our smoke detectors every once in a while.



\end{enumerate}


We can solve several of these problems at once by looking up test functions dynamically.
Python stores the variables and functions we define in a dictionary.
We can get that dictionary by calling the function \texttt{globals}:


\begin{lstlisting}[frame=single,frameround=tttt]
g = {k:str(v) for (k, v) in globals().items()}

import json
import sys
json.dump(g, sys.stdout, indent=2)
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
{
  "__name__": "__main__",
  "__doc__": "None",
  "__package__": "None",
  "__loader__": "<_frozen_importlib_external.SourceFileLoader object at 0x100c04e50>",
  "__spec__": "None",
  "__annotations__": "{}",
  "__builtins__": "<module 'builtins' (built-in)>",
  "__file__": "/Users/gvwilson/sdpy/src/tester/show_globals.py",
  "__cached__": "None"
}
\end{lstlisting}



We can loop over the keys of this dictionary and find things with particular names:


\begin{lstlisting}[frame=single,frameround=tttt]
def test_sign_negative():
    assert sign(-3) == -1

def test_sign_positive():
    assert sign(19) == 1

def test_sign_zero():
    assert sign(0) == 0

def test_sign_error():
    assert sgn(1) == 1

def show_tests():
    """Show all functions that start with 'test_'."""
    for name in globals():
        if name.startswith("test_"):
            print(name)

if __name__ == "__main__":
    show_tests()
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
test_sign_negative
test_sign_positive
test_sign_zero
test_sign_error
\end{lstlisting}



\noindent which means we can find all the tests in a module,
call them,
and keep track of their results:


\begin{lstlisting}[frame=single,frameround=tttt]
# Something to test (doesn't handle zero properly).
def sign(value):
    if value < 0:
        return -1
    else:
        return 1


# These two should pass.
def test_sign_negative():
    assert sign(-3) == -1


def test_sign_positive():
    assert sign(19) == 1


# This one should fail.
def test_sign_zero():
    assert sign(0) == 0


# This one is an error (misspelled function).
def test_sign_error():
    assert sgn(1) == 1


def run_tests(prefix):
    """Run all the functions whose names start with the given prefix."""
    prefixed_names = [n for n in globals() if n.startswith(prefix)]
    for name in prefixed_names:
        func = globals()[name]
        try:
            func()
            print(f"pass: {name}")
        except AssertionError as e:
            print(f"fail: {name} {str(e)}")
        except Exception as e:
            print(f"error: {name} {str(e)}")


if __name__ == "__main__":
    run_tests("test_")
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
pass: test_sign_negative
pass: test_sign_positive
fail: test_sign_zero
error: test_sign_error name 'sgn' is not defined
\end{lstlisting}



This approach is less typing and less fragile than our first,
but we can improve it by showing the test function's \glossref{docstring}
if it has one.
Again,
functions are just objects,
which means they can have attributes.
If we give a function a docstring:

\begin{lstlisting}[frame=single,frameround=tttt]
def example():
   "This is documentation."""
   pass
\end{lstlisting}


then \texttt{example.\_\_doc\_\_} contains the string \texttt{"This is documentation."}


We can do more than just print these strings when reporting problems:
we can embed instructions for the test framework in them.
For example,
we could decide that the string \texttt{"test:skip"} means "skip this test",
while \texttt{"test:fail"} means "we expect this test to fail".
Let's rewrite our tests to show this off:


\begin{lstlisting}[frame=single,frameround=tttt]
TEST_FAIL = "test:fail"
TEST_SKIP = "test:skip"


def test_sign_negative():
    "test:skip"
    assert sign(-3) == -1


def test_sign_positive():
    assert sign(19) == 1


def test_sign_zero():
    "test:fail"
    assert sign(0) == 0


def test_sign_error():
    """Expect an error."""
    assert sgn(1) == 1
\end{lstlisting}



\noindent and then modify \texttt{run\_tests} to look for these strings and act accordingly:


\begin{lstlisting}[frame=single,frameround=tttt]
def run_tests(prefix):
    """Run all the functions whose names start with the given prefix."""
    prefixed_names = [n for n in globals() if n.startswith(prefix)]
    for name in prefixed_names:
        func = globals()[name]
        try:
            if TEST_SKIP in func.__doc__:
                print(f"skip: {name}")
            else:
                func()
                print(f"pass: {name}")
        except AssertionError as e:
            if TEST_FAIL in func.__doc__:
                print(f"pass (expected failure): {name}")
            else:
                print(f"fail: {name} {str(e)}")
        except Exception as e:
            doc = f"/{func.__doc__}" if func.__doc__ else ""
            print(f"error: {name}{doc} {str(e)}")
\end{lstlisting}



The output is now:


\begin{lstlisting}[frame=single,frameround=tttt]
skip: test_sign_negative
error: test_sign_positive argument of type 'NoneType' is not iterable
pass (expected failure): test_sign_zero
error: test_sign_error/Expect an error. name 'sgn' is not defined
\end{lstlisting}


\section*{Mock Objects}


We can do more than look up functions in a running program:
we can change them,
and doing this can make testing much easier.
For example,
if our test checks the time of day,
we can temporarily replace the real \texttt{time.time} function
with one that returns a fixed value.
Similarly,
if a test needs data from a database,
we can temporarily replace the function that gets the data
with one that returns a known, fixed dataset in a fraction of the time.


Temporary replacements like this are called \glossref{mock objects}
because they mimic the essential behavior of the real objects in the program.
We usually use objects even if the thing we're replacing is a function,
and rely on the fact that Python lets us create objects that can be called just like functions:


\begin{lstlisting}[frame=single,frameround=tttt]
class Adder:
    def __init__(self, value):
        self.value = value

    def __call__(self, arg):
        return arg + self.value


add_3 = Adder(3)
result = add_3(8)
print(f"add_3(8): {result}")
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
add_3(8): 11
\end{lstlisting}



Let's create a class that:

\begin{enumerate}

\item Defines a \texttt{\_\_call\_\_} method so that instances can be called like functions.

\item Declares the parameters of that method to be \texttt{*args*} and \texttt{**kwargs}
    so that it can be called with any number of regular or keyword arguments.

\item Stores those arguments so we can see how the replaced function was called.

\item Returns either a fixed value or a value produced by a user-defined function.

\end{enumerate}


The whole thing looks like this:


\begin{lstlisting}[frame=single,frameround=tttt]
class Fake:
    """An object that can take the place of a callable."""
    def __init__(self, func=None, value=None):
        """Construct."""
        self.calls = []
        self.func = func
        self.value = value

    def __call__(self, *args, **kwargs):
        """Remember the call and return either the result of
        the function given to the constructor or a constant."""
        self.calls.append([args, kwargs])
        if self.func is not None:
            return self.func(*args, **kwargs)
        return self.value
\end{lstlisting}



For convenience,
let's also define a function that replaces some function we've already defined
with an instance of our \texttt{Fake} class.
This function takes either a fixed value or another function as an argument
and passes those to \texttt{Fake}'s constructor:


\begin{lstlisting}[frame=single,frameround=tttt]
def fixit(name, func=None, value=None):
    """Replace the thing named 'name'."""
    assert name in globals()
    fake = Fake(func, value)
    globals()[name] = fake
    return fake
\end{lstlisting}



Next,
we'll define a function that adds two numbers
and write a test for it:


\begin{lstlisting}[frame=single,frameround=tttt]
def adder(a, b):
    """The function we're testing."""
    return a + b


def test_with_real_function():
    """Does the real function work?"""
    assert adder(2, 3) == 5
\end{lstlisting}



But we can also use \texttt{fixit} to replace the real \texttt{adder} function
with a mock object that always returns 99
and check that it actually does:


\begin{lstlisting}[frame=single,frameround=tttt]
def test_with_fixed_return_value():
    """Can we return a constant instead?"""
    fixit("adder", value=99)
    assert adder(2, 3) == 99
\end{lstlisting}



Another test proves that our \texttt{Fake} class records
all of the calls:


\begin{lstlisting}[frame=single,frameround=tttt]
def test_fake_records_calls():
    """Does the fake object record all calls?"""
    fake = fixit("adder", value=99)
    assert adder(2, 3) == 99
    assert adder(3, 4) == 99
    assert adder.calls == [[(2, 3), {}], [(3, 4), {}]]
\end{lstlisting}



And finally,
the user can provide a function to calculate a return value:


\begin{lstlisting}[frame=single,frameround=tttt]
def test_fake_calculates_result():
    """Can the fake object calculate a value?"""
    fixit("adder", func=lambda left, right: 10 * left + right)
    assert adder(2, 3) == 23
\end{lstlisting}



We can run all of these tests using the same "lookup and call" trick
we developed earlier,
but there's a problem.
Every test except the first one replaces \texttt{adder} with a mock object
but doesn't put \texttt{adder} back when it's done.
As a result,
any test that \emph{doesn't} replace \texttt{adder} will run with
whatever mock object was last put in place:


\begin{lstlisting}[frame=single,frameround=tttt]
pass: test_with_real_function
pass: test_with_fixed_return_value
pass: test_fake_records_calls
pass: test_fake_calculates_result
adder(2, 3) is now 23
\end{lstlisting}



We could fix this by asking users to remember to swap things back when they're done,
but people are forgetful.
Instead,
we can set up a \glossref{context manager} that does this automatically.
A context manager is a class that defines two methods called \texttt{\_\_enter\_\_} and \texttt{\_\_exit\_\_}.
If the class is called \texttt{C},
then when Python encounters a \texttt{with} block like this:

\begin{lstlisting}[frame=single,frameround=tttt]
with C(…args…) as name:
    …do things…
\end{lstlisting}


\noindent it does the following:

\begin{enumerate}

\item Call \texttt{C}'s constructor with the given arguments.

\item Assign the result to the variable \texttt{name}.

\item Call \texttt{name.\_\_enter\_\_()}.

\item Run the code inside the \texttt{with} block.

\item When the block finishes, call \texttt{name.\_\_exit\_\_()}.

\end{enumerate}


\noindent The last step is guaranteed to happen
even if an exception occurred inside the block,
so the context manager always has a chance to clean up after itself.


Here's a mock object that inherits all the capabilities of \texttt{Fake}
and adds the two methods needed by \texttt{with}:


\begin{lstlisting}[frame=single,frameround=tttt]
class ContextFake(Fake):
    """Now make it work as a context manager."""

    def __init__(self, name, func=None, value=None):
        """Construct."""
        super().__init__(func, value)
        self.name = name
        self.original = None

    def __enter__(self):
        """Replace the original function."""
        assert self.name in globals()
        self.original = globals()[self.name]
        globals()[self.name] = self
        return self

    def __exit__(self, exc_type, exc_value, exc_traceback):
        """Put everything back."""
        globals()[self.name] = self.original
\end{lstlisting}



\noindent And here's a test to prove that it works:


\begin{lstlisting}[frame=single,frameround=tttt]
def subber(a, b):
    """Another function to test."""
    return a - b


def check_no_lasting_effects():
    """Make sure the function goes back to working."""
    assert subber(2, 3) == -1
    with ContextFake("subber", value=1234) as fake:
        assert subber(2, 3) == 1234
        assert len(fake.calls) == 1
    assert subber(2, 3) == -1
\end{lstlisting}


\section{Exercises}\label{tester-exercises}

\subsection*{Literal strings}


If we have defined a variable with the test-skipping marker,
why can't we use that variable as the function's docstring like this:

\begin{lstlisting}[frame=single,frameround=tttt]
TEST_SKIP = "test:skip"


def test_sign_negative():
    TEST_SKIP
    assert sign(-3) == -1
\end{lstlisting}

\chapter{Chapter 3: Versioned File Backups}\label{backup}

\begin{itemize}

\item FIXME

\end{itemize}


\noindent 
    Terms defined:
    \glossref{Application Programming Interface}, \glossref{collision (in hashing)}, \glossref{Coordinated Universal Time}, \glossref{cryptographic hash function}, \glossref{data migration}, \glossref{hash code}, \glossref{hash function}, \glossref{mock object}, \glossref{race condition}, \glossref{SHA-256 hash code}, \glossref{streaming API}, \glossref{time of check - time of use}, \glossref{timestamp}, \glossref{version control system}



A \glossref{version control system}\index{version control system}
like \hreffoot{Git}{https://git-scm.com/}\index{Git}\index{version control system!Git}
keeps track of changes to files
so that we can see what we've changed and recover old versions.
Its core is a way to archive files that:

\begin{enumerate}

\item records which versions of which files existed at the same time
    (so that we can go back to a consistent previous state), and

\item stores any particular version of a file only once,
    so that we don't waste disk space.

\end{enumerate}


This chapter builds a tool that does both tasks.
It won't let us create and merge branches;
if you would like to know how that works,
please see \hreffoot{Mary Rose Cook's}{https://maryrosecook.com/}\index{Cook, Mary Rose} \hreffoot{Gitlet}{http://gitlet.maryrosecook.com/}.

\section{Identifying Unique Files}\label{backup-unique}


To avoid storing redundant copies of files,
we need a way to tell when two files contain the same data.
We can't rely on names because files can be renamed or moved over time;
we could compare the files byte by byte,
but a quicker way is to use a \glossref{hash function}\index{hash function}
that turns arbitrary data into a fixed-length string of bits
(\figref{backup-hash-function}).

\figpdf{backup-hash-function}{./backup/hash-function.pdf}{How hash functions speed up lookup.}{0.6}


A hash function always produces the same \glossref{hash code}\index{hash code} for a given input.
A \glossref{cryptographic hash function}\index{cryptographic hash function}\index{hash function!cryptographic}
has two extra properties:

\begin{enumerate}

\item 

The output depends on the entire input:
    changing even a single byte almost certainly results in a different hash code.



\item 

The outputs look like random numbers:
    they are unpredictable and evenly distributed
    (i.e., the odds of getting any specific hash code are the same).



\end{enumerate}


It's easy to write a bad hash function,
but very hard to write one that qualifies as cryptographic.
We will therefore use Python's \hreffoot{hashlib}{https://docs.python.org/3/library/hashlib.html} module
to calculate \glossref{SHA256}\index{hash code!SHA256}\index{SHA256 hash code} hashes of our files.
These are not random enough to keep data secret from a patient, well-funded attacker,
but that's not what we're using them for:
we just want hashes that are random to make
\glossref{collision}\index{hash function!collision}\index{collision (in hashing)}
extremely unlikely.



To calculate the hash of a file,
we create an object that keeps track of the current state of the hashing calculation
and then feed it some bytes.
When we are done,
we call its \texttt{hexdigest} method to  get the final result:


\begin{lstlisting}[frame=single,frameround=tttt]
import hashlib
import sys

BUFFER_SIZE = 4 * 1024 # how much data to read at once

def hash_stream(reader):
    sha256 = hashlib.sha256()
    while True:
        block = reader.read(BUFFER_SIZE)
        if not block:
            break
        sha256.update(block)
    return sha256.hexdigest()

if __name__ == "__main__":
    reader = open(sys.argv[1], "rb")
    result = hash_stream(reader)
    print(result)
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
python hash_stream.py frankenstein.txt
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
0219696507d0db0e8c21c28e1b2de642b94de91be5ad9cfff0c27e5c51456987
\end{lstlisting}



To prove that it really does generate a unique code,
let's calculate the hash of the novel \emph{Dracula} instead:


\begin{lstlisting}[frame=single,frameround=tttt]
python hash_stream.py dracula.txt
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
15d9da9ec739b9c59225ddf9bd5ca26441d761b63556fa92be556ebbf5ad442c
\end{lstlisting}



\section{Saving Files}\label{backup-files}


Many files only change occasionally after they're created, or not at all.
It would be wasteful for a version control system to make copies
each time the user wanted to save a snapshot of a project,
so instead our tool will copy each unique file to something like \texttt{abcd1234.bck},
where \texttt{abcd1234} is a hash of the file's contents.
It will then store a data structure that records the filenames and hash keys for each snapshot.
The hash keys tell it which unique files are part of the snapshot,
while the filenames tell us what each file's contents were called when the snapshot was made
(since files can be moved or renamed).
To restore a particular snapshot,
we will copy the \texttt{.bck} files back to where they were
(\figref{backup-storage}).

\figpdf{backup-storage}{./backup/storage.pdf}{Organization of backup file storage.}{0.6}


The first step is to find all the files in or below a given directory
and calculate their hashes.
The easiest way to find files is to use Python's \hreffoot{glob}{https://docs.python.org/3/library/glob.html} module
to do simple pattern matching.
If we have this directory structure:


\begin{lstlisting}[frame=single,frameround=tttt]
tree --charset ascii sample_dir
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
sample_dir
|-- a.txt
|-- b.txt
`-- subdir
    `-- c.txt

1 directory, 3 files
\end{lstlisting}



\noindent then a single call to \texttt{glob.glob} will find all the files with two-part names:


\begin{lstlisting}[frame=single,frameround=tttt]
import glob
import sys

root_dir = sys.argv[1]
for filename in glob.glob("**/*.*", root_dir=root_dir, recursive=True):
    print(filename)
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
python try_glob.py sample_dir
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
b.txt
a.txt
subdir/c.txt
\end{lstlisting}



Let's combine the two to create a table of files and hashes:


\begin{lstlisting}[frame=single,frameround=tttt]
import csv
import glob
from pathlib import Path
import sys

from hash_stream import hash_stream

def hash_all(root_dir):
    result = []
    for filename in glob.glob("**/*.*", root_dir=root_dir, recursive=True):
        full_name = Path(root_dir, filename)
        with open(full_name, "rb") as reader:
            hash_code = hash_stream(reader)
            result.append((filename, hash_code))
    return result

if __name__ == "__main__":
    root_dir = sys.argv[1]
    table = hash_all(root_dir)
    writer = csv.writer(sys.stdout)
    writer.writerow(["filename", "hash"])
    writer.writerows(table)
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
python hash_all.py sample_dir
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
filename,hash
b.txt,3cf9a1a81f6bdeaf08a343c1e1c73e89cf44c06ac2427a892382cae825e7c9c1
a.txt,17e682f060b5f8e47ea04c5c4855908b0a5ad612022260fe50e11ecb0cc0ab76
subdir/c.txt,5695d82a086b677962a0b0428ed1a213208285b7b40d7d3604876d36a710302a
\end{lstlisting}


\section{Testing}\label{backup-test}


Before we go any further
we need to figure out how we're going to test our code.
The obvious approach is to create directories and sub-directories full of little files to use as fixtures.
However,
as soon as we start backing things up and restoring them
we are going to be changing or deleting those files.
In order to make sure early tests don't contaminate later ones
we would have to re-create those files and directories after each teach.


A better approach is to use a \glossref{mock object}\index{mock object}
instead of the real filesystem.
As described in \chapref{tester},
a mock object has the same interface as the the thing it replaces,
but is designed to be used solely for testing.
The \hreffoot{pyfakefs}{https://jmcgeheeiv.github.io/pyfakefs/} module replaces key functions like \texttt{read}
with functions that act the same
but act on "files" stores in memory
(\figref{backup-mock-fs}).
This prevents our tests from accidentally disturbing the filesystem,
and also makes tests much faster
(since in-memory operations are thousands of times faster than operations that touch the disk).

\figpdf{backup-mock-fs}{./backup/mock-fs.pdf}{Using a mock filesystem to simplify testing.}{0.6}


If we \texttt{import pyfakefs},
we automatically get a fixture\index{fixture} called \texttt{fs}
that we can use to create files:


\begin{lstlisting}[frame=single,frameround=tttt]
from pathlib import Path

def test_simple_example(fs):
    sentence = "This file contains one sentence."
    fs.create_file("alpha.txt", contents=sentence)
    assert Path("alpha.txt").exists()
    with open("alpha.txt", "r") as reader:
        assert reader.read() == sentence
\end{lstlisting}



We can use \texttt{fs} to create more complicated fixtures of our own
with multiple directories and files:


\begin{lstlisting}[frame=single,frameround=tttt]
from pathlib import Path
import pytest

@pytest.fixture
def our_fs(fs):
    fs.create_file("a.txt", contents="aaa")
    fs.create_file("b.txt", contents="bbb")
    fs.create_file("subdir/c.txt", contents="ccc")

def test_nested_example(our_fs):
    assert Path("a.txt").exists()
    assert Path("b.txt").exists()
    assert Path("subdir/c.txt").exists()

def test_deletion_example(our_fs):
    assert Path("a.txt").exists()
    Path("a.txt").unlink()
    assert not Path("a.txt").exists()
\end{lstlisting}



\noindent and then test that \texttt{hash\_all} finds all the files:


\begin{lstlisting}[frame=single,frameround=tttt]
from pathlib import Path
import pytest

from hash_all import hash_all

@pytest.fixture
def our_fs(fs):
    fs.create_file("a.txt", contents="aaa")
    fs.create_file("b.txt", contents="bbb")
    fs.create_file("subdir/c.txt", contents="ccc")

def test_hashing(our_fs):
    result = hash_all(".")
    assert {r[0] for r in result} == {"a.txt", "b.txt", "subdir/c.txt"}
    assert all(len(r[1]) == 64 for r in result)
\end{lstlisting}



\noindent and that hashes change when files change:


\begin{lstlisting}[frame=single,frameround=tttt]
def test_change(our_fs):
    original = hash_all(".")
    original = [entry for entry in original if entry[0] == "a.txt"][0]
    with open("a.txt", "w") as writer:
        writer.write("this is new content for a.txt")
    changed = hash_all(".")
    changed = [entry for entry in changed if entry[0] == "a.txt"][0]
    assert original != changed
\end{lstlisting}


\section{Tracking Backups}\label{backup-track}


The second part of our backup tool keeps track of which files have and haven't been backed up already.
It stores backups in a directory that contains files like \texttt{abcd1234.bck}
(the hash followed by \texttt{.bck})
and CSV manifests that describe the contents of particular snapshots.
The latter are named \texttt{ssssssssss.csv},
where \texttt{ssssssssss} is the \glossref{UTC} \glossref{timestamp} of the backup's creation.



Here's a function that creates a backup:


\begin{lstlisting}[frame=single,frameround=tttt]
def backup(source_dir, backup_dir):
    manifest = hash_all(source_dir)
    timestamp = current_time()
    write_manifest(backup_dir, timestamp, manifest)
    copy_files(source_dir, backup_dir, manifest)
    return manifest
\end{lstlisting}



When writing the manifest,
we check that the backup directory exists,
create it if it does not,
and then save the manifest as CSV:


\begin{lstlisting}[frame=single,frameround=tttt]
def write_manifest(backup_dir, timestamp, manifest):
    backup_dir = Path(backup_dir)
    if not backup_dir.exists():
        backup_dir.mkdir()
    manifest_file = Path(backup_dir, f"{timestamp}.csv")
    with open(manifest_file, "w") as raw:
        writer = csv.writer(raw)
        writer.writerow(["filename", "hash"])
        writer.writerows(manifest)
\end{lstlisting}



\noindent We then copy those files that \emph{haven't} already been saved:


\begin{lstlisting}[frame=single,frameround=tttt]
def copy_files(source_dir, backup_dir, manifest):
    for (filename, hash_code) in manifest:
        source_path = Path(source_dir, filename)
        backup_path = Path(backup_dir, f"{hash_code}.bck")
        if not backup_path.exists():
            shutil.copy(source_path, backup_path)
\end{lstlisting}



\noindent Finally,
we could call \texttt{time.time()} directly to get the current time,
but we will wrap it up to give ourselves something
that we can easily replace with a mock for testing:


\begin{lstlisting}[frame=single,frameround=tttt]
def current_time():
    return f"{time.time()}".split(".")[0]
\end{lstlisting}



We do one test with real files:


\begin{lstlisting}[frame=single,frameround=tttt]
python backup.py sample_dir /tmp/backups
tree --charset ascii /tmp/backups
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
/tmp/backups
|-- 1662312920.csv
|-- 17e682f060b5f8e47ea04c5c4855908b0a5ad612022260fe50e11ecb0cc0ab76.bck
|-- 3cf9a1a81f6bdeaf08a343c1e1c73e89cf44c06ac2427a892382cae825e7c9c1.bck
`-- 5695d82a086b677962a0b0428ed1a213208285b7b40d7d3604876d36a710302a.bck

0 directories, 4 files
\end{lstlisting}



The rest of our tests use a fake filesystem
and a mock replacement for the \texttt{current\_time} function
(so that we know what the manifest file will be called).
The setup is:


\begin{lstlisting}[frame=single,frameround=tttt]
from pathlib import Path
import pytest
from unittest.mock import patch
from backup import backup

FILES = {
    "a.txt": "aaa",
    "b.txt": "bbb",
    "subdir/c.txt": "ccc"
}

@pytest.fixture
def our_fs(fs):
    for name, contents in FILES.items():
        fs.create_file(name, contents=contents)
\end{lstlisting}



\noindent and an example of a single test is:


\begin{lstlisting}[frame=single,frameround=tttt]
def test_nested_example(our_fs):
    with patch("backup.current_time", return_value=1234):
        manifest = backup(".", "/backup")
    for filename, hash_code in manifest:
        assert Path("/backup", f"{hash_code}.bck").exists()
        assert Path("/backup", "1234.csv").exists()
\end{lstlisting}


\section{Exercises}\label{backup-exercises}

\subsection*{Odds of collision}


If hashes were only 2 bits long,
then the chances of collision with each successive file
assuming no previous collision are:


\vspace{\baselineskip}
\begin{tabular}{ll}
\textbf{\underline{Number of Files}} & \textbf{\underline{Odds of Collision}} \\
1 & 0\% \\
2 & 25\% \\
3 & 50\% \\
4 & 75\% \\
5 & 100\% \\
\end{tabular}

\vspace{\baselineskip}


A colleague of yours says this means that if we hash four files,
there's only a 75\% chance of any collision occurring.
What are the actual odds?

\subsection*{Streaming I/O}


Write a small program using \texttt{fs.createReadStream} and \texttt{fs.createWriteStream}
that copies a file piece by piece
instead of reading it into memory and then writing it out again.

\subsection*{Sequencing backups}


Modify the backup program so that manifests are numbered sequentially
as \texttt{00000001.csv}, \texttt{00000002.csv}, and so on
rather than being timestamped.
Why doesn't this solve the time of check/time of use race condition mentioned earlier.

\subsection*{JSON manifests}

\begin{enumerate}

\item 

Modify \texttt{backup.py} so that it can save JSON manifests as well as CSV manifests
    based on a command-line flag.



\item 

Write another program called \texttt{migrate.py} that converts a set of manifests
    from CSV to JSON.
    (The program's name comes from the term \glossref{data migration}.)



\item 

Modify \texttt{backup.py} programs so that each manifest stores the user name of the person who created it
    along with file hashes,
    and then modify \texttt{migrate.py} to transform old files into the new format.



\end{enumerate}

\subsection*{Mock hashes}

\begin{enumerate}

\item 

Modify the file backup program so that it uses a function called \texttt{ourHash} to hash files.



\item 

Create a replacement that returns some predictable value, such as the first few characters of the data.



\item 

Rewrite the tests to use this function.



\end{enumerate}


How did you modify the main program so that the tests could control which hashing function is used?

\subsection*{Comparing manifests}


Write a program \texttt{compare-manifests.py} that reads two manifest files and reports:

\begin{itemize}

\item 

Which files have the same names but different hashes
    (i.e., their contents have changed).



\item 

Which files have the same hashes but different names
    (i.e., they have been renamed).



\item 

Which files are in the first hash but neither their names nor their hashes are in the second
    (i.e., they have been deleted).



\item 

Which files are in the second hash but neither their names nor their hashes are in the first
    (i.e., they have been added).



\end{itemize}

\subsection*{From one state to another}

\begin{enumerate}

\item 

Write a program called \texttt{from\_to.py} that takes the name of a directory
    and the name of a manifest file
    as its command-line arguments,
    then adds, removes, and/or renames files in the directory
    to restore the state described in the manifest.
    The program should only perform file operations when it needs to,
    e.g.,
    it should not delete a file and re-add it if the contents have not changed.



\item 

Write some tests for \texttt{from\_to.py} using pytest and a mock filesystem.



\end{enumerate}

\subsection*{File history}

\begin{enumerate}

\item 

Write a program called \texttt{file\_history.py}
    that takes the name of a file as a command-line argument
    and displays the history of that file
    by tracing it back in time through the available manifests.



\item 

Write tests for your program using pytest and a mock filesystem.



\end{enumerate}

\subsection*{Pre-commit hooks}


Modify \texttt{backup.py} to load and run a function called \texttt{pre\_commit} from a file called \texttt{pre\_commit.py}
stored in the root directory of the files being backed up.
If \texttt{pre\_commit} returns \texttt{True}, the backup proceeds;
if it returns \texttt{False} or throws an exception,
no backup is created.

\chapter{Chapter 4: An Interpreter}\label{interpreter}

\begin{itemize}

\item Compilers and interpreters are programs like any others.

\end{itemize}


\noindent 
    Terms defined:
    \glossref{argument}, \glossref{control flow}, \glossref{dictionary comprehension}, \glossref{dispatch}, \glossref{environment}, \glossref{extensibility}, \glossref{lazy evaluation}, \glossref{parameter}, \glossref{parser}, \glossref{reflection}, \glossref{runtime}, \glossref{signature}



Many software tools take advantage of the fact that
a program is just a data structure.
While the values in a string represent characters
and the values in an image represent pixels,
the values in a program represent instructions.
We can manipulate those instructions just like we manipulate characters and pixels.
This chapter shows how to do that
by building a very small programming language.


Real programming languages have two parts:
a \glossref{parser} that translates the source code into a data structure in memory,
and a \glossref{runtime} that executes the instructions in that data structure.
To keep this chapter manageable we only consider the runtime;
\chapref{parser} will explore parsing.

\section{Expressions}\label{interpreter-expressions}


Let's start by building something that can evaluate simple expressions
like \texttt{1+2} or \texttt{abs(-3.5)}.
We represent each expression as a list
with the name of the operation as the first element
and the values to be operated on as the other elements.
Our first expression is:

\begin{lstlisting}[frame=single,frameround=tttt]
["add", 1, 2]
\end{lstlisting}


\noindent and our second is:

\begin{lstlisting}[frame=single,frameround=tttt]
["abs", -3.5]
\end{lstlisting}


We represent more complicated expressions with nested lists:

\begin{lstlisting}[frame=single,frameround=tttt]
# abs(1+2)
["abs", ["add", 1, 2]]
\end{lstlisting}


We use lists because that's all programs are:
lists of instructions,
some of which are other lists of instructions.
We put the name of the operation first to make it easy to find.


The function to add two expressions looks like this:


\begin{lstlisting}[frame=single,frameround=tttt]
def do_add(args):
    """Add two expressions."""
    assert len(args) == 2
    left = do(args[0])
    right = do(args[1])
    return left + right
\end{lstlisting}



\noindent Its single parameter is a list containing
the two sub-expressions to be evaluated and added.
After checking that it has the right number of parameters,
it calls the function \texttt{do} to evaluate those sub-expressions.
(We've called the function \texttt{do} instead of \texttt{eval}
because Python already has a function called \texttt{eval}.)
Once \texttt{do\_add} has two actual values,
it adds them and returns the result.



\texttt{do\_abs}, which calculates absolute value,
works the same way.
The only differences are that it expects one value instead of two
and calculates a different return value:


\begin{lstlisting}[frame=single,frameround=tttt]
def do_abs(args):
    """Get absolute value of expression."""
    assert len(args) == 1
    val = do(args[0])
    return abs(val)
\end{lstlisting}



\texttt{do\_add} and \texttt{do\_abs} both rely on \texttt{do},
which figures out what function corresponds to an operation name
and calls it:


\begin{lstlisting}[frame=single,frameround=tttt]
def do(expr):
    """Evaluate an expression."""

    # Integers evaluate to themselves.
    if isinstance(expr, int):
        return expr

    # Lists trigger function calls.
    assert isinstance(expr, list)
    if expr[0] == "abs":
        return do_abs(expr[1:])
    if expr[0] == "add":
        return do_add(expr[1:])
    assert False, f"Unknown operation {expr[0]}"
\end{lstlisting}



\texttt{do} starts by checking if its input is an integer.
If so,
it returns that value right away
because integers "evaluate" to themselves:
no more calculation is needed.
Otherwise,
\texttt{do} checks that its parameter is a list
and then uses the first value in the list
to decide what other function to call.
This process is often called \glossref{dispatch}.


Finally,
the main body of the program reads
the file containing the instructions to execute,
calls \texttt{do},
and prints the result:


\begin{lstlisting}[frame=single,frameround=tttt]
def main():
    assert len(sys.argv) == 2, "Usage: expr.py filename"
    with open(sys.argv[1], "r") as reader:
        program = json.load(reader)
    result = do(program)
    print(f"=> {result}")


if __name__ == "__main__":
    main()
\end{lstlisting}



Since our instructions are a list (of lists of lists...)
we can use \texttt{json.load} to read the input file.
If that file is:


\begin{lstlisting}[frame=single,frameround=tttt]
["add", ["abs", -3], 2]
\end{lstlisting}



\noindent then our little interpreter prints:


\begin{lstlisting}[frame=single,frameround=tttt]
=> 5
\end{lstlisting}



This is a lot of code to do something that Python already does,
but the key point is that
\emph{this is what Python does internally}.
When we run our little interpreter with:

\begin{lstlisting}[frame=single,frameround=tttt]
$ python expr.py expr.tll
\end{lstlisting}


Python reads \texttt{expr.py},
turns it into a data structure with operation identifiers and constants,
then uses those operation identifiers to decide what functions to call.
Those functions are written in C,
but the cycle of lookup, call, and recurse is exactly the same.

\section{Variables}\label{interpreter-variables}


Adding up constants is a good start,
but real programming languages let us give names to values.
We can add this to our interpreter
by passing around a dictionary containing all the variables seen so far.
Such a dictionary is often called an \glossref{"environment}
because it is the setting in which expressions are evaluated.


We can easily modify existing functions like \texttt{do\_abs}
to take an environment as an extra parameter and pass it on to \texttt{do} as needed:


\begin{lstlisting}[frame=single,frameround=tttt]
def do_abs(env, args):
    """Get absolute value of expression."""
    assert len(args) == 1
    val = do(env, args[0])
    return abs(val)
\end{lstlisting}



Looking up variables when we need their values is straightfoward.
We check that we have a variable name and that the name is in the environment,
then return the stored value:


\begin{lstlisting}[frame=single,frameround=tttt]
def do_get(env, args):
    """Get the value of a variable."""
    assert len(args) == 1
    assert isinstance(args[0], str)
    assert args[0] in env, f"Unknown variable {args[0]}"
    return env[args[0]]
\end{lstlisting}



To define a new variable or change an existing one,
we evaluate an expression and store its value in the environment:


\begin{lstlisting}[frame=single,frameround=tttt]
def do_get(env, args):
    """Get the value of a variable."""
    assert len(args) == 1
    assert isinstance(args[0], str)
    assert args[0] in env, f"Unknown variable {args[0]}"
    return env[args[0]]
\end{lstlisting}



We need to add one more function to make this all work.
Our programs no longer consist of a single expression;
instead,
we may have several expressions that set variables' values
and then use them in calculations.
To handle this,
we can add a function \texttt{do\_seq} that runs a sequence of expressions one by one:


\begin{lstlisting}[frame=single,frameround=tttt]
def do_seq(env, args):
    """Execute a sequence of operations."""
    assert len(args) > 0
    for item in args:
        result = do(env, item)
    return result
\end{lstlisting}



Let's try it out.
Our test program is:


\begin{lstlisting}[frame=single,frameround=tttt]
[
    "seq",
    ["set", "alpha", 1],
    ["set", "beta", 2],
    ["add", ["get", "alpha"], ["get", "beta"]]
]
\end{lstlisting}



\noindent and its output is:


\begin{lstlisting}[frame=single,frameround=tttt]
=> 3
\end{lstlisting}


\section{Reflection}\label{interpreter-reflection}


\texttt{do\_seq} is our first piece of \glossref{control flow}
that controls when and how other expressions are evaluated.
Before we add more (and more basic operations)
let's have a look at the current state of \texttt{do}:


\begin{lstlisting}[frame=single,frameround=tttt]
def do(env, expr):
    """Evaluate an expression in an environment."""

    # Integers evaluate to themselves.
    if isinstance(expr, int):
        return expr

    # Lists trigger function calls.
    assert isinstance(expr, list)
    if expr[0] == "abs":
        return do_abs(env, expr[1:])
    if expr[0] == "add":
        return do_add(env, expr[1:])
    if expr[0] == "get":
        return do_get(env, expr[1:])
    if expr[0] == "seq":
        return do_seq(env, expr[1:])
    if expr[0] == "set":
        return do_set(env, expr[1:])
    assert False, f"Unknown operation {expr[0]}"
\end{lstlisting}



The sequence of \texttt{if} statements that decide what function to call
is going to become unreadably long.
Let's create a lookup table instead:


\begin{lstlisting}[frame=single,frameround=tttt]
OPS = {
    "abs": do_abs,
    "add": do_add,
    "get": do_get,
    "seq": do_seq,
    "set": do_set,
}
\end{lstlisting}



\noindent and then look up the function we want:


\begin{lstlisting}[frame=single,frameround=tttt]
def do(env, expr):
    """Evaluate an expression in an environment."""

    # Integers evaluate to themselves.
    if isinstance(expr, int):
        return expr

    # Lists trigger function calls.
    assert isinstance(expr, list)
    assert expr[0] in OPS, f"Unknown operation {expr[0]}"
    func = OPS[expr[0]]
    return func(env, expr[1:])
\end{lstlisting}



This lookup table is the central point of this chapter.
A program is just a data structure
with instructions and functions instead of characters and pixels.
That means we can put functions in other data structures such as dictionaries
just like we would put in strings or images.


Of course we still have to know how to call the function,
i.e.,
what parameters to give it in what order.
That's why all the \texttt{do\_*} functions have exactly the same \glossref{signature}:
we can all any of them with an environment and a list of arguments
without knowing exactly which function we're calling.


We can go one step further with our lookup table.
Python has a built-in function called \texttt{globals}
that returns a dictionary of all the variables defined at the top level of the current running program.
This dictionary is essentially the same thing as the environment
we are passing around in our interpreter.
If we call the function in a newly-launched interpreter we get this:

\begin{lstlisting}[frame=single,frameround=tttt]
>>> globals()
{
    '__name__': '__main__',
    '__doc__': None,
    '__package__': None,
    '__loader__': <class '_frozen_importlib.BuiltinImporter'>,
    '__spec__': None,
    '__annotations__': {},
    '__builtins__': <module 'builtins' (built-in)>
}
\end{lstlisting}


\noindent which shows us that Python has defined seven variables for us
before our code even starts running.
(We'll take a closer look at some of these variables in future chapters.)


If we define a variable of our own and then re-run \texttt{globals},
our variable shows up along with the predefined ones:

\begin{lstlisting}[frame=single,frameround=tttt]
>>> example = 3
>>> globals()
{
    '__name__': '__main__',
    '__doc__': None,
    '__package__': None,
    '__loader__': <class '_frozen_importlib.BuiltinImporter'>,
    '__spec__': None,
    '__annotations__': {},
    '__builtins__': <module 'builtins' (built-in)>,
    'example': 3
}
\end{lstlisting}


Let's use this to add every function whose name starts with \texttt{do\_}
to the \texttt{OPS} lookup table:


\begin{lstlisting}[frame=single,frameround=tttt]
OPS = {
    name.replace("do_", ""): func
    for (name, func) in globals().items()
    if name.startswith("do_")
}
\end{lstlisting}



\noindent Line by line:

\begin{enumerate}

\item 

We are using a \glossref{dictionary comprehension}
    to create a dictionary in a single statement.



\item 

Each key-value pair in the dictionary is the name of an operation
    and the function that implements the operation.
    The operation's name is what comes after \texttt{do\_} in the function's name.



\item 

We only add functions whose names start with \texttt{do\_}.



\end{enumerate}


Looking things up in a live program like this is called \glossref{reflection}.
We're going to use it a lot in the chapters to come.

\section{Statements}\label{interpreter-statements}


We're finally ready to add some more control flow to our little languages.
Our goal is to execute this program,
which starts with the number 1 and doubles it four times:


\begin{lstlisting}[frame=single,frameround=tttt]
[
    "seq",
    ["comment", "Double a number repeatedly"],
    ["set", "a", 1],
    ["print", "initial", ["get", "a"]],
    [
        "repeat",
        4,
        [
            "seq",
            ["set", "a", ["add", ["get", "a"], ["get", "a"]]],
        ["if",
        ["leq", ["get", "a"], 10],
        ["print", "small", ["get", "a"]],
        ["print", "large", ["get", "a"]]
        ]
        ]
    ]
]
\end{lstlisting}



The simplest of the new operations is \texttt{comment},
which does nothing and returns \texttt{None}:


\begin{lstlisting}[frame=single,frameround=tttt]
def do_comment(env, args):
    """Ignore instructions.

    ["comment" "text"] => None
    """
    return None
\end{lstlisting}



An \texttt{if} statement is a bit more complex.
If its first argument is true it evaluates and returns its second argument
(the "if" branch).
Otherwise,
it evaluates and returns its second argument (the "else" branch):


\begin{lstlisting}[frame=single,frameround=tttt]
def do_if(env, args):
    """Make a choice: only one sub-expression is evaluated.

    ["if" C A B] => A if C else B
    """
    assert len(args) == 3
    cond = do(env, args[0])
    choice = args[1] if cond else args[2]
    return do(env, choice)
\end{lstlisting}



\noindent This is called \glossref{lazy evaluation}:
\texttt{do\_if} only evaluates what it absolutely needs to.
Most languages do this so that we can write things like:

\begin{lstlisting}[frame=single,frameround=tttt]
if x != 0:
    return 1/x
else:
    return None
\end{lstlisting}


\noindent If the language always evaluated both branches
then the code shown above would fail whenever \texttt{x} was zero,
even though it's supposed to handle that case.

\section{Functions}\label{interpreter-functions}


There are many ways to assess the design of a piece of software \cite{Wilson2022}.
One of them is to ask how \glossref{extensible} it is,
i.e.,
how easily we can add or change things.
The answer for our little interpreter is, "Pretty easily."
Whenever we want it to do something new,
like a \texttt{for} loop,
all we have to do is add a function called \texttt{do\_whatever}
that takes an environment and a list of arguments.
That function is added to the lookup table automatically
and no other function needs to be modified in order to use it.


Once our little language has variables, loops, and conditionals
it can do everything that any programming language can do.
However,
writing programs will still be painful
because our little language isn't extensible:
there's no way for users to define new operations within the language itself.


Doing this makes the code less than 60 lines longer:

\begin{enumerate}

\item 

Instead of using a single dictionary to store an environment
    we use a list of dictionaries.
    The first dictionary is the global environment;
    the others store variables belonging to active function calls.



\item 

When we get or set a variable,
    we check the most recent environment first
    (i.e., the one that's last in the list);
    if the variable isn't there we look in the global environment.
    We \emph{don't} look at the environments in between.



\item 

A function definition looks like:

\begin{lstlisting}[frame=single,frameround=tttt]
["def", "same", ["num"], ["get", "num"]]
\end{lstlisting}


It has a name, a (possibly empty) list of parameter names,
and a single instruction as a body
(which will usually be a \texttt{"seq"} instruction).



\item 

Functions are stored in the environment like any other value.
    The value stored for the function defined above would be:

\begin{lstlisting}[frame=single,frameround=tttt]
["func", ["num"], ["get", "num"]]
\end{lstlisting}


We don't need to store the name: that's recorded by the environment,
just like it is for any other variable.



\item 

A function call looks like:

\begin{lstlisting}[frame=single,frameround=tttt]
["call", "same", 3]
\end{lstlisting}


The values passed to the functions are normally expressions rather than constants,
and are \emph{not} put in a sub-list.
The implementation:
1.  Evaluates all of these expressions.
2.  Looks up the function.
3.  Creates a new environment whose keys are the parameters' names
    and whose values are the expressions' values.
4.  Calls \texttt{do} to run the function's action and captures the result.
5.  Discards environment created two steps previously.
6.  Returns the function's result.



\end{enumerate}


Here's the implementation of \texttt{do\_def}:


\begin{lstlisting}[frame=single,frameround=tttt]
def do_def(env, args):
    """Define a new function.

    ["def" name [...params...] body] => None # and define function
    """
    assert len(args) == 3
    name = args[0]
    params = args[1]
    body = args[2]
    env_set(env, name, ["func", params, body])
    return None
\end{lstlisting}



And here's the implementation of \texttt{do\_call}:


\begin{lstlisting}[frame=single,frameround=tttt]
def do_def(env, args):
    """Define a new function.

    ["def" name [...params...] body] => None # and define function
    """
    assert len(args) == 3
    name = args[0]
    params = args[1]
    body = args[2]
    env_set(env, name, ["func", params, body])
    return None
\end{lstlisting}



Our test program looks like this:


\begin{lstlisting}[frame=single,frameround=tttt]
[
    "seq",
    [
        "def",
        "double",
        ["num"],
        ["add", ["get", "num"], ["get", "num"]]
    ],
    ["set", "a", 1],
    [
        "repeat",
        4,
        [
            "seq",
            ["set", "a", ["call", "double", ["get", "a"]]],
            ["print", ["get", "a"]]
        ]
    ]
]
\end{lstlisting}



\noindent and its output is:


\begin{lstlisting}[frame=single,frameround=tttt]
2
4
8
16
=> None
\end{lstlisting}



Once again,
the key point is that
\emph{this is how Python and other languages work}.

\section{Exercises}\label{interpreter-exercises}

\subsection*{Arrays}


Implement fixed-size one-dimensional arrays:
\texttt{["array", "new", 10]} creates an array of 10 elements,
while other instructions get and set particular array elements by index.

\subsection*{While Loops}

\begin{enumerate}

\item 

Add a \texttt{while} loop using a Python \texttt{while} loop.



\item 

Add a \texttt{while} loop using recursion.



\end{enumerate}

\subsection*{Loop Counter}


The \texttt{"repeat"} instruction runs some other instruction(s) several times,
but there is no way to access the loop counter inside those instructions.
Modify \texttt{"repeat"} so that programs can do this.
(Hint: allow people to create a new variable to hold the loop counter's current value.)

\subsection*{Better Error Handling}


Several of the instruction functions started with \texttt{assert} statements,
which means that users get a stack trace of TLL itself
when there's a bug in their program.

\begin{enumerate}

\item 

Define a new exception class called \texttt{TLLException}.



\item 

Write a utility function called \texttt{check}
    that raises a \texttt{TLLException} with a useful error message
    when there's a problem.



\item 

Add a \texttt{catch} statement to handle these errors.



\end{enumerate}

\subsection*{Tracing}


Add a \texttt{--trace} command-line flag to the interpreter.
When enabled, it makes TLL print a messages showing each function call and its result.

\subsection*{Early Return}


Add a \texttt{"return"} instruction to TLL that ends a function call immediately
and returns a single value.

\subsection*{Variable Arguments}


Add variable-length parameter lists to functions.

\subsection*{Nested Scopes}


The interpreter allows users to define functions inside functions.
What variables can the inner function access when you do this?
What variables \emph{should} it be able to access?
What would you have to do to enable this?

\chapter{Chapter 5: A Dataframe}\label{dataframe}

\begin{itemize}

\item Create abstract base classes to specify interfaces.

\item Store two-dimensional data as rows or as columns.

\item Use reflection to match data to function parameters.

\item Measure performance to evaluate engineering tradeoffs.

\end{itemize}


\noindent 
    Terms defined:
    \glossref{abstract class}, \glossref{benchmark}, \glossref{column-wise storage}, \glossref{concrete class}, \glossref{immutable}, \glossref{index (a database)}, \glossref{join (tables)}, \glossref{row-wise storage}



Whether your tool of choice is Python, R, SQL, or Excel,
you're almost certainly doing data science on tables
with named columns that have the same type of value in every row.
To explore how they work,
we build two implementations of dataframes in Python:
one that stores values in columns,
the other that stores them in rows.
And to explain how to choose between them,
we measure their performance.

\section{Storing Columns}\label{dataframe-cols}


We start by creating an \glossref{abstract class}
that defines the methods our two dataframe classes will support.
This class (unimaginatively called \texttt{DF})
requires \glossref{concrete classes} to implement eight methods:

\begin{itemize}

\item \texttt{ncol}: report the number of columns.

\item \texttt{nrow}: report the number of rows.

\item \texttt{cols}: return the set of column names.

\item \texttt{eq}: check whether this dataframe is equal to another.

\item \texttt{get}: get a scalar value from a specified column and row.

\item \texttt{set}: set the scalar value in a specified column and row.

\item \texttt{select}: create a new dataframe containing some or all of the original's columns.

\item \texttt{filter}: create a new dataframe containing some or all of the original's rows.

\end{itemize}


\begin{lstlisting}[frame=single,frameround=tttt]
from abc import ABC

class DF(ABC):
    """Dataframe interface."""

    def ncol(self):
        """Report the number of columns."""

    def nrow(self):
        """Report the number of rows."""

    def cols(self):
        """Return the set of column names."""

    def eq(self, other):
        """Check equality of two dataframes."""

    def get(self, col, row):
        """Get a scalar value."""

    def select(self, *names):
        """Select a subset of columns."""

    def filter(self, func):
        """Select a subset of rows."""
\end{lstlisting}




We then derive a class \texttt{DfCol} that uses \glossref{column-wise} storage.
Each column is stored as a list of values,
all of which are of the same type.
The dataframe is a dictionary of such lists,
all of which have the same length:


\begin{lstlisting}[frame=single,frameround=tttt]
from df_base import DF


class DfCol(DF):
    """Column-wise dataframe."""

    def __init__(self, **kwargs):
        """Initialize from `name=[values]`."""
        assert len(kwargs) > 0
        assert all_eq(len(kwargs[k]) for k in kwargs)
        for k in kwargs:
            assert all_eq(type(v) for v in kwargs[k])
        self._data = kwargs
\end{lstlisting}



Some methods are almost trivial to implement on top of this storage mechanism;
others are more difficult.
Three of the easy ones return the number of rows and columns
and the names of the columns:


\begin{lstlisting}[frame=single,frameround=tttt]
    def ncol(self):
        """Report the number of columns."""
        return len(self._data)

    def nrow(self):
        """Report the number of rows."""
        n = set(self._data.keys()).pop()
        return len(self._data[n])

    def cols(self):
        """Return the set of column names."""
        return set(self._data.keys())
\end{lstlisting}



FIXME


Testing for equality is also relatively simple.
Two dataframes are the same if they have exactly the same columns
and the same values in every column:


\begin{lstlisting}[frame=single,frameround=tttt]
    def eq(self, other):
        """Check equality of two dataframes."""
        assert isinstance(other, DF)
        for n in self._data:
            if n not in other.cols():
                return False
            for i in range(len(self._data[n])):
                if self.get(n, i) != other.get(n, i):
                    return False
        return True
\end{lstlisting}



\noindent Notice that we use \texttt{other.cols()} and \texttt{other.get()}
rather than reaching into the other dataframe.
We defined the abstract base class because
we expect to implement dataframes in several different ways.
Those other ways will probably not use the same data structures,
so we can only rely on the interface defined in the base class.


Getting individual values is straightforward:


\begin{lstlisting}[frame=single,frameround=tttt]
    def get(self, col, row):
        """Get a scalar value."""
        assert col in self._data
        assert 0 <= row < len(self._data[col])
        return self._data[col][row]
\end{lstlisting}



\noindent Selecting a subset of columns is also straightforward:


\begin{lstlisting}[frame=single,frameround=tttt]
    def select(self, *names):
        """Select a subset of columns."""
        assert all(n in self._data for n in names)
        return DfCol(**{n:self._data[n] for n in names})
\end{lstlisting}



\noindent Notice,
though,
that the dataframe created by \texttt{select}
re-uses the columns of the original dataframe.
This is safe and efficient so long as columns are \glossref{immutable},
i.e.,
so long as their contents are never changed in place.


Time to write some tests.
This one checks that we can construct a dataframe with some values:


\begin{lstlisting}[frame=single,frameround=tttt]
def test_construct_with_two_pairs():
    df = DfCol(a=[1, 2], b=[3, 4])
    assert df.get("a", 0) == 1
    assert df.get("a", 1) == 2
    assert df.get("b", 0) == 3
    assert df.get("b", 1) == 4
\end{lstlisting}



\noindent while this one checks that \texttt{filter} works correctly:


\begin{lstlisting}[frame=single,frameround=tttt]
def test_filter():
    def odd(a, b):
        return (a % 2) == 1
    df = DfCol(a=[1, 2], b=[3, 4])
    assert df.filter(odd).eq(DfCol(a=[1], b=[3]))
\end{lstlisting}


\section*{Storing Rows}


Column-wise storage makes selecting columns easy but filtering rows hard.
If we expect to do more filtering than selecting
it might be more efficient to use \glossref{row-wise} storage.
The class \texttt{DfRow} is derived from the same abstract base class \texttt{DF} as \texttt{DfCol},
so it has to have the same interface.
However,
it stores data as a single list of dictionaries,
each with the same keys and the same types of values:


\begin{lstlisting}[frame=single,frameround=tttt]
from df_base import DF


class DfRow(DF):
    """Row-wise dataframe."""

    def __init__(self, rows):
        """Initialize from a list of rows."""
        assert len(rows) > 0
        assert all(dict_match(r, rows[0]) for r in rows)
        self._data = rows
\end{lstlisting}



\noindent Notice that \texttt{DfRow}'s constructor \emph{doesn't} have the same signature as \texttt{DfCol}.
At some point in our code we have to decide which of the two classes to construct.
If we design our code well that decision will be made in exactly one place
and everything else will rely solely on the common interface defined by \texttt{DF}.
But since we have to type something different at the point of construction,
it's OK for the constructors to be different.


The basic operations \texttt{ncol}, \texttt{nrow}, and \texttt{cols} are straightforward:


\begin{lstlisting}[frame=single,frameround=tttt]
    def ncol(self):
        """Report the number of columns."""
        return len(self._data[0].keys())

    def nrow(self):
        """Report the number of rows."""
        return len(self._data)

    def cols(self):
        """Return the set of column names."""
        return set(self._data[0].keys())
\end{lstlisting}



\noindent Whenever we need information about columns,
we look at the first row.
The assumption that there \emph{is} a first row means we can't represent
an empty dataframe;
we'll explore this in the exercises.


Getting values and checking for equality are also straightforward.
(See the source code for their implementation.)
Selecting and filtering are more interesting,
since they are the whole point of this implementation.
To select columns we must build a new list of dictionaries,
each of which has only some of the keys of the original:


\begin{lstlisting}[frame=single,frameround=tttt]
    def select(self, *names):
        """Select a subset of columns."""
        assert all(n in self._data[0] for n in names)
        rows = [{key:r[key] for key in names} for r in self._data]
        return DfRow(rows)
\end{lstlisting}



\noindent To filter,
we simply pass each row to the user-supplied filter function:


\begin{lstlisting}[frame=single,frameround=tttt]
    def filter(self, func):
        """Select a subset of rows."""
        params = list(inspect.signature(func).parameters.keys())
        result = [r for r in self._data if func(**r)]
        return DfRow(result)
\end{lstlisting}



These operations are the inverses of their \texttt{DfCol} counterparts:
we have to rearrange data to select
but can use the existing data as-is to filter
rather than vice versa.


Since \texttt{DfCol} and \texttt{DfRow} have the same interface,
we can recycle the tests we wrote for the former.
We obviously need to change the objects we construct,
so let's use this opportunity to write helper functions
to create the dataframes we use in multiple tests:


\begin{lstlisting}[frame=single,frameround=tttt]
def odd_even():
    return DfRow([{"a": 1, "b": 3}, {"a": 2, "b": 4}])


def a_only():
    return DfRow([{"a": 1}, {"a": 2}])
\end{lstlisting}



Creating fixtures in functions is so common
that \hreffoot{pytest}{https://docs.pytest.org/} has built-in support for it;
we will explore this in the exercises.
With these functions in hand our tests look like:


\begin{lstlisting}[frame=single,frameround=tttt]
def test_construct_with_two_pairs():
    df = odd_even()
    assert df.get("a", 0) == 1
    assert df.get("a", 1) == 2
    assert df.get("b", 0) == 3
    assert df.get("b", 1) == 4
\end{lstlisting}


\section{Performance}\label{dataframe-performance}


So how do our two classes perform?
To find out,
let's write a short program to create dataframes of each kind
and then time how long it takes to select their columns and filter their rows.
To keep things simple
we will create dataframes whose columns are called \texttt{label\_1}, \texttt{label\_2}, and so on,
and whose values are all integers in the range 0–9.
A thorough set of \glossref{benchmarks} would create columns of other kinds as well,
but this is enough to illustrate the technique.


\begin{lstlisting}[frame=single,frameround=tttt]
SPREAD = 10


def make_col(nrow, ncol):
    """Make a column-oriented dataframe."""
    def _col(n, start):
        return [((start + i) % SPREAD) for i in range(n)]

    fill = {f"label_{c}":_col(nrow, c) for c in range(ncol)}
    return DfCol(**fill)


def make_row(nrow, ncol):
    """Make a row-oriented dataframe."""
    labels = [f"label_{c}" for c in range(ncol)]

    def _row(r):
        return {c:((r + i) % SPREAD) for (i, c) in enumerate(labels)}

    fill = [_row(r) for r in range(nrow)]
    return DfRow(fill)
\end{lstlisting}



To time filtering,
we arbitrarily decide that we will keep rows an even value in the first column.
Again,
if we were doing this for real
we would look at some actual programs
to see what fraction of rows filtering usually kept,
and then model that.


\begin{lstlisting}[frame=single,frameround=tttt]
def time_filter(df):
    """Time filtering operation."""
    def f(label_0, **args):
        return label_0 % 2 == 1

    start = time.time()
    df.filter(f)
    return time.time() - start
\end{lstlisting}



\noindent Notice that \texttt{time\_filter} doesn't know or care
whether it's being given a \texttt{DfCol} or a \texttt{DfRow}.
That's the whole point of deriving them from a base class:
we can use them interchangeably.


Timing \texttt{select} is similar to timing \texttt{filter}.
Again,
we make an arbitrary decision about how many columns to keep
(in this case one third):


\begin{lstlisting}[frame=single,frameround=tttt]
def time_select(df):
    """Time selection operation."""
    indices = [i for i in range(df.ncol()) if ((i % 3) == 0)]
    labels = [f"label_{i}" for i in indices]
    start = time.time()
    df.select(*labels)
    return time.time() - start
\end{lstlisting}



Finally,
we write a function that takes a list of strings like \texttt{3x3} or \texttt{100x20},
creates a dataframe of each kind and of each size,
times operations,
and reports the results:


\begin{lstlisting}[frame=single,frameround=tttt]
def sweep(sizes):
    """Time operations on various sizes of dataframes."""
    sizes = [s.split("x") for s in sizes]
    sizes = [(int(s[0]), int(s[1])) for s in sizes]
    results = [["nrow", "ncol", "filter col", "select col", "filter row", "select row"]]
    for (nrow, ncol) in sizes:
        df_col = make_col(nrow, ncol)
        df_row = make_row(nrow, ncol)
        results.append([
            nrow, ncol,
            time_filter(df_col), time_select(df_col),
            time_filter(df_row), time_select(df_row)
        ])
    csv.writer(sys.stdout).writerows(results)
\end{lstlisting}



The results are shown in \tblref{dataframe-timing}.
For a 1000 by 1000 dataframe
selection is over 250 times faster with column-wise storage than with row-wise,
while filtering is 1.8 times slower.
FIXME

\begin{table}
\begin{tabular}{llllll}
\textbf{\underline{nrow}} & \textbf{\underline{ncol}} & \textbf{\underline{filter col}} & \textbf{\underline{select col}} & \textbf{\underline{filter row}} & \textbf{\underline{select row}} \\
10 & 10 & 8.87e-05 & 7.70e-05 & 4.41e-05 & 2.50e-05 \\
100 & 100 & 0.00275 & 4.10e-05 & 0.00140 & 8.76e \\
1000 & 1000 & 0.146 & 0.000189 & 0.0787 & 0.0508 \\
10000 & 10000 & 19.0 & 0.00234 & 9.97 & 5.57 \\
\end{tabular}
\caption{Dataframe timings}
\label{dataframe-timing}
\end{table}



We can get much more insight using Python \hreffoot{cProfile}{https://docs.python.org/3/library/profile.html} module:


\begin{lstlisting}[frame=single,frameround=tttt]
python -m cProfile -s tottime timing.py 1000x1000 | head -n 20
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
nrow,ncol,filter col,select col,filter row,select row
1000,1000,0.18020892143249512,0.0003190040588378906,0.11076593399047852,0.07521200180053711
         2370750 function calls (2368230 primitive calls) in 0.666 seconds

   Ordered by: internal time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
  1836500    0.160    0.000    0.160    0.000 util.py:9(<genexpr>)
        1    0.090    0.090    0.180    0.180 df_col.py:66(filter)
     1000    0.084    0.000    0.084    0.000 timing.py:27(<dictcomp>)
     1000    0.076    0.000    0.076    0.000 df_col.py:71(<dictcomp>)
4842/2342    0.065    0.000    0.278    0.000 {built-in method builtins.all}
     1000    0.058    0.000    0.058    0.000 timing.py:16(<listcomp>)
     2500    0.052    0.000    0.276    0.000 util.py:5(dict_match)
        1    0.036    0.036    0.036    0.036 df_row.py:63(<listcomp>)
     1000    0.020    0.000    0.020    0.000 df_row.py:55(<dictcomp>)
   500005    0.012    0.000    0.012    0.000 {method 'append' of 'list' objects}
        1    0.006    0.006    0.666    0.666 timing.py:1(<module>)
        2    0.001    0.000    0.291    0.145 timing.py:35(time_filter)
        3    0.001    0.000    0.002    0.001 df_col.py:13(__init__)
\end{lstlisting}



Ignoring the first two lines (which are the output of our program),
the table tells us:

\begin{itemize}

\item 

the number of times each function or method was called;



\item 

the total time spent in those calls (which is what we care about most);



\item 

the time spent per call; and



\item 

the cumulative time spent in that call and all the things it calls,
    both per call and in total.



\end{itemize}


Right away we can see that the \texttt{dict\_match} function
that checks the consistency of the rows in a row-oriented dataframe
is eating up a lot of time.
It's only called in the constructor,
but on the other hand,
we're constructing a new dataframe for each \texttt{filter} and \texttt{select},
so removing that check would actually speed things up.


Looking down a little further,
the dictionary comprehension in \texttt{DfCol.filter} takes a lot of time as well.
That isn't surprising:
we're copying the values out of the columns into a temporary dictionary
for every row when we filter,
and building all those temporary dictionaries adds up to a lot of time.


\section{Exercises}\label{dataframe-exercises}

\subsection*{Empty dataframes}


An empty dataframe is as reasonable and as useful as an empty string or an empty list.
\texttt{DfCol} can represent this,
but \texttt{DfRow} cannot:
if the list of dictionaries is empty,
we cannot ask for columns' names.
Derive another dataframe class from \texttt{DF} that uses row-wise storage
but can represent a dataframe with no rows.

\subsection*{Unified constructors}


Modify the constructors of \texttt{DfRow} and \texttt{DfCol} to have the same signatures.
Where and why might this be useful?

\subsection*{Fixture functions}


Read the documentation for the \texttt{@fixture} decorator in \hreffoot{pytest}{https://docs.pytest.org/}
and the modify the tests in this chapter to use it.

\subsection*{Using arrays}


Derive another dataframe class from \texttt{DF}
that uses Python's \hreffoot{array}{https://docs.python.org/3/library/array.html} module for column-wise storage.
How does it performance compared to other implementations?

\subsection*{Crossover}

\begin{enumerate}

\item 

At what ratio of filters to selects are \texttt{DfRow} and \texttt{DfCol} equally fast?
    (Your answer may depend on the size of the dataframe.)



\item 

How does the relative performance of the two classes change
    if tables have a fixed number of columns (such as 10 or 20)
    but an increasing numbers of rows?
    Is this scenario more realistic?



\end{enumerate}

\subsection*{Filtering by strings}


Modify the comparison of filter and select to work with tables
that contain columns of strings instead of columns of numbers
and see how that changes performance.
For testing,
creating random 4-letter strings using the characters A-Z
and then filter by:

\begin{itemize}

\item an exact match,

\item strings starting with a specific character, and

\item strings that contain a specific character

\end{itemize}

\subsection*{Join performance}


A join combines data from two tables based on matching keys.
For example,
if the two tables are:


\vspace{\baselineskip}
\begin{tabular}{ll}
\textbf{\underline{Key}} & \textbf{\underline{Left}} \\
A & a1 \\
B & b1 \\
C & c1 \\
\end{tabular}

\vspace{\baselineskip}


\noindent and:


\vspace{\baselineskip}
\begin{tabular}{ll}
\textbf{\underline{Key}} & \textbf{\underline{Right}} \\
A & a2 \\
A & a3 \\
B & b2 \\
\end{tabular}

\vspace{\baselineskip}


\noindent then the join is:


\vspace{\baselineskip}
\begin{tabular}{lll}
\textbf{\underline{Key}} & \textbf{\underline{Left}} & \textbf{\underline{Right}} \\
A & a1 & a2 \\
A & a1 & a3 \\
B & b1 & b2 \\
\end{tabular}

\vspace{\baselineskip}


Write a test to compare the performance of row-wise vs. column-wise storage
when joining two tables based on matching numeric keys.
Does the answer depend on the fraction of keys that match?

\subsection*{Join optimization}


The simplest way to \glossref{join} two tables is
to look for matching keys using a double loop.
An alternative is to build an \glossref{index} for each table
and then use it to construct matches.
For example, suppose the tables are:


\vspace{\baselineskip}
\begin{tabular}{ll}
\textbf{\underline{Key}} & \textbf{\underline{Left}} \\
A & a1 \\
B & b1 \\
C & c1 \\
\end{tabular}

\vspace{\baselineskip}


\noindent and:


\vspace{\baselineskip}
\begin{tabular}{ll}
\textbf{\underline{Key}} & \textbf{\underline{Right}} \\
A & a2 \\
A & a3 \\
B & b2 \\
\end{tabular}

\vspace{\baselineskip}


The first step is to create a \texttt{Map} showing where each key is found in the first table:

\begin{lstlisting}[frame=single,frameround=tttt]
{A: [0], B: [1], C: [2]}
\end{lstlisting}


\noindent The second step is to create a similar \texttt{Map} for the second table:

\begin{lstlisting}[frame=single,frameround=tttt]
{A: [0, 1], B: [2]}
\end{lstlisting}


\noindent We can then loop over the keys in one of the maps,
look up values in the second map,
and construct all of the matches.


Write a function that joins two tables this way.
Is it faster or slower than using a double loop?
How does the answer depend on the number of keys and the fraction that match?

\chapter{Chapter 6: A Pipeline Runner}\label{pipeline}

\begin{itemize}

\item FIXME

\end{itemize}


\noindent 
    Terms defined:
    \glossref{decorator}, \glossref{overlay configuration}, \glossref{provenance}



Data science is all the rage these days.
No one can agree on exactly what it is,
but everyone's definition includes processing large datasets.
Data scientists usually do this with pipelines
that apply several operations to the data one after another.
In order to automate, share, and check their work,
they want the processing steps to be reproducible:
someone else,
somewhere else,
ought to be able to re-run the exact same steps
without consulting the original author.


This chapter builds a small toolkit for running pipelines
that is easy to extend
and that automatically records the \glossref{provenance}
of each output.
Since this isn't a book about data science,
we will use common text processing operations in our examples.
Please feel free to replace them with complicated mathematics.

\section{Pipelines as Lists}\label{pipeline-list}


Suppose we have five functions:

\begin{itemize}

\item \texttt{read(filename)} reads a CSV file and returns a list of rows,
   each of which is a list of values.

\item \texttt{head(data, num)} returns the first \texttt{num} rows of a dataset.

\item \texttt{tail(data, num)} returns the last \texttt{num} rows.

\item \texttt{left(data, num)} and \texttt{right(data, num)} return
    the first and last \texttt{num} columns of the data respectively.

\end{itemize}


We can combine these functions in two ways.
The first uses nested function calls:

\begin{lstlisting}[frame=single,frameround=tttt]
left(tail(head(read("sample_data.csv"), 10), 5), 2)
\end{lstlisting}


\noindent but this quickly becomes hard to read.
(Quick, is the 10 being passed to \texttt{head} or to \texttt{tail}?)
The second uses temporary variables:

\begin{lstlisting}[frame=single,frameround=tttt]
data = read("sample_data.csv")
data = head(data, 10)
data = tail(frame, 5)
data = left(data, 2)
\end{lstlisting}


\noindent which is easier to read when there are more than a handful of stages,
but can be error-prone.
(Did you notice that the code above is passing \texttt{frame} to \texttt{tail}
instead of \texttt{data}?)


But a function is just another kind of object,
so we can put it in a list along with its arguments like this:


\begin{lstlisting}[frame=single,frameround=tttt]
from functions import *

pipeline = [
    [read, "sample_data.csv"],
    [head, 10],
    [tail, 5],
    [left, 2]
]

for (i, step) in enumerate(pipeline):
    func, params = step[0], step[1:]
    if i == 0:
        data = func(*params)
    else:
        data = func(data, *params)
\end{lstlisting}


\section{Configuration}\label{pipeline-config}


Every function remembers the name it was given when it was defined:


\begin{lstlisting}[frame=single,frameround=tttt]
def proof(x, y):
    pass

print(proof.__name__)
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
proof
\end{lstlisting}



\noindent so we can convert a list of functions into a lookup table:


\begin{lstlisting}[frame=single,frameround=tttt]
from functions import left, right

def make_table(*functions):
    return {f.__name__: f for f in functions}

table = make_table(left, right)
print(table)
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
{'left': <function left at 0x10f938310>, 'right': <function right at 0x10f9383a0>}
\end{lstlisting}



We should therefore be able to construct a pipeline
from a YAML file like this
without needing a long set of \texttt{if} statements
to match names to functions:


\begin{lstlisting}[frame=single,frameround=tttt]
- name: read
  params:
  - sample_data.csv
- name: head
  params:
  - 10
- name: tail
  params:
  - 5
- name: left
  params:
  - 2
\end{lstlisting}



The code that runs the steps described in this pipeline
creates the lookup table
and then calls each stage with the given parameters:


\begin{lstlisting}[frame=single,frameround=tttt]
def pipeline(config_file, *functions):
    functions = {f.__name__: f for f in functions}

    with open(config_file, "r") as reader:
        config = yaml.safe_load(reader)

    data = None
    for stage in config:
        func_name, params = stage["name"], stage["params"]
        func = functions[func_name]
        if data is None:
            data = func(*params)
        else:
            data = func(data, *params)

    return data
\end{lstlisting}


\section{Collecting Functions}\label{pipeline-collect}


Our \texttt{pipeline} function works,
but the call to it isn't any prettier
than the nested function calls we set out to replace:


\begin{lstlisting}[frame=single,frameround=tttt]
result = pipeline(sys.argv[1], read, head, tail, left, right)
\end{lstlisting}



We could move the line that turns a list of functions into a dictionary
out of \texttt{pipeline}
and require the user to build that lookup table for us,
but there's a cleaner way.
Let's define a \glossref{decorator}
that adds a function to a lookup table:


\begin{lstlisting}[frame=single,frameround=tttt]
EXPORTS = {}

def export(func):
    global EXPORTS
    EXPORTS[func.__name__] = func
    return func
\end{lstlisting}



\noindent We can then use this decorator
to mark the functions that we want to make available
to the pipeline:


\begin{lstlisting}[frame=single,frameround=tttt]
@export
def read(filename):
    with open(filename, "r") as reader:
        return [row for row in csv.reader(reader)]

@export
def head(data, num):
    return data[:num]
\end{lstlisting}



\noindent and then in our pipeline code
we can import the \texttt{EXPORTS} table
(renaming it to make its purpose a little clearer):


\begin{lstlisting}[frame=single,frameround=tttt]
from decorated import EXPORTS as functions
\end{lstlisting}



\noindent The call to \texttt{pipeline} is then:


\begin{lstlisting}[frame=single,frameround=tttt]
result = pipeline(sys.argv[1], functions)
\end{lstlisting}



If we want to make functions from many files
available to our pipeline,
we'll have to merge their exported dictionaries.
We'll explore this in the exercises.

\section{Configuration and Provenance}\label{pipeline-provenance}


Our modified pipeline also doesn't allow for any global configuration
(i.e., parameters that are shared between stages)
and doesn't keep a record of what it actually did.
We will tackle these problems together.


First,
we want to be able to pass some global parameters to all stages of the pipeline
as well as local (per-stage) parameters.
When we're done,
our configuration file will like this:


\begin{lstlisting}[frame=single,frameround=tttt]
overall:
  debug: true
pipeline:
  - name: read
    params:
    - sample_data.csv
  - name: head
    params:
    - 10
  - name: tail
    params:
    - 5
  - name: left
    params:
    - 2
\end{lstlisting}



We don't want our generic \texttt{pipeline} function
to have to know about the functions it runs,
so let's modify all of the latter to take optional arguments
as shown below:


\begin{lstlisting}[frame=single,frameround=tttt]
@export
def read(filename, **kwargs):
    with open(filename, "r") as reader:
        return [row for row in csv.reader(reader)]

@export
def head(data, num, **kwargs):
    return data[:num]
\end{lstlisting}



Our revised \texttt{pipeline} function is now:


\begin{lstlisting}[frame=single,frameround=tttt]
def pipeline(config_file, functions):
    with open(config_file, "r") as reader:
        config = yaml.safe_load(reader)
    overall = config.get("overall", {})

    data = None
    for stage in config["pipeline"]:
        func_name, params = stage["name"], stage["params"]
        func = functions[func_name]
        if data is None:
            data = func(*params, **overall)
        else:
            data = func(data, *params, **overall)

    return data
\end{lstlisting}



We also want to know what each run of a pipeline actually did.
Let's rewrite the runner to create a list of provenance records:


\begin{lstlisting}[frame=single,frameround=tttt]
def pipeline(config_file, functions):
    with open(config_file, "r") as reader:
        config = yaml.safe_load(reader)
    overall = config.get("overall", {})

    data = None
    provenance = []
    for stage in config["pipeline"]:
        func_name, params = stage["name"], stage["params"]
        func = functions[func_name]
        data, info = run(functions, data, func_name, params, overall)
        provenance.append(info)

    return data, provenance
\end{lstlisting}



The \texttt{run} function finds and runs the requested function,
then records its name,
its parameters,
how long it took to execute,
and how big its output was:


\begin{lstlisting}[frame=single,frameround=tttt]
def run(functions, data, func_name, params, overall):
    info = {
        "name": func_name,
        "params": params
    }
    start_time = time.time()
    func = functions[func_name]
    if data is None:
        data = func(*params, **overall)
    else:
        data = func(data, *params, **overall)
    info["time"] = time.time() - start_time
    info["size"] = data_size(data)
    return data, info

def data_size(data):
    return [0, 0] if not data else [len(data), len(data[0])]
\end{lstlisting}



If we run this and dump the provenance record as YAML we get:


\begin{lstlisting}[frame=single,frameround=tttt]
- name: read
  params:
  - sample_data.csv
  size:
  - 15
  - 10
  time: 8.511543273925781e-05
- name: head
  params:
  - 10
  size:
  - 10
  - 10
  time: 1.9073486328125e-06
- name: tail
  params:
  - 5
  size:
  - 5
  - 10
  time: 9.5367431640625e-07
- name: left
  params:
  - 2
  size:
  - 5
  - 2
  time: 4.0531158447265625e-06
\end{lstlisting}



Let's come back to configuration.
Right now we have put everything for our pipeline in one self-contained file,
but in real situations our pipelines will probably share a lot of settings.
Many large applicatons allow up to four layers of configuration:

\begin{enumerate}

\item 

A system-wide configuration file for general settings.



\item 

A user-specific configuration file for personal preferences.



\item 

A job-specific file with settings for a particular run.



\item 

Command-line options to change things that commonly change.



\end{enumerate}


This is sometimes called \glossref{overlay configuration}
because each level overrides the ones before it.
It's more complex that our example needs,
but is worth building to see how it's done:


\begin{lstlisting}[frame=single,frameround=tttt]
import os
from pathlib import Path
import yaml

ENV_VAR = "MCCOLE_CONFIG"
HOME = "HOME"
CONFIG_FILE = ".mccole"

def load_config(filename):
    return _load_system() | load_user() | _load_file(filename)

def _load_system():
    var = os.getenv(ENV_VAR)
    return _load_file(var) if var else {}

def _load_user():
    home = os.getenv(HOME)
    if not home:
        return {}
    filename = Path(home, CONFIG_FILE)
    if not filename.exists():
        return {}
    return _load_file(filename)

def _load_file(filename):
    with open(filename, "r") as reader:
        return yaml.safe_load(reader)
\end{lstlisting}


\section{Exercises}\label{pipeline-exercises}

\subsection*{Merging functions}


Modify the pipeline code so that it can load runnable functions from many different files.
What should you do if two or more files export functions with the same names?

\subsection*{Error handling}


Our pipeline doesn't catch exceptions or do any other error handling.
Modify it so that it does.

\chapter{Chapter 7: A Build Manager}\label{builder}

\begin{itemize}

\item FIXME

\end{itemize}


\noindent 
    Terms defined:
    \glossref{automatic variable}, \glossref{build manager}, \glossref{build recipe}, \glossref{build rule}, \glossref{comma-separated values}, \glossref{compiled language}, \glossref{dependency (in build)}, \glossref{directed acyclic graph}, \glossref{JavaScript Object Notation}, \glossref{link (a program)}, \glossref{pattern rule (in build)}, \glossref{stale (in build)}, \glossref{target (in build)}, \glossref{topological order}, \glossref{Yet Another Markup Language}



Programs in \glossref{compiled languages}\index{compiled language}\index{language!compiled}
like C\index{C} and Java\index{Java}
have to be translated into lower-level forms before they can run.
There are usually two stages to the translation:
compiling each source file into some intermediate form,
and then \glossref{linking}\index{linking (compiled language)}\index{compiled language!linking} the compiled modules
to each other and to libraries
to create a runnable program
(\figref{builder-compiling}).

\figpdf{builder-compiling}{./builder/compiling.pdf}{Compiling source files and linking the resulting modules.}{0.6}


If a source file hasn't changed,
there's no need to recompile it before linking.
A \glossref{build manager}\index{build manager} takes a description of what depends on what,
figures out which files are out of date,
determines an order in which to rebuild things,
and then does whatever is required and only what is required \cite{Smith2011}.


The first build manager,
\hreffoot{Make}{https://www.gnu.org/software/make/}\index{Make},
was created to handle compilation of C programs,
but build managers are used to update packages,
regenerate websites (\chapref{templating}),
or re-create documentation from source code (\chapref{docgen}).
This chapter creates a simple build manager to illustrate how they work.

\section{Structure}\label{builder-structure}


The input to a build manager is a set of rules,
each of which has:

\begin{itemize}

\item 

a \glossref{target}\index{build target}\index{target!build},
    which is the file to be updated;



\item 

some \glossref{dependencies}\index{dependency (in build)}\index{build!dependency},
    which are the things that file depends on;
    and



\item 

a \glossref{recipe}\index{recipe (in build)}\index{build!recipe}
    that specifies how to update the target
    if it is out of date compared to its dependencies.



\end{itemize}


The target of one rule can be a dependency of another rule,
so their relationships form a \glossref{directed acyclic graph}\index{directed acyclic graph (DAG)}\index{DAG}
(\figref{builder-dependencies}).
The graph is directed because "A depends on B" is a one-way relationship,
while "acyclic" means it cannot contains loops:
if something depends on itself we can never finish updating it.

\figpdf{builder-dependencies}{./builder/dependencies.pdf}{How a build manager finds and respects dependencies.}{0.6}


We say that a target is \glossref{stale}\index{stale (in build)}\index{build!stale}
if it is older than any of its dependencies.
When this happens,
we use the recipes to bring it up to date.
Our build manager therefore must:

\begin{enumerate}

\item 

Read a file containing rules.



\item 

Construct the dependency graph.



\item 

Figure out which targets are stale.



\item 

Build those targets,
    making sure to build things \emph{before} anything that depends on them is built.



\end{enumerate}


\section{Representing Rules}\label{builder-rules}


We will store our rules in \glossref{YAML} files like this:


\begin{lstlisting}[frame=single,frameround=tttt]
- target: A
  depends:
  - B
  - C
  recipes:
  - "update A from B and C"
- target: B
  depends:
  - C
  recipes:
  - "update B from C"
- target: C
  depends: []
  recipes: []
\end{lstlisting}



\noindent We could equally well have used \glossref{JSON},
but it wouldn't have made sense to use \glossref{CSV}:
rules have a nested structure,
and CSV doesn't represent nesting gracefully.


We would normally implement all of the methods required by the builder at once,
but to make the evolving code easier to follow we will write them them one by one.
Let's start by writing a class that loads a configuration file:


\begin{lstlisting}[frame=single,frameround=tttt]
import yaml

class ConfigLoader:
    def __init__(self, rules_file):
        self.rules_file = rules_file

    def build(self):
        self.load_config()

    def load_config(self):
        with open(self.rules_file, "r") as reader:
            self.rules = yaml.load(reader, Loader=yaml.FullLoader)

        assert isinstance(self.rules, list), \
            "Configuration must be array"

        for rule in self.rules:
            self._check(rule)
\end{lstlisting}



We need to check each rule because YAML is a generic file format
that doesn't know anything about the extra requirements of our rules:


\begin{lstlisting}[frame=single,frameround=tttt]
    def _check(self, rule):
        """Check a single rule."""
        assert ("target" in rule) and isinstance(rule["target"], str), \
            f"Rule {rule} does not have 'target'"

        assert ("depends" in rule) and \
            isinstance(rule["depends"], list) and \
            all(isinstance(dep, str) for dep in rule["depends"]), \
            f"Bad 'depends' for rule {rule}"

        assert ("recipes" in rule) and \
            isinstance(rule["recipes"], list) and \
            all(isinstance(r, str) for r in rule["recipes"]), \
            f"Bad 'recipes' for rule {rule}"
\end{lstlisting}



The next step is to turn the configuration into a graph in memory.
We derive a class from \texttt{ConfigLoader} so that we can recycle the code we've already written
and call a couple of methods that we are planning to add:


\begin{lstlisting}[frame=single,frameround=tttt]
import networkx as nx
from networkx.readwrite import json_graph

from config_loader import ConfigLoader

class GraphCreator(ConfigLoader):

    def build(self):
        self.load_config()
        self.build_graph()
        self.check_cycles()
\end{lstlisting}



We use the \hreffoot{networkx}{https://networkx.org/} module to manage nodes and links
rather than writing our own classes for graphs,
and store the recipe to rebuild a node in that node:


\begin{lstlisting}[frame=single,frameround=tttt]
    def build_graph(self):
        self.graph = nx.DiGraph()

        for rule in self.rules:
            self.graph.add_node(rule["target"], recipes=rule["recipes"])

        for rule in self.rules:
            for d in rule["depends"]:
                self.graph.add_edge(d, rule["target"])
\end{lstlisting}



\texttt{networkx} provides implementations of some common graph algorithms,
including one to find cycles,
so let's write that next:


\begin{lstlisting}[frame=single,frameround=tttt]
    def check_cycles(self):
        cycles = list(nx.algorithms.simple_cycles(self.graph))
        assert len(cycles) == 0, \
            f"Dependency graph contains cycles {cycles}"
\end{lstlisting}



Finally,
we write a \texttt{\_\_str\_\_} method so we can see what we've built:


\begin{lstlisting}[frame=single,frameround=tttt]
    def __str__(self):
        temp = json_graph.node_link_data(self.graph)
        temp = {k:v for (k, v) in temp.items() if k in {"nodes", "links"}}
        return json.dumps(temp, indent=2)
\end{lstlisting}



When we run this with our three simple rules as input we get:


\begin{lstlisting}[frame=single,frameround=tttt]
python graph_creator.py three_simple_rules.yml
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
{
  "nodes": [
    {
      "recipes": [
        "update A from B and C"
      ],
      "id": "A"
    },
    {
      "recipes": [
        "update B from C"
      ],
      "id": "B"
    },
    {
      "recipes": [],
      "id": "C"
    }
  ],
  "links": [
    {
      "source": "B",
      "target": "A"
    },
    {
      "source": "C",
      "target": "A"
    },
    {
      "source": "C",
      "target": "B"
    }
  ]
}
\end{lstlisting}



Let's write a quick test to make sure the cycle detector works as intended:


\begin{lstlisting}[frame=single,frameround=tttt]
- target: A
  depends:
  - B
  recipes:
  - "update A from B"
- target: B
  depends:
  - A
  recipes:
  - "update B from A"
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
python graph_creator.py circular_rules.yml
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]

\end{lstlisting}


\section{Stale Files}\label{builder-timestamp}


The next step is to figure out which files are out of date.
Make does this by comparing the timestamps\index{timestamp!in build}\index{build!timestamp} of the files in question,
but this isn't always reliable:
computers' clocks may be slightly out of sync\index{clock synchronization (in build)}\index{build!clock synchronization},
which can produce a wrong answer on a networked filesystem,
and the operating system may only report file update times to the nearest millisecond
(which seemed very short in 1970 but seems very long today).


More modern build systems store a hash\index{hash code!in build}\index{build!hash code} of each file's contents
and compare the current hash to the stored one to see if the file has changed.
We looked at hashing in \chapref{backup},
so we will use the timestamp approach here,
but instead of using a mock filesystem
we will load another configuration file that specifies fake timestamps for files:


\begin{lstlisting}[frame=single,frameround=tttt]
A: 2
B: 5
C: 8
\end{lstlisting}



Since we want to associate those timestamps with files,
we add steps to the constructor and the \texttt{build} method
to read the timestamp file and add information to the graph's nodes:


\begin{lstlisting}[frame=single,frameround=tttt]
class AddTimestamps(GraphCreator):

    def __init__(self, rules_file, times_file):
        super().__init__(rules_file)
        self.times_file = times_file

    def build(self):
        self.load_config()
        self.build_graph()
        self.add_timestamps()
        self.check_cycles()
\end{lstlisting}



\noindent and then implement \texttt{add\_timestamps}:


\begin{lstlisting}[frame=single,frameround=tttt]
    def add_timestamps(self):
        with open(self.times_file, "r") as reader:
            times = yaml.load(reader, Loader=yaml.FullLoader)
        missing = {n for n in self.graph.nodes} - {n for n in times.keys()}
        assert not missing, \
            f"Name(s) missing from times: {', '.join(missing)}"
        nx.set_node_attributes(self.graph, times, "timestamp")
\end{lstlisting}



Before we move on,
let's make sure that adding timestamps works as we want:


\begin{lstlisting}[frame=single,frameround=tttt]
python add_timestamps.py three_simple_rules.yml add_timestamps.yml
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
{
  "nodes": [
    {
      "recipes": [
        "update A from B and C"
      ],
      "timestamp": 2,
      "id": "A"
    },
    {
      "recipes": [
        "update B from C"
      ],
      "timestamp": 5,
      "id": "B"
    },
    {
      "recipes": [],
      "timestamp": 8,
      "id": "C"
    }
  ],
  "links": [
    {
      "source": "B",
      "target": "A"
    },
    {
      "source": "C",
      "target": "A"
    },
    {
      "source": "C",
      "target": "B"
    }
  ]
}
\end{lstlisting}


\section{Updating Files}\label{builder-update}


To figure out which recipes to execute and in which order,
we set the pretended current time to the latest time of any file,
then look at each file in topological order.
If a file is older than any of its dependencies,
we update the file \emph{and} its pretended timestamp
to trigger an update of anything that depends on it.
First,
we add a step to \texttt{build}:


\begin{lstlisting}[frame=single,frameround=tttt]
class UpdateTimestamps(AddTimestamps):

    def build(self):
        self.load_config()
        self.build_graph()
        self.add_timestamps()
        self.check_cycles()
        self.run()
\end{lstlisting}



Next,
we implement the \texttt{run} method.
We can pretend that updating a file always takes one unit of time,
so we advance our fictional clock by one for each build.
Using \texttt{networkx.topological\_sort} to create the topological order,
we get this:


\begin{lstlisting}[frame=single,frameround=tttt]
    def run(self):
        times = nx.get_node_attributes(self.graph, "timestamp")
        current_time = 1 + max(times.values())
        print(f"{current_time}: START")
        for name in reversed(list(nx.topological_sort(self.graph))):
            if self.is_stale(name):
                self.update(name, current_time)
                self.graph.nodes[name]["timestamp"] = current_time
                current_time += 1
        print(f"{current_time}: END")
\end{lstlisting}



The \texttt{run} method:

\begin{enumerate}

\item 

Gets a sorted list of nodes.



\item 

Sets the starting time to be one unit past the largest file time.



\item 

Checks each file in order.
    If that file is stale,
    we print the steps we would run and then update the file's timestamp.
    We only advance the notional current time when we do an update.



\end{enumerate}


In order to check if a file is stale,
we see if any of its dependencies currently have timestamps
greater than or equal to the target's timestamp:


\begin{lstlisting}[frame=single,frameround=tttt]
    def is_stale(self, name):
        return any(
            self.graph.nodes[p]["timestamp"] >= self.graph.nodes[name]["timestamp"]
            for p in self.graph.predecessors(name)
        )
\end{lstlisting}



Our \texttt{update} method simply prints the actions it would take:


\begin{lstlisting}[frame=single,frameround=tttt]
    def update(self, name, time):
        print(f"- {name} ({time}):")
        for r in self.graph.nodes[name]["recipes"]:
            print(f"  - {r}")
\end{lstlisting}



When we run this,
it seems to do the right thing:


\begin{lstlisting}[frame=single,frameround=tttt]
python update_timestamps.py three_simple_rules.yml add_timestamps.yml
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
9: START
- A (9):
  - update A from B and C
- B (10):
  - update B from C
11: END
\end{lstlisting}


\section{Variables}\label{builder-variables}


We don't want to have to write a hundred nearly-identical recipes
if our website has a hundred blog posts
or a hundred pages of documentation.
Instead,
we want to be able to write generic \glossref{build rules}\index{build!rule}\index{rule (in build)}.
To do this we need:

\begin{itemize}

\item 

a way to define a set of files;



\item 

a way to specify a generic rule;
    and



\item 

a way to fill in parts of that rule.



\end{itemize}


We will achieve this by overriding \texttt{build\_graph} to replace variables in recipes with values.
Once again,
object-oriented programming helps us change only what we need to change,
provided we divided our problem into sensible chunks in the first place.


Make provides
\glossref{automatic variables}\index{automatic variable (in build)}\index{build!automatic variable}
with names like \texttt{\$<} and \texttt{\$@}
to represent the parts of a rule.
Our variables will be more readable:
we will use \texttt{@TARGET} for the target,
\texttt{@DEPENDENCIES} for the dependencies (in order),
and \texttt{@DEP[1]}, \texttt{@DEP[2]}, and so on for specific dependencies
(\figref{builder-pattern-rules}).

\figpdf{builder-pattern-rules}{./builder/pattern_rules.pdf}{Turning patterns rules into runnable commands.}{0.6}


Our variable expander looks like this:


\begin{lstlisting}[frame=single,frameround=tttt]
    def expand_variables(self):
        for target in self.graph.nodes:
            self.expand_one(target)

    def expand_one(self, target):
        dependencies = list(self.graph.predecessors(target))
        recipes = self.graph.nodes[target]["recipes"]
        for (ir, recipe) in enumerate(recipes):
            result = recipe\
                .replace("@TARGET", target)\
                .replace("@DEPENDENCIES", " ".join(dependencies))
            for (id, d) in enumerate(dependencies):
                result = result.replace(f"@DEP[{id+1}]", d)
            self.graph.nodes[target]["recipes"][ir] = result
\end{lstlisting}



The first thing we do is test that it works when there \emph{aren't} any variables to expand
by running it on the same example we used previously:


\begin{lstlisting}[frame=single,frameround=tttt]
python expand_variables.py three_simple_rules.yml add_timestamps.yml
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
9: START
- A (9):
  - update A from B and C
- B (10):
  - update B from C
11: END
\end{lstlisting}



\noindent This is perhaps the most important reason to create tests:
they tell us if something we have added or changed
has broken something that used to work
so that we have a solid base for new code.


\begin{lstlisting}[frame=single,frameround=tttt]
- target: A
  depends:
  - B
  - C
  recipes:
  - "update @TARGET from @DEPENDENCIES"
- target: B
  depends:
  - C
  recipes:
  - "update @TARGET from @DEP[1]"
- target: C
  depends: []
  recipes: []
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
python expand_variables.py three_variable_rules.yml add_timestamps.yml
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
9: START
- A (9):
  - update A from B C
- B (10):
  - update B from C
11: END
\end{lstlisting}


\section{Generic Rules}\label{builder-generic}


Now we need to add \glossref{pattern rules}\index{pattern rule (in build)}\index{build!pattern rule}:
Our test rules file is:


\begin{lstlisting}[frame=single,frameround=tttt]
- target: left.out
  depends: []
  recipes: []
- target: left.in
  depends: []
  recipes: []
- target: right.out
  depends: []
  recipes: []
- target: right.in
  depends: []
  recipes: []
- target: "%.out"
  depends:
  - "%.in"
  recipes:
  - "update @TARGET from @DEPENDENCIES"
\end{lstlisting}



\noindent and our first attempt at reading it extracts rules before expanding variables:


\begin{lstlisting}[frame=single,frameround=tttt]
class PatternAttempt(ExpandVariables):

    def build(self):
        self.load_config()
        self.build_graph()
        self.extract_rules()
        self.expand_rules()
        self.expand_variables()
        self.add_timestamps()
        self.check_cycles()
        self.run()

    def extract_rules(self):
        self.rules = {}
        for target in self.graph.nodes:
            if "%" in target:
                self.rules[target] = self.graph.nodes[target]["recipes"]
        for name in self.rules:
            self.graph.remove_node(name)
\end{lstlisting}



However,
it doesn't work:


\begin{lstlisting}[frame=single,frameround=tttt]
python pattern_attempt.py pattern_rules.yml pattern_times.yml
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
Traceback (most recent call last):
  File "/u/sdpy/builder/pattern_attempt.py", line 33, in <module>
    builder.build()
  File "/u/sdpy/builder/pattern_attempt.py", line 9, in build
    self.extract_rules()
  File "/u/sdpy/builder/pattern_attempt.py", line 20, in extract_rules
    self.rules[target] = self.graph.nodes[target]["recipes"]
KeyError: 'recipes'
\end{lstlisting}



After a bit of poking around we realize that
we're looking at the rule for \texttt{\%.in}.
A bit more poking around and we realize that
when we created edges in the graph between a target and its dependencies,
\texttt{networkx} automatically added a node for the dependency
if one didn't exist yet.
As a result,
when we say that \texttt{\%.out} depends on \texttt{\%.in},
we wind up with a node for \texttt{\%.in} that doesn't have any recipes.


We can fix our problem by changing the \texttt{build\_graph} method
so that it saves pattern rules in a dictionary
and then builds the graph from the non-pattern rules:


\begin{lstlisting}[frame=single,frameround=tttt]
    def build_graph(self):
        self.patterns = {
            r["target"]:r for r in self.rules
            if "%" in r["target"]
        }
        self.rules = [r for r in self.rules if "%" not in r["target"]]
        super().build_graph()
\end{lstlisting}



Expanding rules relies on two helper methods:


\begin{lstlisting}[frame=single,frameround=tttt]
    def expand_rules(self):
        for target in self.graph.nodes:
            if (rule := self.find_rule(target)):
                self.fill_in(target, rule)
\end{lstlisting}



The first helper finds rules:


\begin{lstlisting}[frame=single,frameround=tttt]
    def find_rule(self, target):
        if "." not in target:
            return None
        suffix = target.split(".")[-1]
        key = f"%.{suffix}"
        return self.patterns.get(key, None)
\end{lstlisting}



\noindent and the second adds links and recipes to the graph:


\begin{lstlisting}[frame=single,frameround=tttt]
    def fill_in(self, target, rule):
        stem = target.split(".")[0]
        self.graph.nodes[target]["recipes"] = rule["recipes"]
        depends = [d.replace("%", stem) for d in rule["depends"]]
        for d in depends:
            self.graph.add_edge(d, target)
\end{lstlisting}



We're finally ready to test:


\begin{lstlisting}[frame=single,frameround=tttt]
python pattern_final.py pattern_rules.yml pattern_times.yml
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
4: START
- right.out (4):
  - update left.out from left.in
- left.out (5):
  - update left.out from left.in
6: END
\end{lstlisting}


\section{Discussion}\label{builder-discuss}


We have added a lot of steps to our original template method,
which makes it a bit of a stretch to claim that the overall operation hasn't changed.
Knowing what we know now,
we could go back and modify the original \texttt{SkeletonBuilder.build} method
to include those extra steps and provide do-nothing implementations.


The root of the problem is that we didn't anticipate all the steps that would be involved
when we wrote our template method.
It typically takes a few child classes for this to settle down;
if it never does,
then Template Method\index{Template Method pattern}\index{design pattern!Template Method} is probably the wrong pattern for our situation.
Realizing this isn't a failure in initial design:
we always learn about our problem as we try to capture it in code,
and if we know enough to anticipate 100\% of the issues that are going to come up,
it's time to put what we've learned in a library for future use.

\section{Exercises}\label{builder-exercises}

\subsection*{Handle failure}

\begin{enumerate}

\item 

Modify the build manager to accommodate build steps that fail.



\item 

Write tests to check that this change works correctly.



\end{enumerate}

\subsection*{Merge files}


Modify the build manager so that it can read multiple build files
and execute their combined rules.

\subsection*{Conditional execution}


Modify the build manager so that:

\begin{enumerate}

\item 

The user can pass \texttt{variable=true} and \texttt{variable=false} arguments on the command-line
    to define variables.



\item 

Rules can contain an \texttt{if: variable} field.



\item 

Those rules are only executed if the variable is defined and true.



\end{enumerate}


Write tests to check that this works correctly.

\subsection*{Define filesets}


Modify the build manager so that users can define sets of files:

\begin{lstlisting}[frame=single,frameround=tttt]
fileset:
  name: everything
  contains:
    - X
    - Y
    - Z
\end{lstlisting}


\noindent and then refer to them later:

\begin{lstlisting}[frame=single,frameround=tttt]
- target: P
  depends:
  - @everything
\end{lstlisting}

\subsection*{Globbing}


Modify the build manager so that it can dynamically construct a set of files:

\begin{lstlisting}[frame=single,frameround=tttt]
glob:
  name: allAvailableInputs
  pattern: "./*.in"
\end{lstlisting}


\noindent and then refer to them later:

\begin{lstlisting}[frame=single,frameround=tttt]
- target: P
  depends:
  - @allAvailableInputs
\end{lstlisting}

\subsection*{Use hashes}

\begin{enumerate}

\item 

Write a program called \texttt{build\_init.py} that calculates a hash
    for every file mentioned in the build configuration
    and stores the hash along with the file's name in \texttt{build\_hash.json}.



\item 

Modify the build manager to compare the current hashes of files
    with those stored in \texttt{build\_hash.json}
    in order to determine what is out of date,
    and to update \texttt{build\_hash.json} each time it runs.



\end{enumerate}

\chapter{Chapter 8: Matching Regular Expressions}\label{matching}

\begin{itemize}

\item Use regular expressions to match patterns in text and extra data.

\item Have objects control other objects using the Chain of Responsibility design pattern.

\item Use inheritance to make matchers composable.

\end{itemize}


\noindent 
    Terms defined:
    \glossref{base class}, \glossref{Chain of Responsibility pattern}, \glossref{eager matching}, \glossref{greedy algorithm}, \glossref{lazy matching}, \glossref{Open-Closed Principle}, \glossref{polymorphism}



Sooner or later everyone needs to scrape data out of text files.
Regular expressions are the best tool for the job,
so this chapter explores how they work by building a simple but extensible pattern matcher.
Our approach is inspired by Brian Kernighan's\index{Kernighan, Brian} entry
in \cite{Oram2007}.

\section{Simple Patterns}\label{matching-simple}


Our matcher will initially handle just the five cases shown in
\tblref{pattern-matching-cases}.
These cases are a small subset of what Python's \texttt{re} module provides,
but as Kernighan\index{Kernighan, Brian} wrote,
"This is quite a useful class;
in my own experience of using regular expressions on a day-to-day basis,
it easily accounts for 95 percent of all instances."

\begin{table}
\begin{tabular}{ll}
\textbf{\underline{Meaning}} & \textbf{\underline{Character}} \\
Any literal character \emph{c} & \emph{c} \\
Any single character & . \\
Beginning of input & {\textasciicircum} \\
End of input & \$ \\
Zero or more of something & * \\
\end{tabular}
\caption{Pattern matching cases.}
\label{pattern-matching-cases}
\end{table}



Matching is conceptually simple.
If the first element of the pattern matches where we are,
we see if the rest of the pattern matches what's left;
otherwise,
we see if the the pattern will match further along.
The function users call handles the special case of \texttt{{\textasciicircum}} at the start of a pattern
matching the start of the target string being searched.
It then tries the pattern against each successive substring of the target string
until it finds a match or runs out of characters:


\begin{lstlisting}[frame=single,frameround=tttt]
def match(pattern, text):
    if not pattern:
        return True

    # "^" at start of pattern matches start of text.
    if (pattern[0] == "^"):
        return match_here(pattern, 1, text, 0)

    # Try all possible starting points for pattern.
    # We need a do-while loop to handle the case of
    # matching an empty string.
    i_text = 0
    while True:
        if (match_here(pattern, 0, text, i_text)):
            return True
        i_text += 1
        if i_text == len(text):
            break

    # Nothing worked.
    return False
\end{lstlisting}



\texttt{match\_here} does the matching and recursing:


\begin{lstlisting}[frame=single,frameround=tttt]
def match_here(pattern, i_pattern, text, i_text):
    # There is no more pattern to match.
    if i_pattern == len(pattern):
        return True

    # "$" at end of pattern matches end of text.
    if ((i_pattern == (len(pattern) - 1)) and (pattern[i_pattern] == "$") and (i_text == len(text))):
        return True

    # "*" following current character means match many.
    if (((len(pattern) - i_pattern) > 1) and (pattern[i_pattern + 1] == "*")):
        while ((i_text < len(text)) and (text[i_text] == pattern[i_pattern])):
            i_text += 1
        return match_here(pattern, i_pattern + 2, text, i_text)

    # There is no more text to match.
    if i_text == len(text):
        return False

    # Match a single character.
    if ((pattern[i_pattern] == ".") or (pattern[i_pattern] == text[i_text])):
        return match_here(pattern, i_pattern + 1, text, i_text + 1)

    # Nothing worked.
    return False
\end{lstlisting}



We use a table of test cases and expected results to test it:


\begin{lstlisting}[frame=single,frameround=tttt]
def main():
    tests = [
        ["", "", True],
        ["a", "a", True],
        ["b", "a", False],
        ["a", "ab", True],
        ["b", "ab", True],
        ["ab", "ba", False],
        ["^a", "ab", True],
        ["^b", "ab", False],
        ["a$", "ab", False],
        ["a$", "ba", True],
        ["a*", "", True],
        ["a*", "baac", True],
        ["ab*c", "ac", True],
        ["ab*c", "abc", True],
        ["ab*c", "abbbc", True],
        ["ab*c", "abxc", False]
    ]
    for (regexp, text, expected) in tests:
        actual = match(regexp, text)
        result = "pass" if actual == expected else "fail"
        print(f"'{regexp}' X '{text}' == {actual}: {result}")

main()
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
'' X '' == True: pass
'a' X 'a' == True: pass
'b' X 'a' == False: pass
'a' X 'ab' == True: pass
'b' X 'ab' == True: pass
'ab' X 'ba' == False: pass
'^a' X 'ab' == True: pass
'^b' X 'ab' == False: pass
'a$' X 'ab' == False: pass
'a$' X 'ba' == True: pass
'a*' X '' == True: pass
'a*' X 'baac' == True: pass
'ab*c' X 'ac' == True: pass
'ab*c' X 'abc' == True: pass
'ab*c' X 'abbbc' == True: pass
'ab*c' X 'abxc' == False: pass
\end{lstlisting}



This program seems to work,
but it actually contains an error that we will correct in the exercises.
(Think about what happens if we match the pattern \texttt{a*ab} against the string \texttt{{\textquotesingle}aab{\textquotesingle}}.)
It is also hard to extend:
\texttt{match\_here} is already very complicated,
and handling parentheses in patterns like \texttt{a(bc)*d} will make it more complicated still.

\section{A Direct Matcher}\label{matching-direct}


Instead of packing all our code into one function,
we can implement each kind of match separately.
Doing this makes it easier to add more matchers:
we just define something we can mix in with the matchers we already have.


Rather than matching text immediately,
though,
we will create objects that know how to do matches
so that we can build a complex matcher once and re-use it many times.
This is a common pattern in text processing:
we may want to apply a regular expression to each line in a large set of files,
so recycling matchers makes programs more efficient.


Each matching object has a method
that takes the target string and the index to start matching at as inputs.
Its output is the index to continue matching at
or \texttt{None} indicating that matching failed.
We can combine these objects to match complex patterns.


The first step to implement this is to write test cases,
which forces us to define how our classes will work:


\begin{lstlisting}[frame=single,frameround=tttt]
def main():
    tests = [
        ["a", "a", True, Lit("a")],
        ["b", "a", False, Lit("b")],
        ["a", "ab", True, Lit("a")],
        ["b", "ab", True, Lit("b")],
        ["ab", "ab", True, Seq(Lit("a"), Lit("b"))],
        ["ba", "ab", False, Seq(Lit("b"), Lit("a"))],
        ["ab", "ba", False, Lit("ab")],
        ["^a", "ab", True, Seq(Start(), Lit("a"))],
        ["^b", "ab", False, Seq(Start(), Lit("b"))],
        ["a$", "ab", False, Seq(Lit("a"), End())],
        ["a$", "ba", True, Seq(Lit("a"), End())],
        ["a*", "", True, Any("a")],
        ["a*", "baac", True, Any("a")],
        ["ab*c", "ac", True, Seq(Lit("a"), Any("b"), Lit("c"))],
        ["ab*c", "abc", True, Seq(Lit("a"), Any("b"), Lit("c"))],
        ["ab*c", "abbbc", True, Seq(Lit("a"), Any("b"), Lit("c"))],
        ["ab*c", "abxc", False, Seq(Lit("a"), Any("b"), Lit("c"))],
        ["ab|cd", "xaby", True, Alt(Lit("ab"), Lit("cd"))],
        ["ab|cd", "acdc", True, Alt(Lit("ab"), Lit("cd"))],
        ["a(b|c)d", "xabdy", True, Seq(Lit("a"), Alt(Lit("b"), Lit("c")), Lit("d"))],
        ["a(b|c)d", "xabady", False, Seq(Lit("a"), Alt(Lit("b"), Lit("c")), Lit("d"))]
    ]
    for (pattern, text, expected, matcher) in tests:
        actual = matcher.match(text)
        result = "pass" if actual == expected else "fail"
        print(f"'{pattern}' X '{text}' == {actual}: {result}")
\end{lstlisting}



Next,
we define a \glossref{base class} that all matchers will inherit from.
This class contains the \texttt{match} method that users will call
so that we can start matching right away
no matter what kind of matcher we have at the top level of our pattern.


\begin{lstlisting}[frame=single,frameround=tttt]
class MatchBase:
    def __init__(self):
        pass

    def match(self, text, start=0):
        return None
\end{lstlisting}



\noindent The base class defines a \texttt{match} method
that other classes will fill in with actual matching code.
The base implementation of this method always returns \texttt{None}
so that if we forget to implement it in a derived class,
matching will always fail.


We can now define each matching class,
like this one for literal characters:


\begin{lstlisting}[frame=single,frameround=tttt]
class Lit(MatchBase):
    def __init__(self, chars):
        super().__init__()
        self.chars = chars

    def match(self, text, start=0):
        nextIndex = start + len(self.chars)
        if nextIndex > len(text):
            return None
        if text[start:nextIndex] != self.chars:
            return None
        return nextIndex
\end{lstlisting}



\noindent Our tests now run, but most of them fail:
"most" because we expect some tests not to match,
so the test runner reports \texttt{true}.
This output tells us how much work we have left to do:
when all of these tests pass,
we're finished.


What about repetition?
It can just apply a pattern over and over to consume as many matches as possible—or can it?
Suppose we have the pattern \texttt{a*ab}.
This ought to match the text \texttt{"ab"}, but will it?
\texttt{*} is \glossref{greedy}\index{greedy algorithm}\index{algorithm!greedy}:
it matches as much as it can.
(This is also called \glossref{eager matching}\index{eager matching}\index{matching!eager}.)
As a result,
\texttt{a*} will match the leading \texttt{"a"}, leaving nothing for the literal \texttt{a} to match.
Our current implementation doesn't give us a way to try other possible matches when this happens.

\section{An Alternative Design}\label{matching-alternative}


Let's re-think our design
and have each matcher take its own arguments and a \texttt{rest} parameter containing the rest of the matchers
(FIXME).
(We will provide a default of \texttt{None} in the creation function
so we don't have to type \texttt{None} over and over again.)
Each matcher will try each of its possibilities and then see if the rest will also match:


\begin{lstlisting}[frame=single,frameround=tttt]
class RegexBase:
    def __init__(self, rest=None):
        self.rest = rest

    def match(self, text):
        for i in range(len(text) + 1):
            if self._match(text, i) is not None:
                return True
        return False

    def _match(self, text, start):
        raise NotImplementedError("derived classes must override '_match'")
\end{lstlisting}



FIXME


This design means we can get rid of \texttt{Seq},
but it does mean our expressions become deeply nested.
For example, the expression to match \texttt{ab*c} is:

\begin{lstlisting}[frame=single,frameround=tttt]
Lit("a", Any(Lit("b"), Lit("c")))
\end{lstlisting}


Here's how this strategy works for matching a literal expression:


\begin{lstlisting}[frame=single,frameround=tttt]
from re_base import RegexBase

class Lit(RegexBase):
    def __init__(self, chars, rest=None):
        super().__init__(rest)
        self.chars = chars

    def _match(self, text, start):
        next_index = start + len(self.chars)
        if next_index > len(text):
            return None
        if text[start:next_index] != self.chars:
            return None
        if not self.rest:
            return next_index
        return self.rest._match(text, next_index)
\end{lstlisting}



The \texttt{\_match} method checks whether all of the pattern matches the target text
starting at the current location.
If so,
it checks whether the rest of the overall pattern matches what's left.
Matching the start \texttt{{\textasciicircum}} and end \texttt{\$} anchors is just as straightforward.


\begin{lstlisting}[frame=single,frameround=tttt]
from re_base import RegexBase

class Start(RegexBase):
    def _match(self, text, start):
        if start != 0:
            return None
        if not self.rest:
            return 0
        return self.rest._match(text, start)
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
from re_base import RegexBase

class End(RegexBase):
    def _match(self, text, start):
        if start != len(text):
            return None
        if not self.rest:
            return len(text)
        return self.rest._match(text, start)
\end{lstlisting}



Matching either/or is done by trying the first pattern and the rest,
and if that fails,
trying the second pattern and the rest:


\begin{lstlisting}[frame=single,frameround=tttt]
from re_base import RegexBase

class Alt(RegexBase):
    def __init__(self, left, right, rest=None):
        super().__init__(rest)
        self.left = left
        self.right = right

    def _match(self, text, start):
        for pat in (self.left, self.right):
            after_pat = pat._match(text, start)
            if after_pat is not None:
                if not self.rest:
                    return after_pat
                after_rest = self.rest._match(text, after_pat)
                if after_rest is not None:
                    return after_rest
        return None
\end{lstlisting}



To match a repetition,
we figure out the maximum number of matches that might be left,
then count down until something succeeds.
(We start with the maximum because matching is supposed to be greedy.)
Each non-empty repetition matches at least one character,
so the number of remaining characters is the maximum number of matches worth trying.


\begin{lstlisting}[frame=single,frameround=tttt]
from re_base import RegexBase

class Any(RegexBase):
    def __init__(self, child, rest=None):
        super().__init__(rest)
        self.child = child

    def _match(self, text, start):
        max_possible = len(text) - start
        for num in range(max_possible, -1, -1):
            after_many = self._match_many(text, start, num)
            if after_many is not None:
                return after_many
        return None

    def _match_many(self, text, start, num):
        for i in range(num):
            start = self.child._match(text, start)
            if start is None:
                return None
        if self.rest:
            return self.rest._match(text, start)
        return start
\end{lstlisting}



With these classes in place,
our tests all pass:


FIXME


The most important thing about this design is how extensible it is:
if we want to add other kinds of matching,
all we have to do is add more classes.
That extensibility comes from the lack of centralized decision-making,
which in turn comes from our use of \glossref{polymorphism}\index{polymorphism}
and the \glossref{Chain of Responsibility}\index{Chain of Responsibility pattern}\index{design pattern!Chain of Responsibility} design pattern.
Each component does its part and asks something else to handle the remaining work;
so long as each component takes the same inputs,
we can put them together however we want.

\begin{callout}


\subsubsection*{The Open-Closed Principle}


The \glossref{Open-Closed Principle}\index{Open-Closed Principle}\index{software design!Open-Closed Principle} states that
software should be open for extension but closed for modification,
i.e., that it should be possible to extend functionality
without having to rewrite existing code.
As we said in FIXME,
this allows old code to use new code,
but only if our design permits the kinds of extensions people are going to want to make.
Since we can't anticipate everything,
it is normal to have to revise a design the first two or three times we try to extend it.
As \cite{Brand1995} said of buildings,
the things we make learn how to do things better as we use them.

\end{callout}

\section{Exercises}\label{matching-exercises}

\subsection*{Find and Fix the Error}


The first regular expression matcher contains an error:
the pattern \texttt{{\textquotesingle}a*ab{\textquotesingle}} should match the string \texttt{{\textquotesingle}aab{\textquotesingle}} but doesn't.
Figure out why it fails and fix it.

\subsection*{One More Than Length}


The main loop in \texttt{RegexBase.match} has \texttt{+1} inside the \texttt{range} call.
Which test(s) break when this is removed and why?

\subsection*{Find All}


Modify the regular expression matcher to return \emph{all} matches rather than just the first one.

\subsection*{Find One or More}


Extend the regular expression matcher to support \texttt{+}, meaning "one or more".

\subsection*{Match Sets of Characters}


Add a new regular expression matching class that matches any character from a set,
so that \texttt{Charset({\textquotesingle}aeiou{\textquotesingle})} matches any lower-case vowel.

\subsection*{Make Repetition More Efficient}


Rewrite \texttt{Any} so that it does not repeatedly re-match text.

\subsection*{Lazy Matching}


The regular expressions we have seen so far are \glossref{eager}:
they match as much as they can, as early as they can.
An alternative is \glossref{lazy matching},
in which expressions match as little as they need to.
For example,
given the string \texttt{"ab"},
an eager match with the expression \texttt{ab*} will match both letters
(because \texttt{b*} matches a 'b' if one is available)
but a lazy match will only match the first letter
(because \texttt{b*} can match no letters at all).
Implement lazy matching for the \texttt{*} operator.

\subsection*{Optional Matching}


The \texttt{?} operator means "optional",
so that \texttt{a?} matches either zero or one occurrences of the letter 'a'.
Implement this operator.

\chapter{Chapter 9: Parsing Regular Expressions}\label{parser}

\begin{itemize}

\item Use existing file formats rather than creating new ones.

\item Tokenize input text and then parse the tokens.

\item Parsing in two or more passes is often simpler than parsing in a single pass.

\item Every formal language corresponds to an abstract machine and vice versa.

\end{itemize}


\noindent 
    Terms defined:
    \glossref{finite state machine}, \glossref{literal}, \glossref{parser}, \glossref{precedence}, \glossref{token}, \glossref{Turing Machine}, \glossref{well-formed}, \glossref{Yet Another Markup Language}



We constructed objects to create regular expressions in \chapref{matching}.
It takes a lot less typing to write them as strings,
but if we do that we need to a \glossref{parser}\index{parser}
to convert those strings to objects.

\begin{table}
\begin{tabular}{ll}
\textbf{\underline{Meaning}} & \textbf{\underline{Character}} \\
Any literal character \emph{c} & \emph{c} \\
Beginning of input & {\textasciicircum} \\
End of input & \$ \\
Zero or more of something & * \\
Either/or & | \\
Grouping & (...) \\
\end{tabular}
\caption{Regular expression grammar.}
\label{parser-grammar}
\end{table}



\tblref{parser-grammar} shows the grammar our parser will handle.
When we are done
we should be able to parse \texttt{{\textasciicircum}(a|b|\$)*z\$} as
"start of text",
"any number of 'a', 'b', or '\$'",
"a single 'z',
and "end of text".
To keep our discussion focused on parsing
we will create a tree of objects (\figref{parser-parse-tree})
rather than instances of the regular expression classes from \chapref{matching};
the exercises will tackle the latter.

\figpdf{parser-parse-tree}{./parser/expression_tree.pdf}{Representing the result of parsing a regular expression as an tree.}{0.6}


\section{Tokenization}\label{parser-tokenize}


A \glossref{token}\index{token (in parsing)} is an atom of text,
such as the digits making up a number or the letters making up a variable name.
Our grammar's tokens are the special characters \texttt{*}, \texttt{|}, \texttt{(}, \texttt{)}, \texttt{{\textasciicircum}}, and \texttt{\$};
any sequence of one or more other characters is a single multi-letter token.
This classification guides the design of our parser:

\begin{enumerate}

\item 

If a character is special, create a token for it.



\item 

If it is a \glossref{literal}\index{literal (in parsing)} then
    combine it with the current literal if there is one
    or start a new literal.



\item 

Since \texttt{{\textasciicircum}} and \texttt{\$} are either special or regular depending on position,
    we must treat them as separate tokens or as part of a literal
    based on where they appear.



\end{enumerate}


We can translate these rules almost directly into code
to create a list of dictionaries whose keys are \texttt{"kind"} and \texttt{"loc"} (short for location),
with the extra key \texttt{"value"} for literal values:


\begin{lstlisting}[frame=single,frameround=tttt]
SIMPLE = {
  "*": "Any",
  "|": "Alt",
  "(": "GroupStart",
  ")": "GroupEnd"
}

def tokenize(text):
    result = []
    for (i, c) in enumerate(text):
        if c in SIMPLE:
            result.append({"kind": SIMPLE[c], "loc": i})
        elif (c == "^") and (i == 0):
            result.append({"kind": "Start", "loc": i})
        elif (c == "$") and (i == (len(text) - 1)):
            result.append({"kind": "End", "loc": i})
        else:
            combine_or_push(result, c, i)
    return result
\end{lstlisting}



The helper function \texttt{combine\_or\_push} does exactly what its name says.
If the thing most recently added to the list of tokens isn't a literal,
the new character becomes a new token;
otherwise,
it appends the new character to the literal:


\begin{lstlisting}[frame=single,frameround=tttt]
def combine_or_push(so_far, character, location):
  if (len(so_far) == 0) or (so_far[-1]["kind"] != "Lit"):
      so_far.append({"kind": "Lit", "value": character, "loc": location})
  else:
      so_far[-1]["value"] += character
\end{lstlisting}



We can try this out with a three-line test program:


\begin{lstlisting}[frame=single,frameround=tttt]
import json

from tokenizer_collapse import tokenize

test = "^a^b*"
result = tokenize(test)
print(test, "=>", json.dumps(result, indent=2))
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
^a^b* => [
  {
    "kind": "Start",
    "loc": 0
  },
  {
    "kind": "Lit",
    "value": "a^b",
    "loc": 1
  },
  {
    "kind": "Any",
    "loc": 4
  }
]
\end{lstlisting}



This simple tokenizer is readable, efficient, and wrong.
The problem is that the expression \texttt{ab*} means "a single \texttt{a} followed by zero or more \texttt{b}".
If we combine the letters \texttt{a} and \texttt{b} as we read them,
though,
we wind up with "zero or more repetitions of \texttt{ab}":


\begin{lstlisting}[frame=single,frameround=tttt]
import json

from tokenizer_collapse import tokenize

test = "ab*"
result = tokenize(test)
print(test, "=>", json.dumps(result, indent=2))
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
ab* => [
  {
    "kind": "Lit",
    "value": "ab",
    "loc": 0
  },
  {
    "kind": "Any",
    "loc": 2
  }
]
\end{lstlisting}



The solution is to treat each regular character as its own literal in this stage
and then combine things later:
Doing this lets us get rid of the nested \texttt{if} for handling \texttt{{\textasciicircum}} and \texttt{\$} as well:


\begin{lstlisting}[frame=single,frameround=tttt]
def tokenize(text):
    result = []
    for (i, c) in enumerate(text):
        if c in SIMPLE:
            result.append({"kind": SIMPLE[c], "loc": i})
        elif (c == "^") and (i == 0):
            result.append({"kind": "Start", "loc": i})
        elif (c == "$") and (i == (len(text) - 1)):
            result.append({"kind": "End", "loc": i})
        else:
            result.append({"kind": "Lit", "loc": i, "value": c})
    return result
\end{lstlisting}



Software isn't done until it's tested,
so let's build some tests.
The listing below shows a few of these
along with the output for the full set:


\begin{lstlisting}[frame=single,frameround=tttt]
from tokenizer import tokenize


def test_tokenize_single_character():
    assert tokenize("a") == [
        {"kind": "Lit", "value": "a", "loc": 0}
    ]


def test_tokenize_char_sequence():
    assert tokenize("ab") == [
        {"kind": "Lit", "value": "a", "loc": 0},
        {"kind": "Lit", "value": "b", "loc": 1}
    ]


def test_tokenize_start_anchor_alone():
    assert tokenize("^") == [
        {"kind": "Start", "loc": 0}
    ]


def test_tokenize_start_anchor_followed_by_characters():
    assert tokenize("^a") == [
        {"kind": "Start", "loc": 0},
        {"kind": "Lit", "value": "a", "loc": 1}
    ]



def test_tokenize_complex_expression():
    assert tokenize("^a*(bcd|e^)*f$gh$") == [
        {"kind": "Start", "loc": 0},
        {"kind": "Lit", "loc": 1, "value": "a"},
        {"kind": "Any", "loc": 2},
        {"kind": "GroupStart", "loc": 3},
        {"kind": "Lit", "loc": 4, "value": "b"},
        {"kind": "Lit", "loc": 5, "value": "c"},
        {"kind": "Lit", "loc": 6, "value": "d"},
        {"kind": "Alt", "loc": 7},
        {"kind": "Lit", "loc": 8, "value": "e"},
        {"kind": "Lit", "loc": 9, "value": "^"},
        {"kind": "GroupEnd", "loc": 10},
        {"kind": "Any", "loc": 11},
        {"kind": "Lit", "loc": 12, "value": "f"},
        {"kind": "Lit", "loc": 13, "value": "$"},
        {"kind": "Lit", "loc": 14, "value": "g"},
        {"kind": "Lit", "loc": 15, "value": "h"},
        {"kind": "End", "loc": 16}
    ]
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
============================= test session starts ==============================
platform darwin -- Python 3.10.4, pytest-7.1.2, pluggy-1.0.0
rootdir: /Users/gregwilson/sdpy/src/parser
collected 18 items

test_tokenizer.py ..................                                     [100%]

============================== 18 passed in 0.02s ==============================
\end{lstlisting}


\section{Assembling the Tree}\label{parser-tree}


We now have a list of tokens,
but we need a tree that represents the nesting introduced by parentheses
and the way that \texttt{*} applies to whatever comes before it.
Let's trace a few cases in order to see how to build this tree:

\begin{enumerate}

\item 

If the regular expression is \texttt{a}, we create a \texttt{Lit} token for the letter \texttt{"a"}.



\item 

If the regular expression is \texttt{a*},
    we create a \texttt{Lit} token for the \texttt{"a"} and append it to the output list.
    When we see the \texttt{*},
    we take that \texttt{Lit} token off the tail of the output list
    and replace it with an \texttt{Any} token that has the \texttt{Lit} token as its child.



\item 

What about the regular expression \texttt{(ab)}?
    We don't know how long the group is going to be when we see the \texttt{(},
    so we add the parenthesis to the output as a marker.
    We then add the \texttt{Lit} tokens for the \texttt{"a"} and \texttt{"b"} until we see the \texttt{)},
    at which point we pull tokens off the end of the output list
    until we get back to the \texttt{(} marker.
    When we find it,
    we put everything we have temporarily collected into a \texttt{Group} token and append it to the output list.
    This algorithm automatically handles patterns like \texttt{(a*)} and \texttt{(a(b*)c)}.



\item 

What about \texttt{a|b}?
    We append a \texttt{Lit} token for \texttt{"a"}, get the \texttt{|} and—we're stuck,
    because we don't yet have the next token we need to finish building the \texttt{Alt}.



\end{enumerate}


One way to solve this problem is to check to see if the thing on the top of the stack is waiting to combine
each time we append a new token.
However,
this still doesn't handle \texttt{a|b*} properly:
The pattern is supposed to mean "one \texttt{"a"} or any number of \texttt{"b"}",
but the check-and-combine strategy will turn it into the equivalent of \texttt{(a|b)*}.


A better solution is to leave some partially-completed tokens in the output
and compress them later (\figref{parser-mechanics}).
If our input is \texttt{a|b} we can:

\begin{enumerate}

\item 

Append a \texttt{Lit} token for \texttt{"a"}.



\item 

When we see \texttt{|},
    make that \texttt{Lit} token the left child of the \texttt{Alt}
    and append that to the output without filling in the right child.



\item 

Append the \texttt{Lit} token for \texttt{"b"}.



\item 

After all tokens have been handled,
    look for partially-completed \texttt{Alt} tokens and make whatever comes after them their right child.



\end{enumerate}


Again, this automatically handles patterns like \texttt{(ab)|c*|(de)}.

\figpdf{parser-mechanics}{./parser/mechanics.pdf}{Mechanics of combining tokens while parsing regular expressions.}{0.6}


Let's turn this idea into code.
The main structure of our parser is:


\begin{lstlisting}[frame=single,frameround=tttt]
from tokenizer import tokenize

def parse(text):
    result = []
    all_tokens = tokenize(text)
    for (i, token) in enumerate(all_tokens):
        is_last = (i == len(all_tokens) - 1)
        handle(result, token, is_last)
    return compress(result)
\end{lstlisting}



We handle tokens case by case
(with a few assertions to check that patterns are \glossref{well formed}):


\begin{lstlisting}[frame=single,frameround=tttt]
def handle(result, token, is_last):
    if token["kind"] == "Lit":
        result.append(token)
    elif token["kind"] == "Start":
        assert len(result) == 0, \
            "Should not have start token after other tokens"
        result.append(token)
    elif token["kind"] == "End":
        assert is_last, \
            "Should not have end token before other tokens"
        result.append(token)
    elif token["kind"] == "GroupStart":
        result.append(token)
    elif token["kind"] == "GroupEnd":
        result.append(group_end(result, token))
    elif token["kind"] == "Any":
        assert len(result) > 0, \
            f'No operand for "*" ({token["loc"]})'
        token["child"] = result.pop()
        result.append(token)
    elif token["kind"] == "Alt":
        assert len(result) > 0, \
            f'No operand for "*" ({token["loc"]})'
        token["left"] = result.pop()
        token["right"] = None
        result.append(token)
    else:
        assert False, f'UNIMPLEMENTED {token["kind"]}'
\end{lstlisting}



When we find the \texttt{)} that marks the end of a group,
we take items from the end of the output list
until we find the matching start
and use them to create a group:


\begin{lstlisting}[frame=single,frameround=tttt]
def group_end(result, token):
    group = {
        "kind": "Group",
        "loc": None,
        "end": token["loc"],
        "children": []
    }
    while True:
        assert len(result) > 0, \
            f'Unmatched end parenthesis ({token["loc"]})'
        child = result.pop()
        if child["kind"] == "GroupStart":
            group["loc"] = child["loc"]
            break
        group["children"].insert(0, child)
    return group
\end{lstlisting}



Finally,
when we have finished with the input,
we go through the output list one last time to fill in the right side of \texttt{Alt}s:


\begin{lstlisting}[frame=single,frameround=tttt]
def compress(raw):
    cooked = []
    while len(raw) > 0:
        token = raw.pop()
        if token["kind"] == "Alt":
            assert len(cooked) > 0, \
                f'No right operand for alt ({token["loc"]})'
            token["right"] = cooked.pop(0)
        cooked.insert(0, token)
    return cooked
\end{lstlisting}



Once again,
it's not done until we've tested it:


\begin{lstlisting}[frame=single,frameround=tttt]
from parser import parse


def test_parse_empty_string():
  assert parse("") == []


def test_parse_single_literal():
  assert parse("a") == [
    {"kind": "Lit", "loc": 0, "value": "a"}
  ]


def test_parse_multiple_literals():
  assert parse("ab") == [
    {"kind": "Lit", "loc": 0, "value": "a"},
    {"kind": "Lit", "loc": 1, "value": "b"}
  ]



def test_parse_alt_of_groups():
  assert parse("a|(bc)") == [
    {
      "kind": "Alt",
      "loc": 1,
      "left": {"kind": "Lit", "loc": 0, "value": "a"},
      "right": {
        "kind": "Group",
        "loc": 2,
        "end": 5,
        "children": [
          {"kind": "Lit", "loc": 3, "value": "b"},
          {"kind": "Lit", "loc": 4, "value": "c"}
        ]
      }
    }
  ]
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
============================= test session starts ==============================
platform darwin -- Python 3.10.4, pytest-7.1.2, pluggy-1.0.0
rootdir: /Users/gregwilson/sdpy/src/parser
collected 15 items

test_parser.py ...............                                           [100%]

============================== 15 passed in 0.01s ==============================
\end{lstlisting}



Our tokenizer and parser is less than 100 lines of code combined,
but they are doing some complex things.
Compared to parsers for things like JSON and YAML,
though,
they are still quite simple.
If we have more operators with different
\glossref{precedences}\index{operator precedence!implementing}
we should switch to the
\hreffoot{shunting-yard algorithm}{https://en.wikipedia.org/wiki/Shunting-yard\_algorithm}\index{shunting-yard algorithm}\index{parser!shunting-yard algorithm},
and if we need to handle a language like JavaScript we should explore tools like \hreffoot{ANTLR}{https://www.antlr.org/}\index{ANTLR}
that can generate a parser automatically from a description of the language to be parsed.
As we said at the start,
though,
if our design requires us to do either of those things we should come up with a simpler design.


\figpdf{parser-fsm}{./parser/finite_state_machine.pdf}{A finite state machine equivalent to a regular expression.}{0.6}

\section{Exercises}\label{parser-exercises}

\subsection*{Create objects}


Modify the parser to return instances of classes derived from
the \texttt{RegexBase} class of \chapref{matching}.

\subsection*{Escape characters}


Modify the parser to handle escape characters,
so that (for example) \texttt{{\textbackslash}*} is interpreted as a literal '*' character
and \texttt{{\textbackslash}{\textbackslash}} is interpreted as a literal backslash.

\subsection*{Lazy matching}


Modify the parser so that \texttt{*?} is interpreted as a single token
meaning "lazy match zero or more".

\subsection*{Character sets}


Modify the parser so that expressions like \texttt{[xyz]} are interpreted to mean
"match any one of the characters 'x', 'y', or 'z'".

\subsection*{Back reference}


Modify the tokenizer so that it recognizes \texttt{{\textbackslash}1}, \texttt{{\textbackslash}2}, and so on to mean "back reference".
The number may contain any number of digits.

\subsection*{Tokenize HTML}

\begin{enumerate}

\item 

Write a tokenizer for a subset of HTML that consists of:

\begin{itemize}

\item Opening tags without attributes, such as \texttt{<div>} and \texttt{<p>}

\item Closing tags, such as \texttt{</p>} and \texttt{</div>}

\item Plain text between tags that does \emph{not} contain '<' or '>' characters

\end{itemize}



\item 

Modify the tokenizer to handle \texttt{key="value"} attributes in opening tags.



\item 

Write tests for your tokenizer.



\end{enumerate}

\subsection*{The Shunting Yard Algorithm}

\begin{enumerate}

\item 

Use the \hreffoot{shunting-yard algorithm}{https://en.wikipedia.org/wiki/Shunting-yard\_algorithm}
    to implement a tokenizer for a simple subset of arithmetic that includes:

\begin{itemize}

\item single-letter variable names

\item single-digit numbers

\item the \texttt{+}, \texttt{*}, and \texttt{{\textasciicircum}} operators, where \texttt{+} has the lowest precedence and \texttt{{\textasciicircum}} has the highest

\end{itemize}



\item 

Write tests for your tokenizer.



\end{enumerate}

\chapter{Chapter 10: A Web Server}\label{server}

\begin{itemize}

\item Every computer on a network has a unique IP address.

\item The Domain Name System (DNS) translates human-readable names into IP addresses.

\item The programs on each computer send and receive messages through numbered sockets.

\item The program that receives a message must interpret the bytes in the message.

\item The HyperText Transfer Protocol (HTTP) specifies one way to interact via messages over sockets.

\item A minimal HTTP request has a method, a URL, and a protocol version.

\item A complete HTTP request may also have headers and a body.

\item An HTTP response has a status code, a status phrase, and optionally some headers and a body.

\item HTTP is a stateless protocol: the application is responsible for remembering things between requests.

\end{itemize}


The Internet is simpler than most people realize
(as well as being more complex than anyone could possibly have imagined).
Most systems still follow the rules they did thirty years ago;
in particular,
most web servers still handle the same kinds of messages in the same way.

\section{Sockets}\label{server-sockets}


Pretty much every program on the web
runs on a family of communication standards called Internet Protocol (IP).
The member of that family which concerns us is the Transmission Control Protocol (TCP/IP),
which makes communication between computers look like reading and writing files.


Programs using IP communicate through sockets.
Each socket is one end of a point-to-point communication channel,
just like a phone is one end of a phone call.
A socket consists of an IP address that identifies a particular machine
and a port number on that machine.
The IP address consists of four 8-bit numbers,
such as \texttt{174.136.14.108};
the Domain Name System (DNS) matches these numbers to symbolic names like \texttt{example.com}
that are easier for human beings to remember.


A port number is a number in the range 0-65535
that uniquely identifies the socket on the host machine.
(If an IP address is like a company's phone number,
then a port number is like an extension.)
Ports 0-1023 are reserved for the operating system's use;
anyone else can use the remaining ports.


Here's a simple socket client:


\begin{lstlisting}[frame=single,frameround=tttt]
import socket
import sys

KILOBYTE = 1024
SERVER_ADDRESS = ("", 8080)

message = sys.argv[1]

sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
sock.connect(SERVER_ADDRESS)
sock.sendall(bytes(message, "utf-8"))
print(f"client sent {len(message)} bytes")

received = sock.recv(KILOBYTE)
received_str = str(received, 'utf-8')
print(f"client received {len(received)} bytes: '{receiver_str}'")
\end{lstlisting}



From top to bottom, it:

\begin{enumerate}

\item Imports some modules and defines some constants.
    The most interesting of these is \texttt{SERVER\_ADDRESS}
    The empty string \texttt{""} for the host means "the current machine";
    we could also use the string \texttt{"localhost"}.

\item We use \texttt{socket.socket} to create a new socket.
    The values \texttt{AF\_INET} and \texttt{SOCK\_STREAM} specify the protocols we're using;
    we'll always use those in our examples,
    so we won't go into details about them.

\item We connect to the server...

\item ...send our message as a bunch of bytes with \texttt{sock.sendall}...

\item ...and print a message saying the data's been sent.

\item We then read up to a kilobyte from the socket with \texttt{sock.recv}.
    If we were expecting longer messages,
    we'd keep reading from the socket until there was no more data,
    but we're trying to keep this example simple.

\item Finally, we print another diagnostic message.

\end{enumerate}


The receiving end is only a little more complicated:


\begin{lstlisting}[frame=single,frameround=tttt]
import socketserver


KILOBYTE = 1024
SERVER_ADDRESS = ("", 8080)


class MyHandler(socketserver.BaseRequestHandler):
    """The request handler class for our server."""

    def handle(self):
        """Handle a single request."""
        data = self.request.recv(KILOBYTE)
        msg = f"got request from {self.client_address[0]}: {len(data)}"
        print(msg)
        self.request.sendall(bytes(msg, "utf-8"))


if __name__ == "__main__":
    server = socketserver.TCPServer(SERVER_ADDRESS, MyHandler)
    server.serve_forever()
\end{lstlisting}



Python's \texttt{socketserver} library provides two things:
a class called \texttt{TCPServer} that listens for incoming connections
and then manages them for us,
and another class called \texttt{BaseRequestHandler}
that does everything \emph{except} process the incoming data.
In order to do that,
we derive a class of our own from \texttt{BaseRequestHandler} that provides a \texttt{handle} method.
Every time \texttt{TCPServer} gets a new connection
it creates a new instance of our class
and calls its \texttt{handle} method.


Our \texttt{handle} method is the inverse of the code that sends request:

\begin{enumerate}

\item Read up to a kilobyte from \texttt{self.request}
    (which is set up automatically for us in the base class \texttt{BaseRequestHandler}).

\item Construct and print a diagnostic message.

\item Use \texttt{self.request} again to send data back to whoever we received the message from.

\end{enumerate}


The easiest way to test this is to open two terminal windows side by side.
\tblref{server-transcript} shows the sequence of operations side by side:

\begin{table}
\begin{tabular}{ll}
\textbf{\underline{Server}} & \textbf{\underline{Client}} \\
\texttt{\$ python socket\_server.py} &  \\
 & \texttt{\$ python socket\_client.py "a test message"} \\
 & \texttt{client sent 14 bytes} \\
\texttt{got request from 127.0.0.1: 14} &  \\
 & \texttt{client received 30 bytes: {\textquotesingle}got request from 127.0.0.1: 14} \\
\end{tabular}
\caption{Sequence of operations in socket client/server interaction}
\label{server-transcript}
\end{table}


\section{HTTP}\label{server-http}


The Hypertext Transfer Protocol (HTTP) describes one way that
programs can exchange data over IP.
HTTP is deliberately simple:
the client sends a request specifying what it wants over a socket connection,
and the server sends some data in response.
The data may be copied from a file on disk,
generated dynamically by a program,
or some mix of the two.


The most important thing about an HTTP request is that it's just text:
any program that wants to can create one or parse one.
An absolutely minimal HTTP request has just a \emph{method},
a \emph{URL},
and a \emph{protocol version}
on a single line separated by spaces:

\begin{lstlisting}[frame=single,frameround=tttt]
GET /index.html HTTP/1.1
\end{lstlisting}


The HTTP method is almost always either "GET" (to fetch information)
or "POST" (to submit form data or upload files).
The URL specifies what the client wants;
it is often a path to a file on disk,
such as \texttt{/index.html},
but (and this is the crucial part)
it's completely up to the server to decide what to do with it.
The HTTP version is usually "HTTP/1.0" or "HTTP/1.1";
the differences between the two don't matter to us.


Most real requests have a few extra lines called \emph{headers},
which are key value pairs like the three shown below:

\begin{lstlisting}[frame=single,frameround=tttt]
GET /index.html HTTP/1.1
Accept: text/html
Accept-Language: en, fr
If-Modified-Since: 16-May-2022
\end{lstlisting}


Unlike the keys in hash tables,
keys may appear any number of times in HTTP headers.
This allows a request to do things like
specify that it's willing to accept several types of content.


Finally,
the \emph{body} of the request is any extra data associated with the request.
This is used when submitting data via web forms,
when uploading files,
and so on.
There must be a blank line between the last header and the start of the body
to signal the end of the headers.
If there is a body,
the request must have a header called \texttt{Content-Length}
that tells the server how many bytes to read in the body of the request.


An HTTP response is formatted like an HTTP request.
Its first line has the protocol,
a \emph{status code} (like 200 for "OK" or 404 for "Not Found")
and a status phrase (like "OK").
There are then some headers,
a blank line,
and the body of the response:

\begin{lstlisting}[frame=single,frameround=tttt]
HTTP/1.1 200 OK
Date: Thu, 16 June 2022 12:28:53 GMT
Server: minserve/2.2.14 (Linux)
Last-Modified: Wed, 15 Jun 2022 19:15:56 GMT
Content-Type: text/html
Content-Length: 53

<html>
<body>
<h1>Hello, World!</h1>
</body>
</html>
\end{lstlisting}


Constructing HTTP requests is tedious,
so most people use libraries to do most of the work.
The most popular such library in Python is called \hreffoot{requests}{https://requests.readthedocs.io/}.


\begin{lstlisting}[frame=single,frameround=tttt]
import requests

response = requests.get("http://third-bit.com/test.html")
print("status code:", response.status_code)
print("content length:", response.headers["content-length"])
print(response.text)
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
status code: 200
content length: 109
<html>
  <head>
    <title>A Test Page</title>
  </head>
  <body>
    <h1>A Test Page</h1>
  </body>
</html>
\end{lstlisting}



\texttt{request.get} sends an HTTP GET request to a server
and returns an object containing the response.
That object's \texttt{status\_code} member is the response's status code;
its \texttt{content\_length} member  is the number of bytes in the response data,
and \texttt{text} is the actual data
(in this case, an HTML page).

\section{Hello, Web}\label{server-static}


We're now ready to write our first simple HTTP server.
The basic idea is simple:

\begin{enumerate}

\item Wait for someone to connect to our server and send an HTTP request;

\item parse that request;

\item figure out what it's asking for;

\item fetch that data (or generate it dynamically);

\item format the data as HTML; and

\item send it back.

\end{enumerate}


Steps 1, 2, and 6 are the same from one application to another,
so the Python standard library has a module called \texttt{http.server}
that contains tools to do that for us
so that we just have to take care of steps 3-5.
Here's our first working web server:


\begin{lstlisting}[frame=single,frameround=tttt]
from http.server import HTTPServer, BaseHTTPRequestHandler

# Page to send back.
PAGE = """\
<html>
<body>
<p>Hello, web!</p>
</body>
</html>
"""

class RequestHandler(BaseHTTPRequestHandler):
    """Handle HTTP requests by returning a fixed "page"."""

    # Handle a GET request.
    def do_GET(self):
        content = bytes(PAGE, "utf-8")
        self.send_response(200)
        self.send_header("Content-Type", "text/html")
        self.send_header("Content-Length", str(len(content)))
        self.end_headers()
        self.wfile.write(content)


if __name__ == "__main__":
    server_address = ("", 8080)
    server = HTTPServer(server_address, RequestHandler)
    server.serve_forever()
\end{lstlisting}



Let's start at the bottom and work our way up.

\begin{enumerate}

\item Again, \texttt{SERVER\_ADDRESS} that specifies the host the server is running on
    and the port it's listening on.

\item The \texttt{HTTPServer} class takes care of parsing requests and sending back responses.
    When we construct it,
    we give it the server address and the name of the class we've written
    that handles requests the way we want—in this case, \texttt{RequestHandler}.

\item Finally, we call the server's \texttt{serve\_forever} method.
    It will then run until it crashes or we stop it with Ctrl-C.

\end{enumerate}


So what does \texttt{RequestHandler} do?

\begin{enumerate}

\item When the server receives a \texttt{GET} request,
    it looks in the request handler for a method called \texttt{do\_GET}.
    (If it gets a \texttt{POST}, it looks for \texttt{do\_POST} and so on.)

\item \texttt{do\_GET} converts the text of the page we want to send back
    from characters to bytes—we'll talk about this below.

\item It then sends a response code (200),
    a couple of headers to say what the content type is
    and how many bytes the receiver should expect,
    and a blank line (produced by the \texttt{end\_headers} method).

\item Finally, \texttt{do\_GET} sends the content of the response
    by calling \texttt{self.wfile.write}.
    \texttt{self.wfile} is something that looks and acts like a write-only file,
    but is actually sending bytes to the socket connection.

\end{enumerate}


If we run this program from the command line,
it doesn't display anything:

\begin{lstlisting}[frame=single,frameround=tttt]
$ python serve_static_content.py
\end{lstlisting}


If we then go to \texttt{http://localhost:8080} with our browser,
though,
we get this in our browser:

\begin{lstlisting}[frame=single,frameround=tttt]
Hello, web!
\end{lstlisting}


and this in our shell:

\begin{lstlisting}[frame=single,frameround=tttt]
127.0.0.1 - - [16/Sep/2022 06:34:59] "GET / HTTP/1.1" 200 -
127.0.0.1 - - [16/Sep/2022 06:35:00] "GET /favicon.ico HTTP/1.1" 200 -
\end{lstlisting}


The first line is straightforward:
since we didn't ask for a particular file,
our browser has asked for '/' (the root directory of whatever the server is serving).
The second line appears because
our browser automatically sends a second request
for an image file called \texttt{/favicon.ico},
which it will display as an icon in the address bar if it exists.

\section{Exercises}\label{server-exercises}


FIXME

\chapter{Chapter 11: A File Cache}\label{filecache}

\begin{itemize}

\item FIXME

\end{itemize}


FIXME

\chapter{Chapter 12: A Database}\label{database}

\begin{itemize}

\item FIXME

\end{itemize}


FIXME

\chapter{Chapter 13: Object Persistence}\label{persistence}

\begin{itemize}

\item FIXME

\end{itemize}


\noindent 
    Terms defined:
    \glossref{alias}, \glossref{atomic value}, \glossref{dynamic dispatch}, \glossref{escape sequence}, \glossref{fixture}, \glossref{JavaScript Object Notation}, \glossref{list comprehension}, \glossref{Open-Closed Principle}, \glossref{signature}



Database tables aren't the only way to save data.
Another option is to store objects as objects,
i.e.,
to save a list of dictionaries of dictionaries as-is
rather than flattering it into rows and columns.
Python's \hreffoot{pickle}{https://docs.python.org/3/library/pickle.html} module does this in a Python-specific way,
saving objects in a custom binary format,
while the \hreffoot{json}{https://docs.python.org/3/library/json.html} module saves some kinds of objects as text
formatted as \glossref{JSON},
which program written in other languages can read.


The phrase "some kinds of objects" is the most important part of the preceding paragraph.
Since programs can define new classes,
a persistence framework has to make one of the following choices:

\begin{enumerate}

\item 

Only handle built-in types,
    or even more strictly,
    only handle types that are common across many languages,
    so that data saved by Python can be read by JavaScript and vice versa.



\item 

Provide an easy way for programs to convert from user-defined types
    to built-in types
    and then save those.
    This choice is less restrictive
    but may lead to some information being lost:
    for example,
    if a program has a \texttt{User} class
    and instances of that class are saved as dictionaries,
    then the program that reads data might wind up with dictionaries instead of users.



\item 

Save class definitions as well as objects' values
    so that when a program reads saved data
    it can reconstruct the classes
    and then create fully-functional instances of them.
    This choice is the most powerful,
    but it is also the hardest to implement,
    particularly across languages.
    It is also the riskiest:
    if a program is reading and then running saved methods,
    it has to trust that those methods aren't doing anything malicious.



\end{enumerate}


This chapter starts by implementing the first option (built-in types only),
then extends it to handle \glossref{aliases}
(which the JSON standard does not),
and finally adds ways to convert user-defined types to storable data.
To keep testing simple
we will store everything in flat text files;
to keep parsing simple
we will store one value per line
rather than using brackets and curly braces.

\section{Built-in Types}\label{persistence-builtin}


The first thing we need to do is decide on a data format.
We will store each \glossref{atomic value} on a line of its own
with the type's name first and the value second.
For example,
the integer \texttt{123} will be saved as:

\begin{lstlisting}[frame=single,frameround=tttt]
int:123
\end{lstlisting}


The function \texttt{save} handles three of Python's built-in types to start with:


\begin{lstlisting}[frame=single,frameround=tttt]
def save(writer, thing):
    if isinstance(thing, bool):
        print(f"bool:{thing}", file=writer)

    elif isinstance(thing, float):
        print(f"float:{thing}", file=writer)

    elif isinstance(thing, int):
        print(f"int:{thing}", file=writer)

    else:
        raise ValueError(f"unknown type of thing {type(thing)}")
\end{lstlisting}



The function that loads data starts by reading a single line,
stripping off the newline at the end
(which is added automatically by the \texttt{print} in \texttt{save}),
and then splitting the line on the colon.
After checking that there are two fields,
it uses the type name in the first field
to decide how to handle the second:


\begin{lstlisting}[frame=single,frameround=tttt]
def load(reader):
    line = reader.readline()[:-1]
    assert line, "Nothing to read"
    fields = line.split(":", maxsplit=1)
    assert len(fields) == 2, f"Badly-formed line {line}"
    key, value = fields

    if key == "bool":
        names = {"True": True, "False": False}
        assert value in names, f"Unknown Boolean {value}"
        return names[value]

    elif key == "float":
        return float(value)

    elif key == "int":
        return int(value)

    else:
        raise ValueError(f"unknown type of thing {line}")
\end{lstlisting}



Saving a list is almost as easy:
we save the number of items in the list,
and then save each item with a recursive called to \texttt{save}.
For example,
the list \texttt{[55, True, 2.71]} is saved as:

\begin{lstlisting}[frame=single,frameround=tttt]
list:3
int:55
bool:True
float:2.71
\end{lstlisting}


The code to do this is:


\begin{lstlisting}[frame=single,frameround=tttt]
    elif isinstance(thing, list):
        print(f"list:{len(thing)}", file=writer)
        for item in thing:
            save(writer, item)
\end{lstlisting}



To load data we just read the specified number of items back into a list:


\begin{lstlisting}[frame=single,frameround=tttt]
    elif key == "list":
        return [load(reader) for _ in range(int(value))]
\end{lstlisting}



Notice that these two functions don't need to know
what kinds of values are in the list.
Each recursive call to \texttt{save} or \texttt{load}
advances the input or output stream
by precisely as many lines as it needs to.


Our functions handle sets in exactly the same was as lists;
the only difference is using the keyword \texttt{set} instead of the keyword \texttt{list}
in the opening line.
To save a dictionary,
we save the number of entries
and then save each key and value in turn:


\begin{lstlisting}[frame=single,frameround=tttt]
    elif isinstance(thing, dict):
        print(f"dict:{len(thing)}", file=writer)
        for (key, value) in thing.items():
            save(writer, key)
            save(writer, value)
\end{lstlisting}



\noindent The code to load a dictionary is analogous.


We now need to write some unit tests.
We will use two tricks when doing this:

\begin{enumerate}

\item 

The \texttt{StringIO} class from Python's \hreffoot{io}{https://docs.python.org/3/library/io.html} module
    allows us to read from strings and write to them
    using the functions we normally use to read and write files.
    Using this lets us run our tests
    without creating lots of little files as a side effect.



\item 

The \texttt{dedent} function from Python's \hreffoot{textwrap}{https://docs.python.org/3/library/textwrap.html} module
    removes leading indentation from the body of a string.
    Using this allows us to indent a textual \glossref{fixture}
    the same way we indent our Python code,
    which makes the test easier to read.



\end{enumerate}


\begin{lstlisting}[frame=single,frameround=tttt]
def test_save_list_flat():
    fixture = [0, False]
    expected = dedent("""\
    list:2
    int:0
    bool:False
    """)
    output = StringIO()
    save(output, fixture)
    assert output.getvalue() == expected
\end{lstlisting}



We still need to decide how to save strings.
We can't just print them out because they might contain newlines,
which would break our reader.
One option would be to save each string's representation,
using the two-character \glossref{escape sequence} \texttt{{\textbackslash}n} for newline.
Instead,
we choose to break each string on newlines
and then save the number of lines
followed by each line.
For example,
the two-line text \texttt{"hello{\textbackslash}nthere"} will be saved as:

\begin{lstlisting}[frame=single,frameround=tttt]
str:2
hello
there
\end{lstlisting}


The \texttt{elif} branch to do this in \texttt{save} is:


\begin{lstlisting}[frame=single,frameround=tttt]
    elif isinstance(thing, set):
        print(f"set:{len(thing)}", file=writer)
        for item in thing:
            save(writer, item)
\end{lstlisting}



\noindent and the corresponding clause in \texttt{load} is:


\begin{lstlisting}[frame=single,frameround=tttt]
    elif key == "str":
        return "\n".join([reader.readline()[:-1] for _ in range(int(value))])
\end{lstlisting}


\section{Converting to Classes}\label{persistance-oop}


The \texttt{save} and \texttt{load} functions we built in the previous section work,
but as we were trying to extend them
we discovered that we had to modify their internals
every time we wanted to do something new.
As we said in \chapref{matching},
the \glossref{Open-Closed Principle}\index{Open-Closed Principle}\index{software design!Open-Closed Principle}
states that it should be possible to extend functionality
without rewriting existing code.
To make the next steps easier,
we will rewrite our functions as classes.


We will also use \glossref{dynamic dispatch}
to handle each item
instead of a multiway \texttt{if} statement.
The core of our saving class is:


\begin{lstlisting}[frame=single,frameround=tttt]
class SaveOop:
    def __init__(self, writer):
        self.writer = writer

    def save(self, thing):
        typename = type(thing).__name__
        method = f"_{typename}"
        assert hasattr(self, method), f"Unknown object type {typename}"
        getattr(self, method)(thing)
\end{lstlisting}



\noindent (We have called it \texttt{SaveOop} instead of just \texttt{Save}
because we are going to create several variations on it.)


\texttt{SaveOop.save} figures out which method to call to save a particular thing
by constructing a name based on the thing's type,
checking whether that method exists,
and then calling it.
To make this work,
the methods that handle specific items
must all have the same \glossref{signature}
so that they can be called interchangeably.
For example,
the methods that write integers and strings are:


\begin{lstlisting}[frame=single,frameround=tttt]
    def _int(self, thing):
        self._write("int", thing)

    def _str(self, thing):
        lines = thing.split("\n")
        self._write("str", len(lines))
        for line in lines:
            print(line, file=self.writer)
\end{lstlisting}



\texttt{LoadOop.load} combines dynamic dispatch with
the string handling of our original \texttt{load} function:


\begin{lstlisting}[frame=single,frameround=tttt]
class LoadOop:
    def __init__(self, reader):
        self.reader = reader

    def load(self):
        line = self.reader.readline()[:-1]
        assert line, "Nothing to read"
        fields = line.split(":", maxsplit=1)
        assert len(fields) == 2, f"Badly-formed line {line}"
        key, value = fields
        method = f"_{key}"
        assert hasattr(self, method), f"Unknown object type {key}"
        return getattr(self, method)(value)
\end{lstlisting}



The methods that load individual items are even simpler:


\begin{lstlisting}[frame=single,frameround=tttt]
    def _float(self, value):
        return float(value)
\end{lstlisting}


\section{Aliasing}\label{persistence-aliasing}


Consider these two lines of code:

\begin{lstlisting}[frame=single,frameround=tttt]
shared = ["shared"]
fixture = [shared, shared]
\end{lstlisting}


\noindent They create the data structure shown on the left in FIXME,
but if we save this structure and then reload it
using what we have built so far
we will wind up with the data structure show on the right.


FIXME


In order to reconstruct the original data correctly we need to:

\begin{enumerate}

\item 

keep track of everything we have saved;



\item 

save a marker instead of the object itself
    when we try to save it a second time;
    and



\item 

reverse this process when loading data.



\end{enumerate}


Luckily,
Python has a built-in function called \texttt{id}
that returns a unique ID for every object in the program.
Even if two lists or dictionaries contain the same data,
\texttt{id} will report different IDs
because they're stored in different locations in memory.
We can use this to:

\begin{enumerate}

\item 

store the IDs of all the objects we've already saved
    in a set, and then



\item 

write a special entry with the keyword \texttt{alias}
    and its unique ID
    when we see an object for the second time.



\end{enumerate}


Here's the start of \texttt{SaveAlias}:


\begin{lstlisting}[frame=single,frameround=tttt]
class SaveAlias(SaveOop):
    def __init__(self, writer):
        super().__init__(writer)
        self.seen = set()

    def save(self, thing):
        thing_id = id(thing)
        if thing_id in self.seen:
            self._write("alias", thing_id, "")
            return

        self.seen.add(id(thing))
        typename = type(thing).__name__
        method = f"_{typename}"
        assert hasattr(self, method), f"Unknown object type {typename}"
        getattr(self, method)(thing)
\end{lstlisting}



\noindent Its constructor creates an empty set of IDs-seen-so-far.
If \texttt{SaveAlias.save} notices that the object it's about to save
has been saved before,
it writes a line like this:

\begin{lstlisting}[frame=single,frameround=tttt]
alias:12345678:
\end{lstlisting}


where \texttt{12345678} is the object's ID.
Otherwise,
it saves the object's type,
its ID,
and either its value or its length:


\begin{lstlisting}[frame=single,frameround=tttt]
    def _list(self, thing):
        self._write("list", id(thing), len(thing))
        for item in thing:
            self.save(item)
\end{lstlisting}



\texttt{SaveAlias.\_list} is a little different from \texttt{SaveOop.\_list}
because it has to save each object's identifier
along with its type and its value or length.
Our \texttt{LoadAlias} class,
on the other hand,
can recycle all the loading methods for particular datatypes
from \texttt{LoadOop}.
All that has to change is the \texttt{load} method itself,
which looks to see if we're restoring aliased data
or loading something new:


\begin{lstlisting}[frame=single,frameround=tttt]
class LoadAlias(LoadOop):
    def __init__(self, reader):
        super().__init__(reader)
        self.seen = {}

    def load(self):
        line = self.reader.readline()[:-1]
        assert line, "Nothing to read"
        fields = line.split(":", maxsplit=2)
        assert len(fields) == 3, f"Badly-formed line {line}"
        key, ident, value = fields

        # the lines below contain a bug
        if key == "alias":
            assert ident in self.seen
            return self.seen[ident]

        method = f"_{key}"
        assert hasattr(self, method), f"Unknown object type {key}"
        result = getattr(self, method)(value)
        self.seen[ident] = result
        return result
\end{lstlisting}



The first test of our new code is:


\begin{lstlisting}[frame=single,frameround=tttt]
def test_aliasing_no_aliasing():
    fixture = ["a", {"b": True, 7: {"c": "d"}}]
    assert roundtrip(fixture) == fixture
\end{lstlisting}



\noindent which uses this helper function:


\begin{lstlisting}[frame=single,frameround=tttt]
def roundtrip(fixture):
    writer = StringIO()
    Save(writer).save(fixture)
    reader = StringIO(writer.getvalue())
    return Load(reader).load()
\end{lstlisting}



\noindent There isn't any aliasing in the test case,
but that's deliberate:
we want to make sure we haven't broken code that was working
before we move on.


Here's a test that actually includes some aliasing:


\begin{lstlisting}[frame=single,frameround=tttt]
def test_aliasing_shared_child():
    shared = ["shared"]
    fixture = [shared, shared]
    result = roundtrip(fixture)
    assert result == fixture
    assert id(result[0]) == id(result[1])
    result[0][0] = "changed"
    assert result[1][0] == "changed"
\end{lstlisting}



\noindent It checks that the aliased sub-list is actually aliased after the data is restored,
and then checks that modifying that sub-list works as it should
(i.e.,
that changes made through one alias are visible through the other).
The second check ought to be redundant,
but it's still comforting.


There's one more case to check,
and unfortunately it turns up a bug in our code.
The two lines:

\begin{lstlisting}[frame=single,frameround=tttt]
fixture = []
fixture.append(fixture)
\end{lstlisting}


\noindent create the data structure shown in FIXME,
in which an object contains a reference to itself.
Our code ought to handle this case but doesn't:
when we try to read in the saved data,
\texttt{LoadAlias.load} sees the \texttt{alias} line
but then says it can't find the object being referred to.


FIXME


\noindent The problem is these lines in \texttt{LoadAlias.load}
marked as containing a bug,
in combination with these lines inherited from \texttt{LoadOop}:


\begin{lstlisting}[frame=single,frameround=tttt]
    def _list(self, value):
        return [self.load() for _ in range(int(value))]
\end{lstlisting}



\noindent Let's trace execution for the saved data:

\begin{lstlisting}[frame=single,frameround=tttt]
list:4484025600:1
alias:4484025600:
\end{lstlisting}

\begin{enumerate}

\item 

The first line tells us that there's a list whose ID is \texttt{4484025600}
    so we \texttt{LoadOop.\_list} to load a list of one element.



\item 

\texttt{LoadOop.\_list} called \texttt{LoadAlias.load} recursively to load that one element.



\item 

\texttt{LoadAlias.load} reads the second line of saved data,
    which tells it to re-use the data whose ID is \texttt{4484025600}.
    But \texttt{LoadOop.\_list} hasn't created and returned that list yet—it
    is still reading in the elements—so
    \texttt{LoadAlias.load} hasn't had a chance to add the list to the \texttt{seen} dictionary
    of previously-read items.



\end{enumerate}


The solution is to reorder the operations,
which unfortunately means writing new versions
of all the methods defined in \texttt{LoadOop}.
The new implementation of \texttt{\_list} is:


\begin{lstlisting}[frame=single,frameround=tttt]
    def _list(self, ident, value):
        result = []
        self.seen[ident] = result
        for _ in range(int(value)):
            result.append(self.load())
        return result
\end{lstlisting}



This method creates the list it's going to return,
adds that list to the \texttt{seen} dictionary immediately,
and \emph{then} loads list items recursively.
We have to pass it the ID of the list
to use as the key in \texttt{seen},
and we have to use a loop rather than a \glossref{list comprehension},
but the changes to \texttt{\_set} and \texttt{\_dict} follow exactly the same pattern.

\section{User-Defined Classes}\label{persistence-extend}


It's time to extend our framework to handle user-defined classes.
We'll start by refactoring our code so that the \texttt{save} method doesn't get any larger:


\begin{lstlisting}[frame=single,frameround=tttt]
class SaveExtend(SaveAlias):
    def __init__(self, writer):
        super().__init__(writer)

    def save(self, thing):
        if self._aliased(thing):
            return
        if self._builtin(thing):
            return
        assert False, f"Don't know how to handle {thing}"

    # [save_aliased]
    def _aliased(self, thing):
        thing_id = id(thing)
        if thing_id not in self.seen:
            return False
        self._write("alias", thing_id, "")
        return True
    # [/save_aliased]

    # [save_builtin]
    def _builtin(self, thing):
        typename = type(thing).__name__
        method = f"_{typename}"
        if not hasattr(self, method):
            return False
        self.seen.add(id(thing))
        getattr(self, method)(thing)
        return True
    # [/save_builtin]
\end{lstlisting}



\noindent The method to handle built-in types is:


\begin{lstlisting}[frame=single,frameround=tttt]
    def _builtin(self, thing):
        typename = type(thing).__name__
        method = f"_{typename}"
        if not hasattr(self, method):
            return False
        self.seen.add(id(thing))
        getattr(self, method)(thing)
        return True
\end{lstlisting}



\noindent and the one that handles aliases is:


\begin{lstlisting}[frame=single,frameround=tttt]
    def _aliased(self, thing):
        thing_id = id(thing)
        if thing_id not in self.seen:
            return False
        self._write("alias", thing_id, "")
        return True
\end{lstlisting}



\noindent None of this code is new:
we've just moved things into methods
to make each piece easier to understand.


So how does a class indicate that it can be saved and loaded by our framework?
One option would be to have it inherit from some base class;
another is just to require it to have some particular method
that gives us what we need.
The second is simpler,
so we arbitrarily decide that
if a class has a method called \texttt{to\_dict},
we'll call that to get its contents as a dictionary
and then persist the dictionary.
Before doing that,
though,
we will save a line indicating that
this dictionary should be used to reconstruct an object
of a particular class:


\begin{lstlisting}[frame=single,frameround=tttt]
    def _extension(self, thing):
        if not hasattr(thing, "to_dict"):
            return False
        self._write("@extension", id(thing), thing.__class__.__name__)
        self.save(thing.to_dict())
        return True
\end{lstlisting}



Loading user-defined classes requires more work
because we have to map class names back to actual classes.
We start by modifying the loader's constructor
to take zero or more extension classes as arguments
and then build a name-to-class lookup table from them:


\begin{lstlisting}[frame=single,frameround=tttt]
class LoadExtend(LoadAlias):
    def __init__(self, reader, *extensions):
        super().__init__(reader)
        self.seen = {}
        self.extensions = {e.__name__:e for e in extensions}
\end{lstlisting}



The \texttt{load} method then looks for aliases,
built-in types,
and extensions in that order.
Instead of using a chain of \texttt{if} statements
we loop over the methods that handle these cases.
If a method decides that it can handle the incoming data
it returns a result;
if it can't,
it throws a \texttt{KeyError},
and if none of the methods handle a case
we fail:


\begin{lstlisting}[frame=single,frameround=tttt]
    def load(self):
        key, ident, value = self._next()
        for method in (self._aliased, self._builtin, self._extension):
            try:
                return method(key, ident, value)
            except KeyError:
                pass
        assert False, f"Don't know how to handle {key} {ident} {value}"
\end{lstlisting}



The code to handle built-ins and aliases is copied from our previous work
and modified to raise \texttt{KeyError}:


\begin{lstlisting}[frame=single,frameround=tttt]
    def _aliased(self, key, ident, value):
        if key != "alias":
            raise KeyError()
        assert ident in self.seen
        return self.seen[ident]

    def _builtin(self, key, ident, value):
        method = f"_{key}"
        if not hasattr(self, method):
            raise KeyError()
        return getattr(self, method)(ident, value)
\end{lstlisting}



The method that handles extensions
checks that the value on the line just read indicates an extension,
then reads the dictionary containing the object's contents
from the input stream
and uses it to build an instance of the right class:


\begin{lstlisting}[frame=single,frameround=tttt]
    def _extension(self, key, ident, value):
        if (key != "@extension") or (value not in self.extensions):
            raise KeyError()
        cls = self.extensions[value]
        contents = self.load()
        return cls(**contents)
\end{lstlisting}



Here's a class that defines the required method:


\begin{lstlisting}[frame=single,frameround=tttt]
class Parent:
    def __init__(self, name):
        self.name = name

    def to_dict(self):
        return {"name": self.name}
\end{lstlisting}



and here's a test to make sure everything works:


\begin{lstlisting}[frame=single,frameround=tttt]
def test_extend_extension_class():
    fixture = Parent("subject")
    writer = StringIO()
    Save(writer).save(fixture)
    reader = StringIO(writer.getvalue())
    result = Load(reader, Parent).load()
    assert isinstance(result, Parent)
    assert result.name == fixture.name
\end{lstlisting}




The tools we have developed do what they're supposed to do,
but please don't ever use them in real applications:
the world already has enough data storage formats.

\section{Exercises}\label{persistence-exercises}

\subsection*{Reset}


\texttt{SaveAlias}, \texttt{SaveExtend}, and their loading counterparts
don't re-set the tables of objects seen so far between runs.

\begin{enumerate}

\item 

If we construct one saver object and use it repeatedly on different data,
    can it create incorrect or misleading archives?



\item 

What about loading?
    If we re-use a loader,
    can it construct objects that aren't what they should be?



\item 

Create new classes \texttt{SaveReset} and \texttt{LoadReset}
    that fix the problems you have identified.
    How much of the existing code did you have to change?



\end{enumerate}

\subsection*{A Dangling Colon}


Why is there a colon at the end of the line \texttt{alias:12345678:}
when we create an alias marker?

\subsection*{Versioning}


We now have several versions of our data storage format.
Early versions of our code can't read the archives created by later ones,
and later ones can't read the archives created early on
(which used two fields per line rather than three).
This problem comes up all the time in long-lived libraries and applications,
and the usual solution is to include some sort of version marker
at the start of each archive
to indicate what version of the software created it
(and therefore how it should be read).
Modify the code we have written so far to do this.

\subsection*{Who Calculates?}


Why doesn't \texttt{LoadAlias.load} calculate object IDs?
Why does it use the IDs saved in the archive instead?

\subsection*{Fallback}

\begin{enumerate}

\item 

Modify \texttt{LoadExtend} so that
    if the user didn't provide the class needed to reconstruct some archived data,
    the \texttt{load} method returns a simple dictionary instead.



\item 

Why is this a bad idea?



\end{enumerate}

\subsection*{Looking Around}


\chapref{tester} introduced the function \texttt{globals},
which can be used to look up everything defined at the top level of a program.

\begin{enumerate}

\item 

Modify \texttt{LoadExtend} so that it looks for classes using \texttt{globals}
    rather than requiring the caller to pass in
    the classes it's allowed to use.



\item 

Why is this a bad idea?



\end{enumerate}

\subsection*{Removing Exceptions}


Rewrite \texttt{LoadExtend} so that it doesn't use exceptions
when \texttt{\_aliased}, \texttt{\_builtin}, and \texttt{extension} decide
they aren't the right method to handle a particular case.
Is the result simpler or more complex than the exception-based approach?

\subsection*{Self-Referential Objects}


Suppose an object contains a reference to itself:

\begin{lstlisting}[frame=single,frameround=tttt]
class Example:
    def __init__(self):
        self.ref = None

ex = Example()
ex.ref = ex
\end{lstlisting}

\begin{enumerate}

\item 

Why can't \texttt{SaveExtend} and \texttt{LoadExtend} handle this correctly?



\item 

How would they have to be changed to handle this?



\end{enumerate}

\chapter{Chapter 14: Binary Storage}\label{binary}

\begin{itemize}

\item FIXME

\end{itemize}


\noindent 
    Terms defined:
    \glossref{absolute error}, \glossref{ANSI character encoding}, \glossref{ASCII character encoding}, \glossref{big endian}, \glossref{binary mode}, \glossref{bit mask}, \glossref{bit shifting}, \glossref{code point}, \glossref{control code}, \glossref{exclusive or}, \glossref{hexadecimal}, \glossref{little endian}, \glossref{relative error}, \glossref{sign and magnitude}, \glossref{Unicode}, \glossref{UTF-32}, \glossref{UTF-8}, \glossref{variable-length encoding}, \glossref{word (of memory)}



Python and other high-level languages shield programmers from low-level details,
but sooner or later someone has to worry about bits and bytes.
This chapter explores computers represent numbers and text
and how to work with binary data.

\section{Integers}\label{binary-int}


Let's start by looking at how integers are stored.
The natural way to do this with ones and zeroes is to use base 2,
so 1001 in binary is (1×8)+(0×4)+(0×2)+(1×1) or 9 base 10.
We can handle negative numbers by reserving the top bit for the sign,
so that 01001 is +9 and 11001 is -9.


This representation has two drawbacks.
The minor one is that it gives us two zeroes,
one positive and one negative.
More importantly,
the hardware needed to do arithmetic
on this \glossref{sign and magnitude} representation
is more complicated than the hardware needed for another scheme
called \{\% g twos\_complement "two's complement" \%].
Instead of mirroring positive values,
two's complement rolls over when going below zero like an odometer.
For example,
with three-bit integers we get the values in \tblref{binary-3bit}

\begin{table}
\begin{tabular}{ll}
\textbf{\underline{Base 10}} & \textbf{\underline{Base 2}} \\
3 & 011 \\
2 & 010 \\
1 & 001 \\
0 & 000 \\
-1 & 111 \\
-2 & 110 \\
-3 & 101 \\
-4 & 100 \\
\end{tabular}
\caption{3-bit integer values}
\label{binary-3bit}
\end{table}



We can still tell whether a number is positive or negative
by looking at the first bit:
negative numbers have a 1, positives have a 0.
However,
two's complement is asymmetric:
since 0 counts as a positive number,
numbers go from -4 to 3, or -16 to 15, and so on.
As a result,
even if \texttt{x} is a valid number,
\texttt{-x} may not be.

\subsection*{Writing Binary}


We can write binary numbers directly in Python by using the \texttt{0b} prefix.
For example, \texttt{0b0011} is the number 3.
Programmers usually use \glossref{hexadecimal} (base 16) instead:
the digits 0–9 have the usual meaning,
and the letters A-F (or a-f) are used to represent the numbers 11–15.
We signal that we're using hexadecimal with a \texttt{0x} prefix,
so \texttt{0xF7} is (15×16)+7 or 247.


Each hexadecimal digit corresponds to four bits (\tblref{binary-hex}),
which makes it easy to translate bits to digits and vice versa:
for example,
\texttt{0xF7} is \texttt{0b11110111}.
As a bonus,
two hexadecimal digits is exactly one byte.

\begin{table}
\begin{tabular}{lll}
\textbf{\underline{Decimal}} & \textbf{\underline{Hexadecimal}} & \textbf{\underline{Bits}} \\
0 & 0 & 0000 \\
1 & 1 & 0001 \\
2 & 2 & 0010 \\
3 & 3 & 0011 \\
4 & 4 & 0100 \\
5 & 5 & 0101 \\
6 & 6 & 0110 \\
7 & 7 & 0111 \\
8 & 8 & 1000 \\
9 & 9 & 1001 \\
10 & A & 1010 \\
11 & B & 1011 \\
12 & C & 1100 \\
13 & D & 1101 \\
14 & E & 1110 \\
15 & F & 1111 \\
\end{tabular}
\caption{Bitwise operations}
\label{binary-hex}
\end{table}


\section{Bitwise Operations}\label{binary-bitops}


Like most languages based on C,
Python provides four operators for working with bits:
\texttt{\&} (and),
\texttt{|} (or),
\texttt{{\textasciicircum}} (xor),
and \texttt{~} (not).
\texttt{\&} yields a 1 only if both its arguments are 1's,
while \texttt{|} yields 1 if either or both of its arguments are 1.
\texttt{{\textasciicircum}}, called \glossref{exclusive or} or "xor" (pronounced "ex-or"),
produces 1 if either but \emph{not} both of its arguments are 1.
Putting it another way,
\texttt{{\textasciicircum}} produces 0 if its inputs are the same and 1 if they are different.
Finally,
\texttt{~} flips its argument: 1 becomes 0, and 0 becomes 1.
When these operators are used on multibit values
they work on corresponding bits independently as shown in \tblref{binary-ops}.

\begin{table}
\begin{tabular}{lll}
\textbf{\underline{Expression}} & \textbf{\underline{Bitwise}} & \textbf{\underline{Result}} \\
\texttt{12 \& 6} & \texttt{1100 \& 0110} & \texttt{4} (\texttt{0100}) \\
\texttt{12 | 6} & \texttt{1100 | 0110} & \texttt{14} (\texttt{1110}) \\
\texttt{12 {\textasciicircum} 6} & \texttt{1100 {\textasciicircum} 0110} & \texttt{10} (\texttt{1010}) \\
\texttt{~ 6} & \texttt{~ 0110} & \texttt{9} (\texttt{1001}) \\
\texttt{12 << 2} & \texttt{1100 << 2} & \texttt{48} (\texttt{110000}) \\
\texttt{12 >> 2} & \texttt{1100 >> 2} & \texttt{3} (\texttt{0011}) \\
\end{tabular}
\caption{Bitwise operations}
\label{binary-ops}
\end{table}



We can set or clear individual bits with these operators.
To set a particular bit,
create a value in which that bit is 1 and the rest are 0.
When this is or'd with a value,
the bit we set is guaranteed to come out 1;
the other bits will be left as they are.
Similarly,
to  set a bit to zero,
create a \glossref{mask} in which that bit is 0 and the others are 1,
then use \texttt{\&} to combine the two.
To make things easier to read,
programmers often set a single bit,
negative it with \texttt{~},
and then use \texttt{\&}:


\begin{lstlisting}[frame=single,frameround=tttt]
mask = ~ 0x0100  # binary 1111 1110 1111 1111
val = val & mask #    clears this ^ bit
\end{lstlisting}



Most C-inspired languages also provide \glossref{bit shifting} operators.
that move bits left or right.
Shifting the bits \texttt{0110} left by one place produces \texttt{1100},
while shifting it right by one place produces \texttt{0011}.
In Python,
this is written \texttt{x << 1} or \texttt{x >> 1}.


Just as shifting a decimal number left corresponds to multiplying by 10,
shifting a binary number left is the same as multiplying it by 2.
Similarly,
shifting a number right corresponds to dividing by 2 and throwing away the remainder,
so \texttt{17 >> 3} is 2.


But what if the top bit of an integer changes from 1 to 0 or vice versa as a result of shifting?
If we're using two's complement,
then the bits \texttt{1111} represent the value -1;
if we shift right we get \texttt{0111} which is 7.
Similarly,
if we shift \texttt{0111} to the left we get \texttt{1110} (assuming we fill in the bottom with 0),
which is -2.


Different languages deal with this problem in different ways.
Python always fills with zeroes.
Java provides two versions of right shift:
\texttt{>>} fills in the high end with zeroes
while \texttt{>>>} copies in the topmost (sign) bit of the original value.
C (and by extension C++) lets the underlying hardware decide,
which means that if you want to be sure of getting a particular answer
you have to handle the top bit yourself.

\section{Text}\label{binary-text}


The rules for storing text make integers look simple.
By the early 1970s most programs used \glossref{ASCII},
which represented unaccented Latin characters using the numbers from 32 to 127.
(The numbers 0 to 31 were used for \glossref{control codes}
such as newline, carriage return, and bell.)
Since computers use 8-bit bytes and the numbers 0–127 only need 7 bits,
programmers were free to use the numbers 128–255 for other characters.
Unfortunately,
different pieces of software used them to represent different symbols:
non-Latin characters,
graphic characters like boxes,
and so on.
The chaos was eventually tamed by the \glossref{ANSI} standard
which (for example) defined the value 231 to mean the character "ç".


But the ANSI standard only solved part of the problem.
The ANSI standard didn't include characters from Turkish, Devanagari, and many other alphabets,
much less the thousands of characters used in some East Asian writing systems.
One solution would have been to use 16 or even 32 bits per character,
but:

\begin{enumerate}

\item existing text files using ANSI would have to be transcribed, and

\item documents would be two or four times larger.

\end{enumerate}


The solution was a new standard called \glossref{Unicode} with two parts.
The first part defined a \glossref{code point} for every character:
U+0065 for an upper-case Latin "A",
U+2605 for a black star,
and so on.
The second part defined ways to store these values in memory.
The simplest of these is \glossref{UTF-32},
which stores every character as a 32-bit number.
This wastes a lot of memory—if the text is written in a Western European language,
UTF-32 uses four times as much storage as necessary—but
since each character is exactly the same size it's very easy to process.


The most popular encoding is \glossref{UTF-8},
which is a \glossref{variable length encoding}.
Every code point from 0 to 127 is stored in a single byte whose high bit is 0,
just as it was in the original ASCII standard.


What if the high bit in the byte is 1?
In that case,
the number of 1's after the high bit but before the first 0
tells UTF-8 how many more bytes that character is using.
For example,
if the first byte of the character is \texttt{11101101} then:

\begin{itemize}

\item the first 1 signals that this is a multi-byte character;

\item the next two 1's signal that the character includes bits
    from the following two bytes as well as this one;

\item the 0 separates the byte count from the first few bits used in the character;
    and

\item the final 1101 is the first four bits of the character.

\end{itemize}


But that's not all:
every byte that's a continuation of a character starts with 10.
This rule means that if we look at any byte in a string
we can immediately tell if it's the start of a character
or the continuation of a character.
Thus,
if we want to represent a character whose code point is 1789:

\begin{itemize}

\item We convert to binary 11011111101.

\item We count and realize that we'll need two bytes:
    the first storing the high 5 bits of the character,
    the second storing the low 6 bits.

\item We encode the high 5 bits as 11011011,
    meaning "start of a character with one continuation byte
    and the 5 payload bits 11011".

\item We encode the low 6 bits as 10111101,
    meaning "a continuation byte with 6 payload bits 111101".

\end{itemize}

\section{Floating Point Numbers}\label{binary-fp}


The rules for floating point numbers make Unicode look simple.
The root of the problem is that
we cannot represent an infinite number of real values
with a finite set of bit patterns.
And no matter what values we represent,
there will be an infinite number of values between each of them that we can't.



Floating point numbers are represented by a sign,
a magnitude,
and an exponent.
In a 32-bit \glossref{word}
the IEEE 754 standard calls for 1 bit of sign,
23 bits for the magnitude (or mantissa),
and 8 bits for the exponent.
We will illustrate how it works using a much smaller representation:
no sign,
3 bits for the magnitude,
and 2 for the exponent.


FIXME


There is a lot of redundancy here
which the IEEE standard avoids by shifting things around.
More importantly,
this format can't represent a lot of values:
for example,
it can store 8 and 10 but not 9.
This is exactly like the problem hand calculators have
with fractions like 1/3:
in decimal, we have to round that to 0.3333 or 0.3334.


But if this scheme has no representation for 9
then 8+1 must be stored as either 8 or 10.
If that's so,
then what is 8+1+1?
If we add from the left,
(8+1)+1 is 8+1 is 8,
but if we add from the right,
8+(1+1) is 8+2 is 10.
Changing the order of operations makes the difference between right and wrong.


The authors of numerical libraries spend a lot of time worrying about things like this.
In this case
sorting the values and then add from smallest to largest
gives the best chance of getting the best possible answer.
In other situations,
like inverting a matrix, the rules are much more complicated.


Another observation about our uneven number line is that
the we can represent are unevenly spaced.
However,
the \emph{relative} spacing between each set of values stays the same:
the first group is separated by 1,
then the separation becomes 2,
then 4,
and so on.
This points us at a couple of useful definitions:

\begin{itemize}

\item 

The \glossref{absolute error} in an approximation
    is the absolute value of the difference
    between the approximation and the actual value.



\item 

The \glossref{relative error}
    is the ratio of the absolute error
    to the absolute value we're approximating.



\end{itemize}


For example,
being off by 1 in approximating 8+1 and 56+1 is the same absolute error,
but the relative error is larger in the first case than in the second.
Relative error is almost always more useful than absolute:
it makes little sense to say that we're off by a hundredth
when the value in question is a billionth.


One implication of this is that
we should never compare floating point numbers with \texttt{==} or \texttt{!=}
because two numbers calculated in different ways
will probably not have exactly the same bits.
It's safe to use \texttt{<}, \texttt{>=}, and other orderings,
though,
since they don't depend on being the same down to the last bit.


If we do want to compare floating point numbers
we can use something like \hreffoot{the \texttt{approx} class}{https://docs.pytest.org/en/4.6.x/reference.html\#pytest-approx} from \hreffoot{pytest}{https://docs.pytest.org/}
which checks whether two numbers are within some tolerance of each other.
A completely different approach is to use something like
the \hreffoot{fractions}{https://docs.python.org/3/library/fractions.html} module,
which (as its name suggests) uses numerators and denominators
to avoid some precision issues.
\hreffoot{This post}{https://www.textualize.io/blog/posts/7-things-about-terminals} describes one clever use of the module.

\section{And Now, Persistence}\label{binary-binary}


So why store data in a format that can't be edited with Notepad or nano?
There are generally four reasons:


\noindent \textbf{Size}: 
The string \texttt{"10239472"} is 8 bytes long,
but the 32-bit integer it represents only needs 4 bytes in memory.
This doesn't matter for small data sets,
but it does for large ones,
and it definitely does when data has to move between disk and memory
or between different computers.


\noindent \textbf{Speed}: 
Adding the integers 34 and 56 is a single machine operation.
Adding the values represented by the strings \texttt{"34"} and \texttt{"56"} is dozens;
we'll explore this in the exercises.
Most programs that read and write text files
convert the values in those files into binary data
using something like the \texttt{int} or \texttt{float} functions,
but if we're going to process the data many times,
it makes sense to avoid paying the conversion cost over and over.


\noindent \textbf{Hardware}: 
Someone, somewhere, has to convert the signal from the thermocouple to a number,
and that signal probably arrives arrives as a stream of 1's and 0's.


\noindent \textbf{Lack of anything better}: 
It's possible to represent images as ASCII art, but sound?
Or video?
It would be possible, but it would hardly be sensible.




Most programs use line-oriented file I/O:
they read characters until they see an end-of-line marker
and then hand back those characters as a string.
We can also use byte-oriented routines,
the most basic of which is simply called \texttt{read}.
If \texttt{stream} is an open file,
then \texttt{stream.read(N)} hands back up to the next N bytes from the file
("up to", because there might not be that much data left).
The result is returned as a string,
but—and this is crucial—there is no guarantee that the values represent characters.
We can concatenate other data onto it,
but if the underlying file is a PNG image,
text-oriented methods like \texttt{string.upper}
won't do anything meaningful.


FIXME


Where there's a \texttt{read} there's a \texttt{write}.
\texttt{stream.write(str)} writes the bytes in the string \texttt{str} to a file that has been opened for writing.
In both the reading and writing cases,
though,
it's very important to open the file in \glossref{binary mode} using either:

\begin{lstlisting}[frame=single,frameround=tttt]
reader = open('input.dat', 'rb')
\end{lstlisting}


\noindent or:

\begin{lstlisting}[frame=single,frameround=tttt]
writer = open('input.dat', 'wb')
\end{lstlisting}


The \texttt{"b"} at the end of the mode string tells Python
\emph{not} to translate Windows line endings (which are the two characters \texttt{"{\textbackslash}r{\textbackslash}n"})
into Unix line endings (the single character \texttt{"{\textbackslash}n"}).
This translation is handy when we're working with text,
since it means our programs only have to deal with one style of line
ending no mater what platform the code is running on,
but it messes up non-textual data.


There's another problem here as well.
C and Fortran store integers as "naked" 32-bit values:
the program uses what the machine provides,
no more and no less.
Python and other dynamic languages usually don't use raw values.
Instead,
they put the value in a larger data structure
that keeps track of its type along with a bit of extra administrative information.
That extra data allows those languages to do garbage collection.
It also allows us to assign values to variables without explicitly declaring their type,
since the value we're assigning carries its type along with it.


A similar issue comes up when we compare Fortran's arrays to Python's lists.
Fortran stores the data in an array side by side in one big block of memory.
Writing this to disk is easy:
if the array starts at location L in memory and has N values,
each of which is B bytes long,
we just copy the bytes from L to L+NB-1 to the file.


A Python list,
on the other hand,
stores pointers to values rather than the values themselves.
To put the values in a file
we can either write them one at a time
or pack them into a contiguous block and write that.
Similarly,
when reading from a file,
we can either grab the values one by one
or read a larger block and then unpack it in memory.


Packing data is a lot like formatting values for textual output.
The format specifies what types of data are being packed,
how big they are (e.g., is this a 32-bit or 64-bit floating point number?),
and how many values there are.
The format exactly determines how much memory is required by the packed representation.
The result of packing values is a block of bytes,
which Python represents as a string,
but as mentioned above,
this isn't a string of characters.


Unpacking reverses this process.
After reading data into memory
we can unpack it according to a format.
The most important thing is that
\emph{we can unpack data any way we want}.
We might pack an integer and then unpack it as four characters,
since both are 32 bits long.
Or we might save two characters,
an integer,
and two more characters,
then unpack it as a 64-bit floating point number.
The bits are just bits:
it's our responsibility to make sure we keep track of their meaning
when they're down there on disk.


In Python we can use the \texttt{struct} module to pack and unpack data.
The function \texttt{pack(format, val\_1, val\_2, ...)}
takes a format string and a bunch of values as arguments,
packs them into a string,
and gives that back to us.
The inverse function, \texttt{unpack(format, string)} takes such a string and a format
and returns a tuple containing the unpacked values.
Here's an example:


\begin{lstlisting}[frame=single,frameround=tttt]
import struct

fmt = "ii" # two 32-bit integers
x = 31
y = 65

binary = struct.pack(fmt, x, y)
print("binary representation:", repr(binary))

normal = struct.unpack(fmt, binary)
print("back to normal:", normal)
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
binary representation: b'\x1f\x00\x00\x00A\x00\x00\x00'
back to normal: (31, 65)
\end{lstlisting}



What is \texttt{{\textbackslash}x1f} and why is it in our data?
If Python finds a character in a string that doesn't have a printable representation,
it prints a 2-digit escape sequence in \glossref{hexadecimal} (base 16).
This uses the letters A-F (or a-f) to represent the digits from 10 to 15,
so that (for example) \texttt{3D5} is (3×16{\textasciicircum}2{\textasciicircum})+(13×16{\textasciicircum}1{\textasciicircum})+(5×16{\textasciicircum}0{\textasciicircum}), or 981 in decimal.
Python is therefore telling us that
our string contains the eight bytes
\texttt{[{\textquotesingle}{\textbackslash}x1f{\textquotesingle}, {\textquotesingle}{\textbackslash}x00{\textquotesingle}, {\textquotesingle}{\textbackslash}x00{\textquotesingle}, {\textquotesingle}{\textbackslash}x00{\textquotesingle}, {\textquotesingle}A{\textquotesingle}, {\textquotesingle}{\textbackslash}x00{\textquotesingle}, {\textquotesingle}{\textbackslash}x00{\textquotesingle}, {\textquotesingle}{\textbackslash}x00{\textquotesingle}]}.
\texttt{1F} in hex is (1×16{\textasciicircum}1{\textasciicircum})+(15×16{\textasciicircum}0{\textasciicircum}), or 31;
\texttt{{\textquotesingle}A{\textquotesingle}} is our 65,
because the ASCII code for an upper-case letter A is the decimal value 65.
All the other bytes are zeroes (\texttt{"{\textbackslash}x00"})
because each of our integers is 32 bits long
and the significant digits only fill one byte's worth of each.

\begin{table}
\begin{tabular}{ll}
\textbf{\underline{Format}} & \textbf{\underline{Meaning}} \\
\texttt{"c"} & Single character (i.e., string of length 1) \\
\texttt{"B"} & Unsigned 8-bit integer \\
\texttt{"h"} & Short (16-bit) integer \\
\texttt{"i"} & 32-bit integer \\
\texttt{"f"} & 32-bit float \\
\texttt{"d"} & Double-precision (64-bit) float \\
\end{tabular}
\caption{`struct` package formats}
\label{binary-formats}
\end{table}



The \texttt{struct} module offers a lot of different formats,
some of which are shown in \tblref{binary-formats}.
The \texttt{"B"}, \texttt{"h"}, and \texttt{"2"} formats deserve some explanation.
\texttt{"B"} takes the least significant 8 bits out of an integer and packs those;
\texttt{"h"} takes the least significant 16 bits and does likewise.
They're needed because binary data formats often store only as much data as they need to,
so we need a way to get 8- and 16-bit values out of files.
(Many audio formats,
for example,
only store 16 bits per sample.)


Any format can be preceded by a count,
so the format \texttt{"3i"} means "three integers":


\begin{lstlisting}[frame=single,frameround=tttt]
from struct import pack

print(pack("3i", 1, 2, 3))
print(pack("5s", bytes("hello", "utf-8")))
print(pack("5s", bytes("a longer string", "utf-8")))
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
b'\x01\x00\x00\x00\x02\x00\x00\x00\x03\x00\x00\x00'
b'hello'
b'a lon'
\end{lstlisting}



We get the wrong answer in the last call
because we only told Python to pack five characters.
How can we tell it to pack all the data that's there regardless of length?


The short answer is that we can't:
we must specify how much we want packed.
But that doesn't mean we can't handle variable-length strings;
it just means that we have to construct the format on the fly:

\begin{lstlisting}[frame=single,frameround=tttt]
format = f"{len(str)}s"
\end{lstlisting}


\texttt{len(str)} is just the length of the string \texttt{str},
so if \texttt{str} contains the string \texttt{"example"},
the expression above will assign the string \texttt{"7s"} to \texttt{format},
which just happens to be exactly the right format to use to pack it.


That's fine when we're writing,
but how do we know how much data to get if we're reading?
For example, suppose we have the two strings "hello" and "Python".
we can pack them like this:

\begin{lstlisting}[frame=single,frameround=tttt]
pack('5s6s', 'hello', 'Python')
\end{lstlisting}


\noindent but how do I know how to unpack 5 characters then 6?
The trick is to save the size along with the data.
If we always use exactly the same number of bytes to store the size,
we can read it back safely,
then use it to figure out how big our string is:

\begin{lstlisting}[frame=single,frameround=tttt]
>>> def pack_string(str):
...     header = pack('i', len(str))
...     body_format = '%ds' % len(str)
...     body = pack(body_format, str)
...     return header + body
...
>>> pack_string('hello')
'\x05\x00\x00\x00hello'
\end{lstlisting}


The unpacking function is almost the same:

\begin{lstlisting}[frame=single,frameround=tttt]
>>> def unpack_string(buffer):
...    header, body = buffer[:4], buffer[4:]
...    unpacked_header = unpack('i', header)
...    length = unpacked_header[0]
...    body_format = '%ds' % length
...    result = unpack(body_format, body)
...    return result
\end{lstlisting}


First, we break the buffer into two parts:
a header that's exactly four bytes long
(i.e., the right size for an integer)
and a body made up of whatever's left.
We then unpack the header,
whose format we know,
to determine how many characters are in the string.
Once we've got that we use the trick shown earlier
to construct the right format on the fly
and then unpack the string and return it.


Something else to notice here is that
the least significant byte of an integer comes first.
This is called \glossref{little-endian} and is used by all Intel processors.
Some other processors put the most significant byte first,
which is called \glossref{big-endian}.
There are pro's and con's to both, which we won't go into here.
What you \emph{do} need to know is that if you move data from one architecture to another,
it's your responsibility to flip the bytes around,
because the machine doesn't know what the bytes mean.
This is such a pain that the \texttt{struct} library and other libraries like
will do things for you if you ask it to.
If you're using \texttt{struct},
the first character of a format string optionally indicates the byte order
(\tblref{binary-endian}).

\begin{table}
\begin{tabular}{llll}
\textbf{\underline{Character}} & \textbf{\underline{Byte order}} & \textbf{\underline{Size}} & \textbf{\underline{Alignment}} \\
\texttt{@} & native & native & native \\
\texttt{=} & native & standard & none \\
\texttt{<} & little & endian & standard none \\
\texttt{>} & big & endian & standard none \\
\texttt{!} & network & standard & none \\
\end{tabular}
\caption{`struct` package endian indicators}
\label{binary-endian}
\end{table}



You should also use the \texttt{struct} library's \texttt{calcsize} function,
which tells you how large (in bytes) the data produced or consumed by a format will be.
For example:

\begin{lstlisting}[frame=single,frameround=tttt]
>>> calcsize('4s')
4

>>> calcsize('3i4s5d')
56
\end{lstlisting}


Binary data is to programming what chemistry is to biology:
you don't want to spend any more time thinking at its level than you have to,
but there's no substitute when you \emph{do} have to.
Please remember that libraries already exist to handle almost every binary format ever created
and to read data from almost every instrument on the market.
You shouldn't worry about 1's and 0's unless you really have to.

\section{Exercises}\label{binary-exercises}


FIXME

\subsection*{Adding strings}


Write a function that takes two strings of digits
and adds them as if they were numbers
\emph{without} actually converting them to numbers.
For example,
\texttt{add\_str("12", "5")} should produce the string \texttt{"17"}.

\subsection*{Roundoff}

\begin{enumerate}

\item Write a program that loops over the integers from 1 to 9
    and uses them to create the values 0.9, 0.09, and so on.

\item Calculate the same values by subtracting 0.1 from 1,
    then subtracting 0.01,
    and so on.

\item Calculate the absolute and relative differences between corresponding values
    (which should be identical).

\item Repeat the exercise using the \texttt{Fraction} class
    from the \hreffoot{fractions}{https://docs.python.org/3/library/fractions.html} module.

\end{enumerate}

\chapter{Chapter 15: HTML Templating}\label{templating}

\begin{itemize}

\item FIXME

\end{itemize}


\noindent 
    Terms defined:
    \glossref{abstract method}, \glossref{Application Programming Interface}, \glossref{dynamic scoping}, \glossref{environment}, \glossref{lexical scoping}, \glossref{stack frame}, \glossref{static site generator}, \glossref{Visitor pattern}



Every program needs documentation in order to be usable,
and the best place to put that documentation is on the web.
Writing and updating pages by hand is time-consuming and error-prone,
particularly when many parts are the same,
so most documentation sites use some kind of
\glossref{static site generator}\index{static site generator}
to create web pages from templates.


At the heart of every static site generator is a page templating system.
Thousands of these have been written in the last thirty years
in every popular programming language
(and one language, \hreffoot{PHP}{https://www.php.net/}\index{PHP}, was created for this purpose).
Most of these systems use one of three designs
(\figref{templating-options}):

\begin{enumerate}

\item 

Mix commands in a language such as JavaScript with the HTML or Markdown
    using some kind of marker to indicate which parts are commands
    and which parts are to be taken as-is.
    This approach is taken by \hreffoot{EJS}{https://ejs.co/}\index{EJS}.



\item 

Create a mini-language with its own commands like \hreffoot{Jekyll}{https://jekyllrb.com/}\index{Jekyll}.
    Mini-languages are appealing because they are smaller and safer than general-purpose languages,
    but eventually they acquire most of the features of a general-purpose language.
    Again, some kind of marker must be used to show
    which parts of the page are code and which are ordinary text.



\item 

Put directives in specially-named attributes in the HTML.
    This approach has been the least popular,
    but since pages are valid HTML,
    it eliminates the need for a special parser.



\end{enumerate}

\figpdf{templating-options}{./templating/options.pdf}{Three different ways to implement page templating.}{0.6}


This chapter builds a simple page templating system using the third strategy.
We will process each page independently by parsing the HTML
and walking the DOM\index{DOM} to find nodes with special attributes.
Our program will execute the instructions in those nodes
to implement loops and if/else statements;
other nodes will be copied as-is to create text.

\section{Syntax}\label{templating-syntax}


Let's start by deciding what "done" looks like.
Suppose we want to turn an array of strings into an HTML list.
Our page will look like this:


FIXME


\noindent The attribute \texttt{z-loop} tells the tool to repeat the contents of that node;
the loop variable and the collection being looped over are separated by a colon.
The \texttt{span} with the attribute \texttt{z-var}
tells the tool to fill in the node with the value of the variable.


When our tool processes this page,
the output will be standard HTML without any traces of how it was created:


FIXME



The next step is to define the \glossref{API} for filling in templates.
Our tool needs the template itself,
somewhere to write its output,
and some variables to use in the expansion.
These variables might come from a configuration file,
from a YAML header in the file itself,
or from some mix of the two;
for the moment,
we will just pass them into the expansion function as an object:


\begin{lstlisting}[frame=single,frameround=tttt]
variables = {
    "names": ["Johnson", "Vaughan", "Jackson"]
}

dom = read_html("template.html")
expander = new Expander(dom, variables)
expander.walk()
print(expander.result)
\end{lstlisting}


\section{Managing Variables}\label{templating-values}


As soon as we have variables we need a way to track their values.
These values might change;
for example,
a loop variable's value can change each time the loop runs.
We also need to maintain multiple sets of variables
so that variables used inside a loop
don't conflict with ones used outside it.
(We don't actually "need" to do this—we could just have one global set of variables—but
if all our variables are global,
all of our programs will be buggy.)


The standard way to manage variables is to create a stack of lookup tables.
Each \glossref{stack frame}\index{stack frame} is a dictionary;
when we need a variable,
we search the stack frames in order to find it.



The values in a running program are sometimes called
an \glossref{environment}\index{environment (to store variables)}\index{call stack!environment},
so we call our stack-handling class \texttt{Env}.
Its methods let us push and pop new stack frames
and find a variable given its name;
if the variable can't be found,
\texttt{Env.find} returns \texttt{None} instead of throwing an exception
(\figref{templating-stack}).


\begin{lstlisting}[frame=single,frameround=tttt]
class Env:

    def __init__(self, initial):
        self.stack = [initial.copy()]

    def push(self, frame):
        self.stack.append(frame)

    def pop(self):
        self.stack.pop()

    def find(self, name):
        for frame in reversed(self.stack):
            if name in frame:
                return frame[name]
        return None

    def __str__(self):
        return str(self.stack)
\end{lstlisting}


\figpdf{templating-stack}{./templating/stack.pdf}{Using a stack to manage variables.}{0.6}

\section{Visiting Nodes}\label{templating-nodes}


HTML pages have a nested structure,
so we will process them using
the \glossref{Visitor}\index{Visitor pattern}\index{design pattern!Visitor} design pattern.
\texttt{Visitor}'s constructor takes the root node of the DOM tree as an argument and saves it.
When we call \texttt{Visitor.walk} without a value,
it starts recursing from that saved root;
if \texttt{.walk} is given a value (as it is during recursive calls),
it uses that instead.


\begin{lstlisting}[frame=single,frameround=tttt]
from abc import ABC, abstractmethod

class Visitor(ABC):

    def __init__(self, root):
        self.root = root

    def walk(self, node=None):
        if node is None:
            node = self.root
        if self.open(node):
            for child in node.children:
                self.walk(child)
        self.close(node)

    @abstractmethod
    def open(self, node):
        pass

    @abstractmethod
    def close(self, node):
        pass
\end{lstlisting}



\noindent \texttt{Visitor} defines two \glossref{abstract methods} \texttt{open} and \texttt{close}
that are called when we first arrive at a node and when we are finished with it
(\figref{templating-visitor}).
Any class derived from \texttt{Visitor} must defined these two methods.

\figpdf{templating-visitor}{./templating/visitor.pdf}{Using the Visitor pattern to evaluate a page template.}{0.6}


The \texttt{Expander} class is specialization of \texttt{Visitor}
that uses an \texttt{Env} to keep track of variables.
It imports a handler
for each type of special node we support—we will write those in a moment—and
uses them to process each type of node:

\begin{enumerate}

\item 

If the node is plain text, copy it to the output.



\item 

If there is a handler for the node,
    call the handler's \texttt{open} or \texttt{close} method.



\item 

Otherwise, open or close a regular tag.



\end{enumerate}


FIXME


To check if there is a handler for a particular node and get that handler
we just look at the node's attributes:


\begin{lstlisting}[frame=single,frameround=tttt]
    def hasHandler(self, node):
        return any(name in self.handlers for name in node.attrs)

    def getHandler(self, node):
        possible = [name for name in node.attrs if name in self.handlers]
        assert len(possible) == 1, \
            f"Should be exactly one handler"
        return self.handlers[possible[0]]
\end{lstlisting}



Finally, we need a few helper methods to show tags and generate output:


\begin{lstlisting}[frame=single,frameround=tttt]
    def showTag(self, node, closing):
        if closing:
            self.output(f"</{node.name}>")
            return
        self.output(f"<{node.name}")
        for name in node.attrs:
            if not name.startswith("z-"):
                self.output(f' {name}="{node.attrs[name]}"')
        self.output(">")

    def output(self, text):
        self.result.append("UNDEF" if text is None else text)

    def getResult(self):
        return "".join(self.result)
\end{lstlisting}



\noindent Notice that this class adds strings to an array and joins them all right at the end
rather than concatenating strings repeatedly.
Doing this is more efficient and also helps with debugging,
since each string in the array corresponds to a single method call.

\section{Implementing Handlers}\label{templating-handlers}


At this point
we have built a lot of infrastructure but haven't actually processed any special nodes.
To do that,
let's write a handler that copies a constant number into the output:


\begin{lstlisting}[frame=single,frameround=tttt]
class z_num:
    @staticmethod
    def open(expander, node):
        expander.showTag(node, False)
        expander.output(node.attrs["z-num"])

    @staticmethod
    def close(expander, node):
        expander.showTag(node, True)
\end{lstlisting}



\noindent The \texttt{z\_num} expander is a class,
but we don't plan to create instances of it.
Instead,
it's just a way to store two functions named \texttt{open} and \texttt{close}.
When we enter a node like \texttt{<span z-num="123"/>}
this handler asks the expander to show an opening tag
followed by the value of the \texttt{z-num} attribute.
When we exit the node,
the handler asks the expander to close the tag.
The handler doesn't know whether things are printed immediately,
added to an output list,
or something else;
it just knows that whoever called it implements the low-level operations it needs.


So much for constants; what about variables?


\begin{lstlisting}[frame=single,frameround=tttt]
class z_var:
    @staticmethod
    def open(expander, node):
        expander.showTag(node, False)
        expander.output(expander.env.find(node.attrs["z-var"]))

    @staticmethod
    def close(expander, node):
        expander.showTag(node, True)
\end{lstlisting}



\noindent This code is almost the same as the previous example.
The only difference is that instead of copying the attribute's value
directly to the output,
we use it as a key to look up a value in the environment.


These two pairs of handlers look plausible, but do they work?
To find out,
we can build a program that loads variable definitions from a JSON file,
reads an HTML template,
and does the expansion:


\begin{lstlisting}[frame=single,frameround=tttt]
import json
import sys

from bs4 import BeautifulSoup

from expander import Expander


def read_json(filename):
    with open(filename, "r") as reader:
        return json.load(reader)


def read_template(filename):
    with open(filename, "r") as reader:
        doc = BeautifulSoup(reader.read(), "html.parser")
        return doc.find("html")


def main():
    variables = read_json(sys.argv[1])
    doc = read_template(sys.argv[2])
    expander = Expander(doc, variables)
    expander.walk()
    print(expander.getResult())


if __name__ == "__main__":
    main()
\end{lstlisting}



We added new variables for our test cases one by one
as we were writing this chapter.
To avoid repeating text repeatedly,
we show the entire set once:


\begin{lstlisting}[frame=single,frameround=tttt]
{
  "firstVariable": "firstValue",
  "secondVariable": "secondValue",
  "variableName": "variableValue",
  "showThis": true,
  "doNotShowThis": false,
  "names": ["Johnson", "Vaughan", "Jackson"]
}
\end{lstlisting}



Our first test:
is static text copied over as-is?


\begin{lstlisting}[frame=single,frameround=tttt]
<html>
  <body>
    <h1>Static Text</h1>
    <p>This page has:</p>
    <ul>
      <li>static</li>
      <li>text</li>
    </ul>
  </body>
</html>
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
<html>
<body>
<h1>Static Text</h1>
<p>This page has:</p>
<ul>
<li>static</li>
<li>text</li>
</ul>
</body>
</html>
\end{lstlisting}



Good.
Now, does the expander handle constants?


\begin{lstlisting}[frame=single,frameround=tttt]
<html>
  <body>
    <p><span z-num="123"/></p>
  </body>
</html>
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
<html>
<body>
<p><span>123</span></p>
</body>
</html>
\end{lstlisting}



What about a single variable?


\begin{lstlisting}[frame=single,frameround=tttt]
<html>
  <body>
    <p><span z-var="variableName"/></p>
  </body>
</html>
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
<html>
<body>
<p><span>variableValue</span></p>
</body>
</html>
\end{lstlisting}



What about a page containing multiple variables?
There's no reason it should fail if the single-variable case works,
but we should still check—again,
software isn't done until it has been tested.


\begin{lstlisting}[frame=single,frameround=tttt]
<html>
  <body>
    <p><span z-var="firstVariable" /></p>
    <p><span z-var="secondVariable" /></p>
  </body>
</html>
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
<html>
<body>
<p><span>firstValue</span></p>
<p><span>secondValue</span></p>
</body>
</html>
\end{lstlisting}


\section{Control flow}\label{templating-flow}


Our tool supports two types of control flow:
conditional expressions and loops.
Since we don't support Boolean expressions like \texttt{and} and \texttt{or},
implementing a conditional is as simple as looking up a variable
and then expanding the node if the value is true:


\begin{lstlisting}[frame=single,frameround=tttt]
class z_if:
    @staticmethod
    def open(expander, node):
        check = expander.env.find(node.attrs["z-if"])
        if check:
            expander.showTag(node, False)
        return check

    @staticmethod
    def close(expander, node):
        if expander.env.find(node.attrs["z-if"]):
            expander.showTag(node, True)
\end{lstlisting}



Let's test it:


\begin{lstlisting}[frame=single,frameround=tttt]
<html>
  <body>
    <p z-if="showThis">This should be shown.</p>
    <p z-if="doNotShowThis">This should <em>not</em> be shown.</p>
  </body>
</html>
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
<html>
<body>
<p>This should be shown.</p>

</body>
</html>
\end{lstlisting}




Finally we have loops.
For these,
we need to get the array we're looping over from the environment
and do something for each of its elements.
That "something" is:

\begin{enumerate}

\item 

Create a new stack frame holding the current value of the loop variable.



\item 

Expand all of the node's children with that stack frame in place.



\item 

Pop the stack frame to get rid of the temporary variable.



\end{enumerate}


\begin{lstlisting}[frame=single,frameround=tttt]
class z_loop:
    @staticmethod
    def open(expander, node):
        index_name, target_name = node.attrs["z-loop"].split(':')
        # delete node.attrs['z-loop']
        expander.showTag(node, False)
        target = expander.env.find(target_name)
        for value in target:
            expander.env.push({index_name: value})
            for child in node.children:
                expander.walk(child)
            expander.env.pop()
        return False

    @staticmethod
    def close(expander, node):
        expander.showTag(node, True)
\end{lstlisting}



Once again,
it's not done until we test it:


\begin{lstlisting}[frame=single,frameround=tttt]
<html>
  <body>
    <p>Expect three items</p>
    <ul z-loop="item:names">
      <li><span z-var="item"/></li>
    </ul>
  </body>
</html>
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
<html>
<body>
<p>Expect three items</p>
<ul>
<li><span>Johnson</span></li>

<li><span>Vaughan</span></li>

<li><span>Jackson</span></li>
</ul>
</body>
</html>
\end{lstlisting}


\section{How We Got Here}\label{templating-learning}


We have just implemented a simple programming language.
It can't do arithmetic,
but if we wanted to add tags like:

\begin{lstlisting}[frame=single,frameround=tttt]
<span z-math="+"><span z-var="width"/><span z-num="1"//>
\end{lstlisting}


\noindent we could.
It's unlikely anyone would use the result—typing all of that
is so much clumsier than typing \texttt{width+1} that people wouldn't use it
unless they had no other choice—but the basic design is there.


We didn't invent any of this from scratch,
any more than we invented the parsing algorithm of \chapref{parser}
or the simple interpreter in \chapref{interpreter}.
Instead,
we did what you are doing now:
we read what other programmers had written
and tried to make sense of the key ideas.


The problem is that "making sense" depends on who we are.
When we use a low-level language,
we incur the cognitive load\index{cognitive load} of assembling micro-steps into something more meaningful.
When we use a high-level language,
on the other hand,
we incur a similar load translating functions of functions of functions
into actual operations on actual data.


More experienced programmers are more capable at both ends of the curve,
but that's not the only thing that changes.
If a novice's comprehension curve looks like the one on the left
of \figref{templating-comprehension},
then an expert's looks like the one on the right.
Experts don't just understand more at all levels of abstraction;
their \emph{preferred} level has also shifted
so that $\sqrt{x^2 + y^2}$
is actually more readable than the medieval expression
"the side of the square whose area is the sum of the areas of the two squares
whose sides are given by the first part and the second part".

\figpdf{templating-comprehension}{./templating/comprehension.pdf}{Novice and expert comprehension curves.}{0.6}


This curve means that for any given task,
the software that is quickest for a novice to comprehend
will almost certainly be different from the software that
an expert can understand most quickly.
In an ideal world our tools would automatically re-represent programs at different levels
just as we could change the colors used for syntax highlighting.
But today's tools don't do that,
and any IDE smart enough to translate between comprehension levels automatically
would also be smart enough to write the code without our help.

\section{Exercises}\label{templating-exercises}

\subsection*{Tracing execution}


Add a directive \texttt{<span z-trace="variable"/>}
that prints the current value of a variable for debugging.

\subsection*{Unit tests}


Write unit tests for template expansion using pytest.

\subsection*{Literal text}


Add a directive \texttt{<div z-literal="true">...</div>} that copies the enclosed text as-is
without interpreting or expanding any contained directives.
(A directive like this would be needed when writing documentation for the template expander.)

\subsection*{Including other files}

\begin{enumerate}

\item 

Add a directive \texttt{<div z-include="filename.html"/>} that includes another file
    in the file being processed.



\item 

Should included files be processed and the result copied into the including file,
    or should the text be copied in and then processed?
    What difference does it make to the way variables are evaluated?



\end{enumerate}

\subsection*{HTML snippets}


Add a directive \texttt{<div z-snippet="variable">...</div>} that saves some text in a variable
so that it can be displayed later.
For example:

\begin{lstlisting}[frame=single,frameround=tttt]
<html>
  <body>
    <div z-snippet="prefix"><strong>Important:</strong></div>
    <p>Expect three items</p>
    <ul>
      <li z-loop="item:names">
        <span z-var="prefix"><span z-var="item"/>
      </li>
    </ul>
  </body>
</html>
\end{lstlisting}


\noindent would printed the word "Important:" in bold before each item in the list.

\subsection*{YAML headers}


Modify the template expander to handle variables defined in a YAML header in the page being processed.
For example, if the page is:

\begin{lstlisting}[frame=single,frameround=tttt]
---
name: "Dorothy Johnson Vaughan"
---
<html>
  <body>
    <p><span z-var="name"/></p>
  </body>
</html>
\end{lstlisting}


\noindent will create a paragraph containing the given name.

\subsection*{Expanding all files}


Write a program \texttt{expand\_all.py} that takes two directory names as command-line arguments
and builds a website in the second directory by expanding all of the HTML files found in the first
or in sub-directories of the first.

\subsection*{Counting loops}


Add a directive \texttt{<div z-index="indexName" z-limit="limitName">...</div>}
that loops from zero to the value in the variable \texttt{limitName},
putting the current iteration index in \texttt{indexName}.

\chapter{Chapter 16: A Package Manager}\label{packman}

\begin{itemize}

\item FIXME

\end{itemize}


FIXME

\chapter{Chapter 17: Page Layout}\label{layout}

\begin{itemize}

\item FIXME

\end{itemize}


\noindent 
    Terms defined:
    \glossref{attribute}, \glossref{cache}, \glossref{confirmation bias}, \glossref{design by contract}, \glossref{easy mode}, \glossref{layout engine}, \glossref{Liskov Substitution Principle}, \glossref{mixin}, \glossref{signature}, \glossref{z-buffering}



You might be reading this as an HTML page,
an e-book (which is basically the same thing),
or on the printed page.
In all three cases
a \glossref{layout engine}\index{layout engine} took some text and some layout instructions
and decided where to put each character and image.
We will build a small layout engine in this chapter
based on \hreffoot{Matt Brubeck's}{https://limpet.net/mbrubeck/}\index{Brubeck, Matt} \hreffoot{tutorial}{https://limpet.net/mbrubeck/2014/08/08/toy-layout-engine-1.html}
to explore how browsers decide what to put where.


Our inputs will be a very small subset of HTML and an equally small subset of CSS.
We will create our own classes to represent these
instead of using those provided by various Node libraries;
to translate the combination of HTML and CSS into text on the screen,
we will label each node in the DOM tree with the appropriate styles,
walk that tree to figure out where each visible element belongs,
and then draw the result as text on the screen.


\chapter{Chapter 18: A Style Checker}\label{linter}

\begin{itemize}

\item FIXME

\end{itemize}

\section{Machinery}\label{linter-machinery}

\begin{itemize}

\item 

\texttt{dump\_ast.py} using \texttt{double.py}



\item 

\texttt{walk\_ast.py}



\end{itemize}

\section*{Duplicate Keys in Dictionaries}

\begin{itemize}

\item \texttt{has\_duplicate\_keys.py} produces \texttt{has\_duplicate\_keys\_ast.out}

\item \texttt{find\_duplicate\_keys.py} with \texttt{has\_duplicate\_keys.py}

\end{itemize}

\section*{Finding Unused Variables}

\begin{itemize}

\item \texttt{find\_unused\_variables.py} and \texttt{has\_unused\_variables.py}

\end{itemize}

\chapter{Exercises}\label{linter-exercises}


FIXME


find unused parameters as well as unused variables

\chapter{Chapter 19: A Documentation Generator}\label{docgen}

\begin{itemize}

\item FIXME

\end{itemize}


FIXME

\chapter{Chapter 20: A Code Generator}\label{codegen}

\begin{itemize}

\item FIXME

\end{itemize}


FIXME

\chapter{Chapter 21: A Virtual Machine and Assembler}\label{vm}

\begin{itemize}

\item FIXME

\end{itemize}


\noindent 
    Terms defined:
    \glossref{Application Binary Interface}, \glossref{assembler}, \glossref{assembly code}, \glossref{bitwise operation}, \glossref{disassembler}, \glossref{instruction pointer}, \glossref{instruction set}, \glossref{label (address in memory)}, \glossref{op code}, \glossref{register}, \glossref{virtual machine}, \glossref{word (of memory)}



You might feel there's still magic in our interpreter,
so let's build something lower-level.
If you want to dive deeper,
have a look at \hreffoot{Bob Nystrom's}{http://journal.stuffwithstuff.com/}\index{Nystrom, Bob}
\emph{\hreffoot{Crafting Interpreters}{https://craftinginterpreters.com/}} \cite{Nystrom2021}.
You may also enjoy the game \hreffoot{Human Resource Machine}{https://tomorrowcorporation.com/humanresourcemachine}\index{Human Resource Machine},
which asks you to solve puzzles of increasing difficulty
using a processor almost as simple as ours.

\section{Architecture}\label{vm-arch}


Every processor has its own \glossref{instruction set}\index{instruction set},
and a compiler translates high-level languages into those instructions.
Compilers often use an intermediate representation called
\glossref{assembly code}\index{assembly code}
that gives instructions human-readable names instead of numbers.
Our \glossref{virtual machine}\index{virtual machine} simulates
a computer with three parts
which are shown in \figref{vm-architecture}
for a program made up of 110 instructions:

\begin{enumerate}

\item 

An \glossref{instruction pointer}\index{instruction pointer} (IP)
    that holds the memory address of the next instruction to execute.
    It is automatically initialized to point at address 0,
    which is where every program must start.
    This rule is part of the \glossref{Application Binary Interface}\index{Application Binary Interface} (ABI)
    for our virtual machine.



\item 

Four \glossref{registers}\index{register (in computer)} named R0 to R3 that instructions can access directly.
    There are no memory-to-memory operations in our VM:
    everything  happens in or through registers.



\item 

256 \glossref{words} of memory, each of which can store a single value.
    Both the program and its data live in this single block of memory;
    we chose the size 256 so that each address will fit in a single byte.



\end{enumerate}

\figpdf{vm-architecture}{./vm/architecture.pdf}{Architecture of the virtual machine.}{0.6}


The instructions for our VM are 3 bytes long.
The \glossref{op code}\index{op code}\index{virtual machine!op code} fits into one byte,
and each instruction may optionally include one or two single-byte operands.
Each operand is a register identifier,
a constant,
or an address
(which is just a constant that identifies a location in memory);
since constants have to fit in one byte,
the largest number we can represent directly is 256.
\tblref{vm-op-codes} uses the letters \texttt{r}, \texttt{c}, and \texttt{a}
to indicate instruction format,
where \texttt{r} indicates a register identifier,
\texttt{c} indicates a constant,
and \texttt{a} indicates an address.

\begin{table}
\begin{tabular}{llllll}
\textbf{\underline{Instruction}} & \textbf{\underline{Code}} & \textbf{\underline{Format}} & \textbf{\underline{Action}} & \textbf{\underline{Example}} & \textbf{\underline{Equivalent}} \\
\texttt{hlt} & 1 & \texttt{--} & Halt program & \texttt{hlt} & \texttt{process.exit(0)} \\
\texttt{ldc} & 2 & \texttt{rc} & Load immediate & \texttt{ldc R0 123} & \texttt{R0 := 123} \\
\texttt{ldr} & 3 & \texttt{rr} & Load register & \texttt{ldr R0 R1} & \texttt{R0 := RAM[R1]} \\
\texttt{cpy} & 4 & \texttt{rr} & Copy register & \texttt{cpy R0 R1} & \texttt{R0 := R1} \\
\texttt{str} & 5 & \texttt{rr} & Store register & \texttt{str R0 R1} & \texttt{RAM[R1] := R0} \\
\texttt{add} & 6 & \texttt{rr} & Add & \texttt{add R0 R1} & \texttt{R0 := R0 + R1} \\
\texttt{sub} & 7 & \texttt{rr} & Subtract & \texttt{sub R0 R1} & \texttt{R0 := R0 - R1} \\
\texttt{beq} & 8 & \texttt{ra} & Branch if equal & \texttt{beq R0 123} & \texttt{if (R0 === 0) PC := 123} \\
\texttt{bne} & 9 & \texttt{ra} & Branch if not equal & \texttt{bne R0 123} & \texttt{if (R0 !== 0) PC := 123} \\
\texttt{prr} & 10 & \texttt{r-} & Print register & \texttt{prr R0} & \texttt{console.log(R0)} \\
\texttt{prm} & 11 & \texttt{r-} & Print memory & \texttt{prm R0} & \texttt{console.log(RAM[R0])} \\
\end{tabular}
\caption{Virtual machine op codes.}
\label{vm-op-codes}
\end{table}



We put our VM's architectural details in a file
that can be shared by other components:


\begin{lstlisting}[frame=single,frameround=tttt]
OPS = {
  "hlt": { "code":  1, "fmt": "--" }, # Halt program
  "ldc": { "code":  2, "fmt": "rv" }, # Load immediate
  "ldr": { "code":  3, "fmt": "rr" }, # Load register
  "cpy": { "code":  4, "fmt": "rr" }, # Copy register
  "str": { "code":  5, "fmt": "rr" }, # Store register
  "add": { "code":  6, "fmt": "rr" }, # Add
  "sub": { "code":  7, "fmt": "rr" }, # Subtract
  "beq": { "code":  8, "fmt": "rv" }, # Branch if equal
  "bne": { "code":  9, "fmt": "rv" }, # Branch if not equal
  "prr": { "code": 10, "fmt": "r-" }, # Print register
  "prm": { "code": 11, "fmt": "r-" }  # Print memory
}

OP_MASK = 0xFF # select a single byte
OP_SHIFT = 8   # shift up by one byte
OP_WIDTH = 6   # op width in characters when printing

NUM_REG = 4    # number of registers
RAM_LEN = 256  # number of words in RAM
\end{lstlisting}



\noindent There isn't a name for this design pattern,
but putting all the constants that define a system in one file
instead of scattering them across multiple files
makes them easier to find as well as ensuring consistency.

\section{Execution}\label{vm-execute}


As in previous chapters,
we will split a class that would normally be written in one piece into several parts for exposition.
We start by defining a class with an instruction pointer, some registers, and some memory
along with a prompt for output:


\begin{lstlisting}[frame=single,frameround=tttt]
from architecture import OP_MASK, OP_SHIFT, NUM_REG, RAM_LEN

COLUMNS = 4
DIGITS = 8

class VirtualMachineBase:

    def __init__(self):
        self.initialize([])
        self.prompt = '>>'


    # [driver]
    @classmethod
    def main(cls):
        import sys
        assert len(sys.argv) == 3, f"Usage: {sys.argv[0]} input|- output|-"
        reader = open(sys.argv[1], "r") if (sys.argv[1] != "-") else sys.stdin
        writer = open(sys.argv[2], "w") if (sys.argv[2] != "-") else sys.stdout

        lines = [ln.strip() for ln in reader.readlines()]
        program = [int(ln, 16) for ln in lines if ln]
        vm = cls()
        vm.initialize(program)
        vm.run()
        vm.show(writer)

# [main]
if __name__ == "__main__":
    VirtualMachineBase.main()
# [/main]
\end{lstlisting}



A program is just an array of numbers representing instructions.
To load one,
we copy those numbers into memory and reset the instruction pointer and registers:


\begin{lstlisting}[frame=single,frameround=tttt]
    def initialize(self, program):
        assert len(program) <= RAM_LEN, \
            "Program is too long for memory"
        self.ram = [
            program[i] if (i < len(program)) else 0
            for i in range(RAM_LEN)
        ]
        self.ip = 0
        self.reg = [0] * NUM_REG
\end{lstlisting}



In order to handle the next instruction,
the VM gets the value in memory that the instruction pointer currently refers to
and moves the instruction pointer on by one address.
It then uses \glossref{bitwise operations}\index{bitwise operation}
to extract the op code and operands from the instruction
(\figref{vm-unpacking}):


\begin{lstlisting}[frame=single,frameround=tttt]
    def fetch(self):
        assert 0 <= self.ip < len(self.ram), \
            f"Program counter {self.ip:06x} out of range 0..{len(self.ram):06x}"
        instruction = self.ram[self.ip]
        self.ip += 1
        op = instruction & OP_MASK
        instruction >>= OP_SHIFT
        arg0 = instruction & OP_MASK
        instruction >>= OP_SHIFT
        arg1 = instruction & OP_MASK
        return [op, arg0, arg1]
\end{lstlisting}


\figpdf{vm-unpacking}{./vm/unpacking.pdf}{Using bitwise operations to unpack instructions.}{0.6}



The next step is to extend our base class with one that has a \texttt{run} method.
As its name suggests,
this runs the program by fetching instructions and executing them until told to stop:


\begin{lstlisting}[frame=single,frameround=tttt]
from architecture import OPS
from vm_base import VirtualMachineBase

class VirtualMachine(VirtualMachineBase):
    def run(self):
        running = True
        while (running):
            op, arg0, arg1 = self.fetch()
            if op == OPS["hlt"]["code"]:
                running = False

            elif op == OPS["ldc"]["code"]:
                self.assert_is_register(arg0)
                self.reg[arg0] = arg1


            else:
                assert False, f"Unknown op {op:06x}"


    def assert_is_register(self, reg):
        assert 0 <= reg < len(self.reg), \
            f"Invalid register {reg:06x}"

    def assert_is_address(self, addr):
        assert 0 <= addr < len(self.ram), \
            f"Invalid register {addr:06x}"

# [main]
if __name__ == "__main__":
    VirtualMachine.main()
# [/main]
\end{lstlisting}



Some instructions are very similar to others,
so we will only look at three here.
The first stores the value of one register in the address held by another register:


\begin{lstlisting}[frame=single,frameround=tttt]
            elif op == OPS["str"]["code"]:
                self.assert_is_register(arg0)
                self.assert_is_register(arg1)
                self.assert_is_address(self.reg[arg1])
                self.ram[self.reg[arg1]] = self.reg[arg0]
\end{lstlisting}



\noindent The first three lines check that the operation is legal;
the fourth one uses the value in one register as an address,
which is why it has nested array indexing.


Adding the value in one register to the value in another register is simpler:


\begin{lstlisting}[frame=single,frameround=tttt]
            elif op == OPS["add"]["code"]:
                self.assert_is_register(arg0)
                self.assert_is_register(arg1)
                self.reg[arg0] += self.reg[arg1]
\end{lstlisting}



\noindent as is jumping to a fixed address if the value in a register is zero:


\begin{lstlisting}[frame=single,frameround=tttt]
            elif op == OPS["beq"]["code"]:
                self.assert_is_register(arg0)
                self.assert_is_address(arg1)
                if self.reg[arg0] == 0:
                    self.ip = arg1
\end{lstlisting}


\section{Assembly Code}\label{vm-assembly}


We could figure out numerical op codes by hand,
and in fact that's what \hreffoot{the first programmers}{http://eniacprogrammers.org/} did.
However,
it is much easier to use an \glossref{assembler}\index{assembler},
which is just a small compiler for a language
that very closely represents actual machine instructions.


Each command in our assembly languages matches an instruction in the VM.
Here's an assembly language program to print the value stored in R1 and then halt:


\begin{lstlisting}[frame=single,frameround=tttt]
# Print initial contents of R1.
prr R1
hlt
\end{lstlisting}



\noindent Its numeric representation is:


\begin{lstlisting}[frame=single,frameround=tttt]
00010a
000001
\end{lstlisting}



One thing the assembly language has that the instruction set doesn't
is \glossref{labels on addresses}\index{label (on address)}.
The label \texttt{loop} doesn't take up any space;
instead,
it tells the assembler to give the address of the next instruction a name
so that we can refer to that address as \texttt{@loop} in jump instructions.
For example,
this program prints the numbers from 0 to 2
(\figref{vm-count_up}):


\begin{lstlisting}[frame=single,frameround=tttt]
# Count up to 3.
# - R0: loop index.
# - R1: loop limit.
ldc R0 0
ldc R1 3
loop:
prr R0
ldc R2 1
add R0 R2
cpy R2 R1
sub R2 R0
bne R2 @loop
hlt
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
000002
030102
00000a
010202
020006
010204
000207
020209
000001
\end{lstlisting}


\figpdf{vm-count_up}{./vm/count_up.pdf}{Flowchart of assembly language program to count up from 0 to 2.}{0.6}


Let's trace this program's execution
(\figref{vm-trace-counter}):

\begin{enumerate}

\item R0 holds the current loop index.

\item R1 holds the loop's upper bound (in this case 3).

\item The loop prints the value of R0 (one instruction).

\item The program adds 1 to R0.
    This takes two instructions because we can only add register-to-register.

\item It checks to see if we should loop again,
    which takes three instructions.

\item If the program \emph{doesn't} jump back, it halts.

\end{enumerate}

\figpdf{vm-trace-counter}{./vm/trace_counter.pdf}{Tracing registers and memory values for a simple counting program.}{0.6}


The implementation of the assembler mirrors the simplicity of assembly language.
The main method gets interesting lines,
finds the addresses of labels,
and turns each remaining line into an instruction:


\begin{lstlisting}[frame=single,frameround=tttt]
    def assemble(self, lines):
        lines = self.clean_lines(lines)
        labels = self.find_labels(lines)
        instructions = [ln for ln in lines if not self.is_label(ln)]
        compiled = [self.compile(instr, labels) for instr in instructions]
        program = self.instructions_to_text(compiled)
        return program

    def clean_lines(self, lines):
        lines = [ln.strip() for ln in lines]
        lines = [ln for ln in lines if len(ln) > 0]
        lines = [ln for ln in lines if not self.is_comment(ln)]
        return lines

    def is_comment(self, line):
        return line.startswith("#")
\end{lstlisting}



To find labels,
we go through the lines one by one
and either save the label \emph{or} increment the current address
(because labels don't take up space):


\begin{lstlisting}[frame=single,frameround=tttt]
    def find_labels(self, lines):
        result = {}
        loc = 0
        for ln in lines:
            if self.is_label(ln):
                label = ln[:-1]
                assert label not in result, \
                    f"Duplicate label {label}"
                result[label] = loc
            else:
                loc += 1
        return result

    def is_label(self, line):
        return line.endswith(":")
\end{lstlisting}



To compile a single instruction we break the line into tokens,
look up the format for the operands,
and pack them into a single value:


\begin{lstlisting}[frame=single,frameround=tttt]
    def compile(self, instruction, labels):
        tokens = instruction.split()
        op, args = tokens[0], tokens[1:]
        assert op in OPS, \
            f"Unknown operation {op}"

        result = 0
        if OPS[op]["fmt"] == "--":
            result = self.combine(OPS[op]["code"])

        elif OPS[op]["fmt"] == "r-":
            result = self.combine(self.register(args[0]), OPS[op]["code"])

        elif OPS[op]["fmt"] == "rr":
            result = self.combine(self.register(args[1]), self.register(args[0]), OPS[op]["code"])

        elif OPS[op]["fmt"] == "rv":
            result = self.combine(self.value(args[1], labels), self.register(args[0]), OPS[op]["code"])

        else:
            assert False, \
                f"Unknown instruction format {OPS[op]['fmt']}"

        return result
\end{lstlisting}



Combining op codes and operands into a single value
is the reverse of the unpacking done by the virtual machine:


\begin{lstlisting}[frame=single,frameround=tttt]
    def combine(self, *args):
        assert len(args) > 0, \
            "Cannot combine no arguments"
        result = 0
        for a in args:
            result <<= OP_SHIFT
            result |= a
        return result
\end{lstlisting}



Finally, we need few utility functions:


\begin{lstlisting}[frame=single,frameround=tttt]
    def instructions_to_text(self, program):
        return [f"{op:06x}" for op in program]

    def register(self, token):
        assert token[0] == "R", \
            f"Register '{token}' does not start with 'R'"
        r = int(token[1:])
        assert 0 <= r < NUM_REG, \
            f"Illegal register {token}"
        return r

    def value(self, token, labels):
        if token[0] != "@":
            return int(token)

        lbl = token[1:]
        assert lbl in labels, \
            f"Unknown label '{token}'"
        return labels[lbl]
\end{lstlisting}



Let's try assembling a program and display its output,
the registers,
and the interesting contents of memory.
As a test,
this program counts up to three:


\begin{lstlisting}[frame=single,frameround=tttt]
# Count up to 3.
# - R0: loop index.
# - R1: loop limit.
ldc R0 0
ldc R1 3
loop:
prr R0
ldc R2 1
add R0 R2
cpy R2 R1
sub R2 R0
bne R2 @loop
hlt
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
>> 0
>> 1
>> 2
R000000 = 000003
R000001 = 000003
R000002 = 000000
R000003 = 000000
000000:   000002  030102  00000a  010202
000004:   020006  010204  000207  020209
000008:   000001  000000  000000  000000
\end{lstlisting}


\section*{Arrays}


It is tedious to write interesting programs when each value needs a unique name.
We can do a lot more once we have collections like arrays\index{array!implementation of},
so let's add those to our assembler.
We don't have to make any changes to the virtual machine,
which doesn't care if we think of a bunch of numbers as individuals or elements of an array,
but we do need a way to create arrays and refer to them.


We will allocate storage for arrays at the end of the program
by using \texttt{.data} on a line of its own to mark the start of the data section
and then \texttt{label: number} to give a region a name and allocate some storage space
(\figref{vm-storage-allocation}).

\figpdf{vm-storage-allocation}{./vm/storage_allocation.pdf}{Allocating storage for arrays in the virtual machine.}{0.6}


This enhancement only requires a few changes to the assembler.
First,
we need to split the lines into instructions and data allocations:


\begin{lstlisting}[frame=single,frameround=tttt]
    def assemble(self, lines):
        lines = self.clean_lines(lines)
        to_compile, to_allocate = self.split_allocations(lines)
        labels = self.find_labels(lines)
        instructions = [ln for ln in to_compile if not self.is_label(ln)]
        base_of_data = len(instructions)
        self.add_allocations(base_of_data, labels, to_allocate)
        compiled = [self.compile(instr, labels) for instr in instructions]
        program = self.instructions_to_text(compiled)
        return program
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
    def split_allocations(self, lines):
        try:
            split = lines.index(DIVIDER)
            return lines[0:split], lines[split+1:]
        except ValueError:
            return lines, []
\end{lstlisting}



Second,
we need to figure out where each allocation lies and create a label accordingly:


\begin{lstlisting}[frame=single,frameround=tttt]
    def add_allocations(self, base_of_data, labels, to_allocate):
        for alloc in to_allocate:
            fields = [a.strip() for a in alloc.split(":")]
            assert len(fields) == 2, \
                f"Invalid allocation directive '{alloc}'"
            lbl, num_words_text = fields
            assert lbl not in labels, \
                f"Duplicate label '{label}' in data allocation"
            num_words = int(num_words_text)
            assert (base_of_data + num_words) < RAM_LEN, \
                f"Allocation '{label}' requires too much memory"
            labels[lbl] = base_of_data
            base_of_data += num_words
\end{lstlisting}



And that's it:
no other changes are needed to either compilation or execution.
To test it,
let's fill an array with the numbers from 0 to 3:


\begin{lstlisting}[frame=single,frameround=tttt]
# Count up to 3.
# - R0: loop index.
# - R1: loop limit.
# - R2: array index.
# - R3: temporary.
ldc R0 0
ldc R1 3
ldc R2 @array
loop:
str R0 R2
ldc R3 1
add R0 R3
add R2 R3
cpy R3 R1
sub R3 R0
bne R3 @loop
hlt
.data
array: 10
\end{lstlisting}



\begin{lstlisting}[frame=single,frameround=tttt]
R000000 = 000003
R000001 = 000003
R000002 = 00000e
R000003 = 000000
000000:   000002  030102  0b0202  020005
000004:   010302  030006  030206  010304
000008:   000307  030309  000001  000000
00000c:   000001  000002  000000  000000
\end{lstlisting}



\section{Exercises}\label{vm-exercises}

\subsection*{Swapping values}


Write an assembly language program that swaps the values in R1 and R2
without affecting the values in other registers.

\subsection*{Reversing an array}


Write an assembly language program that starts with:

\begin{itemize}

\item the base address of an array in one word

\item the length of the array N in the next word

\item N values immediately thereafter

\end{itemize}


\noindent and reverses the array in place.

\subsection*{Increment and decrement}

\begin{enumerate}

\item 

Add instructions \texttt{inc} and \texttt{dec} that add one to the value of a register
    and subtract one from the value of a register respectively.



\item 

Rewrite the examples to use these instructions.
    How much shorter do they make the programs?
    How much easier to read?



\end{enumerate}

\subsection*{Using long addresses}

\begin{enumerate}

\item 

Modify the virtual machine so that the \texttt{ldr} and \texttt{str} instructions
    contain 16-bit addresses rather than 8-bit addresses
    and increase the virtual machine's memory to 64K words to match.



\item 

How does this complicate instruction interpretation?



\end{enumerate}

\subsection*{Operating on strings}


The C programming language stored character strings as non-zero bytes terminated by a byte containing zero.

\begin{enumerate}

\item 

Write a program that starts with the base address of a string in R1
    and finishes with the length of the string (not including the terminator) in the same register.



\item 

Write a program that starts with the base address of a string in R1
    and the base address of some other block of memory in R2
    and copies the string to that new location (including the terminator).



\item 

What happens in each case if the terminator is missing?



\end{enumerate}

\subsection*{Call and return}

\begin{enumerate}

\item 

Add another register to the virtual machine called SP (for "stack pointer")
    that is automatically initialized to the \emph{last} address in memory.



\item 

Add an instruction \texttt{psh} (short for "push") that copies a value from a register
    to the address stored in SP and then subtracts one from SP.



\item 

Add an instruction \texttt{pop} (short for "pop") that adds one to SP
    and then copies a value from that address into a register.



\item 

Using these instructions,
    write a subroutine that evaluates \texttt{2x+1} for every value in an array.



\end{enumerate}

\subsection*{Disassembling instructions}


A \glossref{"disassembler} turns machine instructions into assembly code.
Write a disassembler for the instruction set used by our virtual machine.
(Since the labels for addresses are not stored in machine instructions,
disassemblers typically generate labels like \texttt{@L001} and \texttt{@L002}.)

\subsection*{Linking multiple files}

\begin{enumerate}

\item 

Modify the assembler to handle \texttt{.include filename} directives.



\item 

What does your modified assembler do about duplicate label names?
    How does it prevent infinite includes
    (i.e., \texttt{A.as} includes \texttt{B.as} which includes \texttt{A.as} again)?



\end{enumerate}

\subsection*{Providing system calls}


Modify the virtual machine so that developers can add "system calls" to it.

\begin{enumerate}

\item 

On startup,
    the virtual machine loads an array of functions defined in a file called \texttt{syscalls.py}.



\item 

The \texttt{sys} instruction takes a one-byte constant argument.
    It looks up the corresponding function and calls it with the values of R0-R3 as parameters
    and places the result in R0.



\end{enumerate}

\subsection*{Unit testing}

\begin{enumerate}

\item 

Write unit tests for the assembler.



\item 

Once they are working,
    write unit tests for the virtual machine.



\end{enumerate}

\chapter{Chapter 22: Debugger}\label{debugger}

\begin{itemize}

\item FIXME

\end{itemize}


FIXME

\chapter{Chapter 23: Conclusion}\label{conclusion}


We have come a long way since we first passed a function as an argument in \chapref{tester}.
Saving files in version control,
making sure code meets style rules,
debugging it and bundling it (hopefully in that order)—programmers do these things every day,
and we hope that understanding how they work will help you do them better.


We also hope that your journey won't stop here.
If you would like to add a chapter to this book
or translate it into another programming language,
human language,
or both,
your help would be very welcome:
please see \chapref{introduction} and \appref{contributing} for more information.


\appendix
\chapter{Appendix A: Bibliography}\label{bibliography}

\printbibliography[heading=none]

\chapter{Appendix B: Syllabus}\label{syllabus}

\begin{itemize}

\item \hreffoot{A Testing Framework}{../tester}
\begin{itemize}

\item Functions are objects that can be saved in data structures or passed as arguments to other functions.

\item Python stores variables in a structure like a dictionary.

\item A unit test is a function that takes a fixture, performs an operation, and passes, fails, or produces an error.

\item Use reflection to discover functions and other values in programs at runtime.

\item Replace actual functions with mock objects temporarily to simplify testing.

\end{itemize}



\item \hreffoot{Versioned File Backups}{../backup}
\begin{itemize}

\item FIXME

\end{itemize}



\item \hreffoot{An Interpreter}{../interpreter}
\begin{itemize}

\item Compilers and interpreters are programs like any others.

\end{itemize}



\item \hreffoot{A Dataframe}{../dataframe}
\begin{itemize}

\item Create abstract base classes to specify interfaces.

\item Store two-dimensional data as rows or as columns.

\item Use reflection to match data to function parameters.

\item Measure performance to evaluate engineering tradeoffs.

\end{itemize}



\item \hreffoot{A Pipeline Runner}{../pipeline}
\begin{itemize}

\item FIXME

\end{itemize}



\item \hreffoot{A Build Manager}{../builder}
\begin{itemize}

\item FIXME

\end{itemize}



\item \hreffoot{Matching Regular Expressions}{../matching}
\begin{itemize}

\item Use regular expressions to match patterns in text and extra data.

\item Have objects control other objects using the Chain of Responsibility design pattern.

\item Use inheritance to make matchers composable.

\end{itemize}



\item \hreffoot{Parsing Regular Expressions}{../parser}
\begin{itemize}

\item Use existing file formats rather than creating new ones.

\item Tokenize input text and then parse the tokens.

\item Parsing in two or more passes is often simpler than parsing in a single pass.

\item Every formal language corresponds to an abstract machine and vice versa.

\end{itemize}



\item \hreffoot{A Web Server}{../server}
\begin{itemize}

\item Every computer on a network has a unique IP address.

\item The Domain Name System (DNS) translates human-readable names into IP addresses.

\item The programs on each computer send and receive messages through numbered sockets.

\item The program that receives a message must interpret the bytes in the message.

\item The HyperText Transfer Protocol (HTTP) specifies one way to interact via messages over sockets.

\item A minimal HTTP request has a method, a URL, and a protocol version.

\item A complete HTTP request may also have headers and a body.

\item An HTTP response has a status code, a status phrase, and optionally some headers and a body.

\item HTTP is a stateless protocol: the application is responsible for remembering things between requests.

\end{itemize}



\item \hreffoot{A File Cache}{../filecache}
\begin{itemize}

\item FIXME

\end{itemize}



\item \hreffoot{A Database}{../database}
\begin{itemize}

\item FIXME

\end{itemize}



\item \hreffoot{Object Persistence}{../persistence}
\begin{itemize}

\item FIXME

\end{itemize}



\item \hreffoot{Binary Storage}{../binary}
\begin{itemize}

\item FIXME

\end{itemize}



\item \hreffoot{HTML Templating}{../templating}
\begin{itemize}

\item FIXME

\end{itemize}



\item \hreffoot{A Package Manager}{../packman}
\begin{itemize}

\item FIXME

\end{itemize}



\item \hreffoot{Page Layout}{../layout}
\begin{itemize}

\item FIXME

\end{itemize}



\item \hreffoot{A Style Checker}{../linter}
\begin{itemize}

\item FIXME

\end{itemize}



\item \hreffoot{A Documentation Generator}{../docgen}
\begin{itemize}

\item FIXME

\end{itemize}



\item \hreffoot{A Code Generator}{../codegen}
\begin{itemize}

\item FIXME

\end{itemize}



\item \hreffoot{A Virtual Machine and Assembler}{../vm}
\begin{itemize}

\item FIXME

\end{itemize}



\item \hreffoot{Debugger}{../debugger}
\begin{itemize}

\item FIXME

\end{itemize}



\end{itemize}

\chapter{Appendix C: License}\label{license}


All of the written material on this site is made available under the Creative
Commons - Attribution - NonCommercial 4.0 International license (CC-BY-NC-4.0),
while the software is made available under the Hippocratic License.

\section*{Writing}


\emph{This is a human-readable summary of (and not a substitute for) the license.
For the full legal text of this license, please see
\hreffoot{https://creativecommons.org/licenses/by-nc/4.0/legalcode}{https://creativecommons.org/licenses/by-nc/4.0/legalcode}.}


All of this site is made available under the terms of the Creative Commons
Attribution - NonCommercial 4.0 license. You are free to:

\begin{itemize}

\item 

\textbf{Share} — copy and redistribute the material in any medium or format



\item 

\textbf{Adapt} — remix, transform, and build upon the material



\item 

The licensor cannot revoke these freedoms as long as you follow the license
    terms.



\end{itemize}


Under the following terms:

\begin{itemize}

\item 

\textbf{Attribution} — You must give appropriate credit, provide a link to the
    license, and indicate if changes were made. You may do so in any reasonable
    manner, but not in any way that suggests the licensor endorses you or your
    use.



\item 

\textbf{NonCommercial} — You may not use the material for commercial purposes.



\item 

No additional restrictions — You may not apply legal terms or technological
    measures that legally restrict others from doing anything the license
    permits.



\end{itemize}


Notices:


You do not have to comply with the license for elements of the material in the
public domain or where your use is permitted by an applicable exception or
limitation.


No warranties are given. The license may not give you all of the permissions
necessary for your intended use. For example, other rights such as publicity,
privacy, or moral rights may limit how you use the material.

\section*{Software}


Licensor hereby grants permission by this license ("License"), free of charge,
to any person or entity (the "Licensee") obtaining a copy of this software and
associated documentation files (the "Software"), to deal in the Software without
restriction, including without limitation the rights to use, copy, modify,
merge, publish, distribute, sublicense, and/or sell copies of the Software, and
to permit persons to whom the Software is furnished to do so, subject to the
following conditions:

\begin{itemize}

\item 

The above copyright notice and this License or a subsequent version published
    on the \hreffoot{Hippocratic License Website}{https://firstdonoharm.dev/} shall be
    included in all copies or substantial portions of the Software. Licensee has
    the option of following the terms and conditions either of the above
    numbered version of this License or of any subsequent version published on
    the Hippocratic License Website.



\item 

Compliance with Human Rights Laws and Human Rights Principles:

\begin{enumerate}

\item 

Human Rights Laws. The Software shall not be used by any person or
    entity for any systems, activities, or other uses that violate any
    applicable laws, regulations, or rules that protect human, civil, labor,
    privacy, political, environmental, security, economic, due process, or
    similar rights (the "Human Rights Laws"). Where the Human Rights Laws of
    more than one jurisdiction are applicable to the use of the Software,
    the Human Rights Laws that are most protective of the individuals or
    groups harmed shall apply.



\item 

Human Rights Principles. Licensee is advised to consult the articles of
    the \hreffoot{United Nations Universal Declaration of Human
    Rights}{https://www.un.org/en/universal-declaration-human-rights/} and
    the \hreffoot{United Nations Global
    Compact}{https://www.unglobalcompact.org/what-is-gc/mission/principles}
    that define recognized principles of international human rights (the
    "Human Rights Principles"). It is Licensor's express intent that all use
    of the Software be consistent with Human Rights Principles. If Licensor
    receives notification or otherwise learns of an alleged violation of any
    Human Rights Principles relating to Licensee's use of the Software,
    Licensor may in its discretion and without obligation (i) (a) notify
    Licensee of such allegation and (b) allow Licensee 90 days from
    notification under (i)(a) to investigate and respond to Licensor
    regarding the allegation and (ii) (a) after the earlier of 90 days from
    notification under (i)(a), or Licensee's response under (i)(b), notify
    Licensee of License termination and (b) allow Licensee an additional 90
    days from notification under (ii)(a) to cease use of the Software.



\item 

Indemnity. Licensee shall hold harmless and indemnify Licensor against
    all losses, damages, liabilities, deficiencies, claims, actions,
    judgments, settlements, interest, awards, penalties, fines, costs, or
    expenses of whatever kind, including Licensor's reasonable attorneys'
    fees, arising out of or relating to Licensee's non-compliance with this
    License or use of the Software in violation of Human Rights Laws or
    Human Rights Principles.



\end{enumerate}



\item 

Enforceability: If any portion or provision of this License is determined to
     be invalid, illegal, or unenforceable by a court of competent jurisdiction,
     then such invalidity, illegality, or unenforceability shall not affect any
     other term or provision of this License or invalidate or render
     unenforceable such term or provision in any other jurisdiction. Upon a
     determination that any term or provision is invalid, illegal, or
     unenforceable, to the extent permitted by applicable law, the court may
     modify this License to affect the original intent of the parties as closely
     as possible. The section headings are for convenience only and are not
     intended to affect the construction or interpretation of this License. Any
     rule of construction to the effect that ambiguities are to be resolved
     against the drafting party shall not apply in interpreting this
     License. The language in this License shall be interpreted as to its fair
     meaning and not strictly for or against any party.



\end{itemize}


THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.


\emph{The Hippocratic License is an \hreffoot{Ethical Source license}{https://ethicalsource.dev}.}

\chapter{Appendix D: Code of Conduct}\label{conduct}


In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, gender identity and expression, level of
experience, education, socioeconomic status, nationality, personal appearance,
race, religion, or sexual identity and orientation.

\section*{Our Standards}


Examples of behavior that contributes to creating a positive environment
include:

\begin{itemize}

\item using welcoming and inclusive language,

\item being respectful of differing viewpoints and experiences,

\item gracefully accepting constructive criticism,

\item focusing on what is best for the community, and

\item showing empathy towards other community members.

\end{itemize}


Examples of unacceptable behavior by participants include:

\begin{itemize}

\item the use of sexualized language or imagery and unwelcome sexual
  attention or advances,

\item trolling, insulting/derogatory comments, and personal or political
  attacks,

\item public or private harassment,

\item publishing others' private information, such as a physical or
  electronic address, without explicit permission, and

\item other conduct which could reasonably be considered inappropriate in
  a professional setting

\end{itemize}

\section*{Our Responsibilities}


Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.


Project maintainers have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, or to ban temporarily or permanently any
contributor for other behaviors that they deem inappropriate, threatening,
offensive, or harmful.

\section*{Scope}


This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project email
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

\section*{Enforcement}


Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by emailing the project team. All complaints will be reviewed
and investigated and will result in a response that is deemed necessary and
appropriate to the circumstances. The project team is obligated to maintain
confidentiality with regard to the reporter of an incident.  Further details of
specific enforcement policies may be posted separately.


Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

\section*{Attribution}


This Code of Conduct is adapted from the \hreffoot{Contributor Covenant}{https://www.contributor-covenant.org/} version 1.4.

\chapter{Appendix E: Contributing}\label{contributing}


Contributions are very welcome;
please contact us by email or by filing an issue on this site.
All contributors must abide by our Code of Conduct.

\section*{Making Decisions}


This project uses \hreffoot{Martha's Rules}{https://journals.sagepub.com/doi/10.1177/088610998600100206} for consensus decision making:

\begin{enumerate}

\item 

Before each meeting, anyone who wishes may sponsor a proposal by filing an
    issue in the GitHub repository tagged "proposal".  People must file proposals
    at least 24 hours before a meeting in order for them to be considered at that
    meeting, and must include:

\begin{itemize}

\item a one-line summary (the subject line of the issue)

\item the full text of the proposal

\item any required background information

\item pros and cons

\item possible alternatives

\end{itemize}



\item 

A quorum is established in a meeting if half or more of voting members are
    present.



\item 

Once a person has sponsored a proposal, they are responsible for it.  The
    group may not discuss or vote on the issue unless the sponsor or their
    delegate is present.  The sponsor is also responsible for presenting the
    item to the group.



\item 

After the sponsor presents the proposal, a "sense" vote is cast for the
    proposal before any discussion:

\begin{itemize}

\item Who likes the proposal?

\item Who can live with the proposal?

\item Who is uncomfortable with the proposal?

\end{itemize}



\item 

If everyone likes or can live with the proposal, it passes immediately.



\item 

If most of the group is uncomfortable with the proposal, it is postponed for
    further rework by the sponsor.



\item 

Otherwise, members who are uncomfortable can briefly state their objections.
    A timer is then set for a brief discussion moderated by the facilitator.
    After 10 minutes or when no one has anything further to add (whichever comes
    first), the facilitator calls for a yes-or-no vote on the question: "Should
    we implement this decision over the stated objections?"  If a majority votes
    "yes" the proposal is implemented.  Otherwise, the proposal is returned to
    the sponsor for further work.



\end{enumerate}

\section*{Formatting}


This material uses \hreffoot{Ivy}{https://www.dmulholl.com/docs/ivy/main/} with some custom extensions.
Run \texttt{make} in the root directory to get a list of available commands.
Some of these rely on scripts in the \texttt{./bin/} directory.

\subsection*{Chapters and Appendices}

\begin{enumerate}

\item 

Each chapter or appendix has a unique slug such as \texttt{topic}.
    Its text lives in \texttt{./src/\emph{topic}/index.md},
    and there is an entry for it in the \texttt{chapters} or \texttt{appendices} list in \texttt{./config.py}
    (which control ordering).



\item 

Each \texttt{index.md} file starts with a YAML header in triple dashes.
    This header must include the key \texttt{title:} with the page's title.



\item 

Each section within a page must use a heading like this:

\begin{lstlisting}[frame=single,frameround=tttt]
## Some Title {: #topic-sometitle}
\end{lstlisting}


This creates an \texttt{h2}-level heading with the HTML ID \texttt{topic-sometitle}.
Use the page's slug instead of \texttt{topic} and a single unhyphenated work
in place of \texttt{sometitle}.



\item 

To create a cross-reference to a chapter or appendix write:

\begin{lstlisting}[frame=single,frameround=tttt]
[% x topic %]
\end{lstlisting}


where \texttt{topic} is the slug of the chapter being referred to.
This shortcode is converted to \texttt{Chapter N} or \texttt{Appendix N} as appropriate.
Please only refer to chapters or appendices, not to sections.



\end{enumerate}

\subsection*{External Links}

\begin{enumerate}

\item 

The table of external links lives in \texttt{./info/links.yml}.
    Please add entries as needed.



\item 

To refer to an external link write:

\begin{lstlisting}[frame=single,frameround=tttt]
[body text][link_key]
\end{lstlisting}



\end{enumerate}


Please do \emph{not} add links directly with \texttt{[text](http://some.url)}:
keeping the links in \texttt{./info/links.yml} ensures consistency
and makes it easier to create a table of external links.

\subsection*{Code Inclusions}

\begin{enumerate}

\item 

To include an entire file as a code sample write:

\begin{lstlisting}[frame=single,frameround=tttt]
[% inc file="some_name.py" %]
\end{lstlisting}


The file must be in or below the directory containing the Markdown file.



\item 

To include only part of a file write:

\begin{lstlisting}[frame=single,frameround=tttt]
[% inc file="some_name.py" keep="some_key" %]
\end{lstlisting}


and put matching tags in the file like this:

\begin{lstlisting}[frame=single,frameround=tttt]
# [some_key]
…lines of code…
# [/some_key]
\end{lstlisting}



\item 

To include several files (such as a program and its output) write:

\begin{lstlisting}[frame=single,frameround=tttt]
[% inc pat="some_stem.*" fill="py out" %]
\end{lstlisting}


This includes \texttt{some\_stem.py} and \texttt{some\_stem.out} in that order.



\end{enumerate}

\subsection*{Figures}

\begin{enumerate}

\item 

Put the image file in the same directory as the chapter or appendix
    and use this to include it:

\begin{lstlisting}[frame=single,frameround=tttt]
[% figure
   slug="topic-someword"
   img="filename.svg"
   caption="Short sentence-case caption."
   alt="Long text describing the figure for the benefit of visually impaired readers."
%]
\end{lstlisting}



\item 

To refer to a figure write:

\begin{lstlisting}[frame=single,frameround=tttt]
[% f topic-someword %]
\end{lstlisting}


This is converted to \texttt{Figure N.K}.



\item 

Use \hreffoot{diagrams.net}{https://www.diagrams.net/} to create SVG diagrams
    using the "sketch" style and a 12-point Comic Sans font.



\item 

Avoid screenshots if at all possible:
    making them display correctly in print is difficult.



\end{enumerate}

\subsection*{Tables}


The Markdown processor used by \hreffoot{Ivy}{https://www.dmulholl.com/docs/ivy/main/} doesn't support attributes on tables,
so we must do something a bit clumsy.

\begin{enumerate}

\item 

To create a table write:

\begin{lstlisting}[frame=single,frameround=tttt]
<div class="table" id="topic-someword" caption="Short sentence-case caption." markdown="1">
| Left | Middle | Right |
| ---- | ------ | ----- |
| blue | orange | green |
| mars | saturn | venus |
</div>
\end{lstlisting}



\item 

To refer to a table write:

\begin{lstlisting}[frame=single,frameround=tttt]
[% t topic-someword %]
\end{lstlisting}


This is converted to \texttt{Table N.K}.



\end{enumerate}

\subsection*{Bibliography}

\begin{enumerate}

\item 

The BibTeX bibliography lives in \texttt{./info/bibliography.bib}.
    Please add entries as needed;
    you may find \hreffoot{https://doi2bib.org}{https://doi2bib.org} useful for creating entries.
    Please format keys as \texttt{Author1234},
    where \texttt{Author} is the first author's family name
    and \texttt{1234} is the year of publication.
    (Use \texttt{Author1234a}, \texttt{Author1234b}, etc. to resolve conflicts.)



\item 

To cite bibliography entries write:

\begin{lstlisting}[frame=single,frameround=tttt]
[% b key1 key2 key3 %]
\end{lstlisting}



\end{enumerate}

\subsection*{Glossary}


Please do not worry about adding entries for now;
we will fill in the glossary during the final editing pass.

\begin{enumerate}

\item 

The glossary lives in \texttt{./info/glossary.yml} and uses \hreffoot{Glosario}{https://github.com/carpentries/glosario} format.



\item 

To cite glossary entries write:

\begin{lstlisting}[frame=single,frameround=tttt]
[% g some_key "text for document" %]
\end{lstlisting}



\end{enumerate}

\subsection*{Index}


Please do not worry about adding entries for now;
we will fill in the index during the final editing pass.

\begin{enumerate}

\item 

To create a simple index entry write:

\begin{lstlisting}[frame=single,frameround=tttt]
[% i "index text" %]body text[% /i %]
\end{lstlisting}



\item 

Separate multiple index entries with semi-colons:

\begin{lstlisting}[frame=single,frameround=tttt]
[% i "first; second; third" %]body text[% /i %]
\end{lstlisting}



\item 

Create sub-entries using \texttt{!}:

\begin{lstlisting}[frame=single,frameround=tttt]
[% i "major!minor" %]body text[% /i %]
\end{lstlisting}



\end{enumerate}

\subsection*{Minor Formatting}

\begin{enumerate}

\item 

To continue a paragraph after a code sample write:

\begin{lstlisting}[frame=single,frameround=tttt]
text of paragraph
which can span multiple lines
{: .continue}
\end{lstlisting}


This has no effect on the appearance of the HTML,
but prevents an unwanted paragraph indent in the PDF version.



\item 

To create a callout box, use:

\begin{lstlisting}[frame=single,frameround=tttt]
<div class="callout" markdown="1">

### Title of callout

text of callout

</div>
\end{lstlisting}


Use "Sentence case" for the callout's title,
and please put blank lines before and after the opening and closing \texttt{<div>} markers.
You \emph{must} include \texttt{markdown="1"} in the opening \texttt{<div>} tag
to ensure that Markdown inside the callout is processed.


Note: earlier versions of this template used \texttt{blockquote} rather than \texttt{div} callouts.
Please only use the former for actual quotations.



\end{enumerate}

\section*{Building the PDF}


We use LaTeX to build the PDF version of this book.
Please don't worry about this for now,
but if you want to see what the print version will look like
you will need to install this packages:

\begin{itemize}

\item \texttt{babel-english}

\item \texttt{babel-greek}

\item \texttt{cbfonts}

\item \texttt{enumitem}

\item \texttt{greek-fontenc}

\item \texttt{keystroke}

\item \texttt{listings}

\item \texttt{textgreek}

\item \texttt{tocbibind}

\end{itemize}

\chapter{Appendix F: Glossary}\label{glossary}



\noindent \textbf{\glosskey{absolute error}}: 
The absolute value of the difference between the observed and the correct value. Absolute error is usually less useful than \glosskey{relative error}.


\noindent \textbf{\glosskey{abstract class}}: 
A class that defines or requires methods it does not implement. An abstract class typically specifies the methods that \glosskey{child classes} must have without providing default implementations.


\noindent \textbf{\glosskey{abstract method}}: 
In \glosskey{object-oriented programming}, a \glosskey{method} that is defined but not implemented. Programmers will define an abstract method in a \glosskey{parent class} to specify operations that \glosskey{child classes} must provide.


\noindent \textbf{\glosskey{actual result (of test)}}: 
The value generated by running code in a test. If this matches the \glosskey{expected result}, the test \glosskey{passes}; if the two are different, the test \glosskey{fails}.


\noindent \textbf{\glosskey{alias}}: 
A second or subsequent reference to the same object. Aliases are useful, but increase the cognitive load on readers who have to remember that all these names refer to the same thing.


\noindent \textbf{\glosskey{ANSI character encoding}}: 
An extension of \glosskey{ASCII} that standardized the characters represented by the codes 128 to 255.


\noindent \textbf{\glosskey{Application Binary Interface} (ABI)}: 
The low-level layout that a piece of software must have to work on a particular kind of machine.


\noindent \textbf{\glosskey{Application Programming Interface} (API)}: 
A set of functions provided by a software library or web service that other software can call.


\noindent \textbf{\glosskey{Application Programming Interface} (API)}: 
A set of functions provided by a software library or web service that other software can call.


\noindent \textbf{\glosskey{argument}}: 
A value passed into a function or method call.


\noindent \textbf{\glosskey{ASCII character encoding}}: 
A standard way to represent the characters commonly used in the Western European languages as 7-bit integers, now largely superceded by \glosskey{Unicode}.


\noindent \textbf{\glosskey{assembler}}: 
A \glosskey{compiler} that translates software written in \glosskey{assembly code} into machine instructions.


\noindent \textbf{\glosskey{assembly code}}: 
A low-level programming language whose statements correspond closely to the actual \glosskey{instruction set} of a particular kind of processor.


\noindent \textbf{\glosskey{assertion}}: 
A \glosskey{Boolean} expression that must be true at a certain point in a program. Assertions may be built into the language (e.g., Python's \texttt{assert} statement) or provided as functions (as with Node's \texttt{assert} library).


\noindent \textbf{\glosskey{associative array}}: 
See \glosskey{dictionary}.


\noindent \textbf{\glosskey{atomic value}}: 
A value that cannot be broken down into smaller parts, such as a Boolean or integer.


\noindent \textbf{\glosskey{attribute}}: 
A name-value pair associated with an object, used to store metadata about the object such as an array's dimensions.


\noindent \textbf{\glosskey{automatic variable}}: 
A variable that is automatically given a value in a \glosskey{build rule}. For example, Make automatically assigns the name of a rule's \glosskey{target} to the automatic variable \texttt{\$@}. Automatic variables are frequently used when writing \glosskey{pattern rules}.


\noindent \textbf{\glosskey{backward-compatible}}: 
A property of a system that enables interoperability with an older legacy system, or with input designed for such a system.


\noindent \textbf{\glosskey{base class}}: 
In \glosskey{object-oriented programming}, a \glosskey{class} from which other classes are derived.


\noindent \textbf{\glosskey{benchmark}}: 
A program or set of programs used to measure the performance of a computer system.


\noindent \textbf{\glosskey{big endian}}: 
A storage scheme in which the most significant part of a number is stored in the byte with the lowest address. For example, the 16-bit big-endian representation of 258 stores 0x01 in the lower byte and 0x02 in the higher byte.


\noindent \textbf{\glosskey{binary mode}}: 
An option for reading or writing files in which each byte is transferred literally. The term is used in contrast with \glosskey{text mode}.


\noindent \textbf{\glosskey{bit mask}}: 
A pattern of bits used to set or clear bits in a byte or \glosskey{word} in memory.


\noindent \textbf{\glosskey{bit shifting}}: 
To move the bits in a byte or \glosskey{word} left or right.


\noindent \textbf{\glosskey{bitwise operation}}: 
An operation that manipulates individual bits in memory. Common bitwise operations include \texttt{and}, \texttt{or}, \texttt{not}, and \texttt{xor}.


\noindent \textbf{\glosskey{Boolean}}: 
Relating to a variable or data type that can have either a logical value of true or false. Named for George Boole, a 19th century mathematician.


\noindent \textbf{\glosskey{build manager}}: 
A program that keeps track of how files depend on one another and runs commands to update any files that are out-of-date. Build managers were invented to \glosskey{compile} only those parts of programs that had changed, but are now often used to implement workflows in which plots depend on results files, which in turn depend on raw data files or configuration files.


\noindent \textbf{\glosskey{build recipe}}: 
The part of a \glosskey{build rule} that describes how to update something that has fallen out-of-date.


\noindent \textbf{\glosskey{build rule}}: 
A specification for a \glosskey{build manager} that describes how some files depend on others and what to do if those files are out-of-date.


\noindent \textbf{\glosskey{byte code}}: 
A set of instructions designed to be executed efficiently by an \glosskey{interpreter}.


\noindent \textbf{\glosskey{cache}}: 
Something that stores copies of data so that future requests for it can be satisfied more quickly. The CPU in a computer uses a hardware cache to hold recently-accessed values; many programs rely on a software cache to reduce network traffic and latency. Figuring out when something in a cache is out-of-date and should be replaced is one of the \glosskey{two hard problems in computer science}.


\noindent \textbf{\glosskey{call stack}}: 
A data structure that stores information about the active subroutines executed.


\noindent \textbf{\glosskey{catch (an exception)}}: 
To handle an error or other unexpected event represented by an \glosskey{exception}.


\noindent \textbf{\glosskey{Chain of Responsibility pattern}}: 
A \glosskey{design pattern} in which each \glosskey{object} either handles a request or passes it on to another object.


\noindent \textbf{\glosskey{child (in a tree)}}: 
A \glosskey{node} in a \glosskey{tree} that is below another node (call the \glosskey{parent}).


\noindent \textbf{\glosskey{child class}}: 
In \glosskey{object-oriented programming}, a \glosskey{class} derived from another class (called the \glosskey{parent class}).


\noindent \textbf{\glosskey{class}}: 
In \glosskey{object-oriented programming}, a structure that combines data and operations (called \glosskey{methods}). The program then uses a \glosskey{constructor} to create an \glosskey{object} with those properties and methods. Programmers generally put generic or reusable behavior in \glosskey{parent classes}, and more detailed or specific behavior in \glosskey{child classes}.


\noindent \textbf{\glosskey{code point}}: 
A number that uniquely identifies a character in the \glosskey{Unicode} standard.


\noindent \textbf{\glosskey{collision (in hashing)}}: 
A situation in which two or more values have the same \glosskey{hash code}.


\noindent \textbf{\glosskey{column-wise storage}}: 
To organize the memory of a two-dimensional table so that the values in each column are laid out in contiguous blocks.


\noindent \textbf{\glosskey{comma-separated values} (CSV)}: 
A text format for tabular data in which each \glosskey{record} is one row and \glosskey{fields} are separated by commas. There are many minor variations, particularly around quoting of \glosskey{strings}.


\noindent \textbf{\glosskey{comma-separated values} (CSV)}: 
A text format for tabular data in which each \glosskey{record} is one row and \glosskey{fields} are separated by commas. There are many minor variations, particularly around quoting of \glosskey{strings}.


\noindent \textbf{\glosskey{compile}}: 
To translate textual source into another form. Programs in \glosskey{compiled languages} are translated into machine instructions for a computer to run, and \glosskey{Markdown} is usually translated into \glosskey{HTML} for display.


\noindent \textbf{\glosskey{compiled language}}: 
Originally, a language such as C or Fortran that is translated into machine instructions for execution. Languages such as Java are also compiled before execution, but into \glosskey{byte code} instead of machine instructions, while \glosskey{interpreted languages} like JavaScript are compiled to byte code on the fly.


\noindent \textbf{\glosskey{compiler}}: 
An application that translates programs written in some languages into machine instructions or \glosskey{byte code}.


\noindent \textbf{\glosskey{concrete class}}: 
A class that can actually be instantiated. The term is used in contrast with \glosskey{abstract class}.


\noindent \textbf{\glosskey{confirmation bias}}: 
The tendency for someone to look for evidence that they are right rather than searching for reasons why they might be wrong.


\noindent \textbf{\glosskey{constructor}}: 
A function that creates an \glosskey{object} of a particular \glosskey{class}.


\noindent \textbf{\glosskey{context manager}}: 
An object that automatically executes some operations at the start of a code block and some other operations at the end of the block.


\noindent \textbf{\glosskey{control code}}: 
Originally a "character" that made a teletype perform some operation, such as moving to the next line or ringing the bell. Only a handful of control codes such as tab and newline are still in common use.


\noindent \textbf{\glosskey{control flow}}: 
The order in which a program executes statements and expressions.


\noindent \textbf{\glosskey{Coordinated Universal Time} (UTC)}: 
The standard time against which all others are defined. UTC is the time at longitude 0°, and is not adjusted for daylight savings. \glosskey{Timestamps} are often reported in UTC so that they will be the same no matter what timezone the computer is in.


\noindent \textbf{\glosskey{cryptographic hash function}}: 
A \glosskey{hash function} that produces an apparently-random value for any input.


\noindent \textbf{\glosskey{data migration}}: 
The act of moving data from one system or format to another.


\noindent \textbf{\glosskey{dataframe}}: 
A two-dimensional data structure for storing tabular data in memory. Rows represent \glosskey{records} and columns represent \glosskey{fields}.


\noindent \textbf{\glosskey{decorator}}: 
A function A that can be applied to another function B when function B is being defined to change its behavior in some way.


\noindent \textbf{\glosskey{defensive programming}}: 
A set of programming practices that assumes mistakes will happen and either reports or corrects them, such as inserting \glosskey{assertions} to report situations that are not ever supposed to occur.


\noindent \textbf{\glosskey{dependency (in build)}}: 
Something that a \glosskey{build target} depends on.


\noindent \textbf{\glosskey{derived class}}: 
In \glosskey{object-oriented programming}, a class that is a direct or indirect extension of a \glosskey{base class}.


\noindent \textbf{\glosskey{design by contract}}: 
A style of designing software in which functions specify the \glosskey{pre-conditions} that must be true in order for them to run and the \glosskey{post-conditions} they guarantee will be true when they return. A function can then be replaced by one with weaker pre-conditions (i.e., it accepts a wider set of input) and/or stronger post-conditions (i.e., it produces a smaller range of output) without breaking anything else.


\noindent \textbf{\glosskey{design pattern}}: 
A recurring pattern in software design that is specific enough to be worth naming, but not so specific that a single best implementation can be provided by a \glosskey{library}.


\noindent \textbf{\glosskey{dictionary}}: 
A data structure that allows items to be looked up by value, sometimes called an \glosskey{associative array}. Dictionaries are often implemented using \glosskey{hash tables}.


\noindent \textbf{\glosskey{dictionary comprehension}}: 
A single expression that constructs a dictionary by looping over key-value pairs.


\noindent \textbf{\glosskey{directed acyclic graph} (DAG)}: 
A \glosskey{directed graph} which does not contain any loops (i.e., it is not possible to reach a \glosskey{node} from itself by following edges).


\noindent \textbf{\glosskey{directed graph}}: 
A \glosskey{graph} whose \glosskey{edges} have directions.


\noindent \textbf{\glosskey{disassembler}}: 
A program that translates machine instructions into \glosskey{assembly code} or some other higher-level language.


\noindent \textbf{\glosskey{dispatch}}: 
To decide what operation to perform next based on the type of an object or its properties.


\noindent \textbf{\glosskey{docstring}}: 
A string at the start of a module, class, or function in Python that is not assigned to a variable, which is used to hold the documentation for that part of code.


\noindent \textbf{\glosskey{Document Object Model} (DOM)}: 
A standard, in-memory representation of \glosskey{HTML} and \glosskey{XML}. Each \glosskey{element} is stored as a \glosskey{node} in a \glosskey{tree} with a set of named \glosskey{attributes}; contained elements are \glosskey{child nodes}.


\noindent \textbf{\glosskey{dynamic dispatch}}: 
To find a function or a property of an \glosskey{object} by name while a program is running. For example, instead of getting a specific property of an object using \texttt{obj.name}, a program might use \texttt{obj[someVariable]}, where \texttt{someVariable} could hold \texttt{"name"} or some other property name.


\noindent \textbf{\glosskey{dynamic scoping}}: 
To find the value of a variable by looking at what is on the \glosskey{call stack} at the moment the lookup is done. Almost all programming languages use \glosskey{lexical-scoping} instead, since it is more predictable.


\noindent \textbf{\glosskey{eager matching}}: 
Matching as much as possible, as early as possible.


\noindent \textbf{\glosskey{easy mode}}: 
A term borrowed from gaming meaning to do something with obstacles or difficulties simplified or removed, often for practice purposes.


\noindent \textbf{\glosskey{edge}}: 
A connection between two \glosskey{nodes} in a \glosskey{graph}. An edge may have data associated with it, such as a name or distance.


\noindent \textbf{\glosskey{element}}: 
A named component in an \glosskey{HTML} or \glosskey{XML} document. Elements are usually written \texttt{<name>}...\texttt{</name>}, where "..." represents the content of the element. Elements often have \glosskey{attributes}.


\noindent \textbf{\glosskey{environment}}: 
The set of variables currently defined in a program.


\noindent \textbf{\glosskey{error (result of test)}}: 
Signalled when something goes wrong in a \glosskey{unit test} itself rather than in the system being tested. In this case, we do not know anything about the correctness of the system.


\noindent \textbf{\glosskey{error handling}}: 
What a program does to detect and correct for errors. Examples include printing a message and using a default configuration if the user-specified configuration cannot be found.


\noindent \textbf{\glosskey{escape sequence}}: 
A series of two or more characters used to represent a character that otherwise couldn't be represented. For example, the escape sequence \texttt{{\textbackslash}"} is used to represent a single \texttt{"} character inside a double-quoted string.


\noindent \textbf{\glosskey{exception}}: 
An object that stores information about an error or other unusual event in a program. One part of a program will create and \glosskey{raise an exception} to signal that something unexpected has happened; another part will \glosskey{catch} it.


\noindent \textbf{\glosskey{exception handler}}: 
A piece of code that deals with an \glosskey{exception} after it is \glosskey{caught}, e.g., by recording a message, retrying the operation that failed, or performing an alternate operation.


\noindent \textbf{\glosskey{exclusive or}}: 
A logical (or bitwise) operator that is true (or 1) if its arguments have different values and false (or 0) if they are the same. Exclusive or implements "either/or" or "one or the other".


\noindent \textbf{\glosskey{expected result (of test)}}: 
The value that a piece of software is supposed to produce when tested in a certain way, or the state in which it is supposed to leave the system.


\noindent \textbf{\glosskey{extensibility}}: 
How easily new features can be added to a program or existing features can be changed.


\noindent \textbf{\glosskey{failure (result of test)}}: 
A test fails if the \glosskey{actual result} does not match the \glosskey{expected result}.


\noindent \textbf{\glosskey{field}}: 
A component of a \glosskey{record} containing a single value. Every record in a database \glosskey{table} has the same fields.


\noindent \textbf{\glosskey{finite state machine} (FSM)}: 
A theoretical model of computing consisting of a \glosskey{directed graph} whose nodes represent the states of the computation and whose arcs show how to move from one state to another. Every \glosskey{regular expression} corresponds to a finite state machine.


\noindent \textbf{\glosskey{fixture}}: 
The thing on which a test is run, such as the \glosskey{parameters} to the function being tested or the file being processed.


\noindent \textbf{\glosskey{generator function}}: 
A function whose state is automatically saved when it returns a value so that execution can be restarted from that point the next time it is called. One example of generator functions use is to produce streams of values that can be processed by \texttt{for} loops.


\noindent \textbf{\glosskey{generic function}}: 
A collection of functions with similar purpose, each operating on a different class of data.


\noindent \textbf{\glosskey{graph (data structure)}}: 
A data structure in which \glosskey{nodes} are connected to one another by \glosskey{edges}.


\noindent \textbf{\glosskey{greedy algorithm}}: 
An algorithm that consumes as much input as possible, as early as possible.


\noindent \textbf{\glosskey{hash code}}: 
A value generated by a \glosskey{hash function}. Good hash codes have the same properties as random numbers in order to reduce the frequency of \glosskey{collisions}.


\noindent \textbf{\glosskey{hash function}}: 
A function that turns arbitrary data into a bit array, or a \glosskey{key}, of a fixed size. Hash functions are used to determine where data should be stored in a \glosskey{hash table}.


\noindent \textbf{\glosskey{hash table}}: 
A data structure that calculates a pseudo-random key (location) for each value passed to it and stores the value in that location. Hash tables enable fast lookup for arbitrary data. This occurs at the cost of extra memory because hash tables must always be larger than the amount of information they need to store, to avoid the possibility of data collisions, when the hash function returns the same key for two different values.


\noindent \textbf{\glosskey{heterogeneous}}: 
Containing mixed data types. For example, an array in Javascript can contain a mix of numbers, character strings, and values of other types.


\noindent \textbf{\glosskey{hexadecimal}}: 
A base-16 numerical representation that uses the letters A-F (or a-f) to represent the values from 10 to 15.


\noindent \textbf{\glosskey{homogeneous}}: 
Containing a single data type. For example, a \glosskey{vector} must be homogeneous: its values must all be numeric, logical, etc.


\noindent \textbf{\glosskey{HyperText Markup Language} (HTML)}: 
The standard \glosskey{markup language} used for web pages. HTML is represented in memory using \glosskey{DOM} (Digital Object Model).


\noindent \textbf{\glosskey{immutable}}: 
Data that cannot be changed after being created. Immutable data is easier to think about, particularly if data structures are shared between several tasks, but may result in higher memory requirements.


\noindent \textbf{\glosskey{index (a database)}}: 
An auxiliary data structure in a database used to speed up search for some entries. An index increases memory and disk requirements but reduces search time.


\noindent \textbf{\glosskey{instance}}: 
An \glosskey{object} of a particular \glosskey{class}.


\noindent \textbf{\glosskey{instruction pointer}}: 
A special \glosskey{register} in a processor that stores the address of the next instruction to execute.


\noindent \textbf{\glosskey{instruction set}}: 
The basic operations that a particular processor can execute directly.


\noindent \textbf{\glosskey{interpreted language}}: 
A high-level language that is not executed directly by the computer, but instead is run by an \glosskey{interpreter} that translates program instructions into machine commands on the fly.


\noindent \textbf{\glosskey{interpreter}}: 
A program whose job it is to run programs written in a high-level \glosskey{interpreted language}. Interpreters can run interactively, but may also execute commands saved in a file.


\noindent \textbf{\glosskey{introspection}}: 
See \glosskey{reflection}.


\noindent \textbf{\glosskey{ISO date format}}: 
An international for formatting dates. While the full standard is complex, the most common form is \texttt{YYYY-MM-DD}, i.e., a four-digit year, a two-digit month, and a two-digit day, separated by hyphens.


\noindent \textbf{\glosskey{Iterator pattern}}: 
A \glosskey{design pattern} in which a temporary \glosskey{object} or \glosskey{generator function} produces each value from a collection in turn for processing. This pattern hides the differences between different kinds of data structures so that everything can be processed using loops.


\noindent \textbf{\glosskey{JavaScript Object Notation} (JSON)}: 
A way to represent data by combining basic values like numbers and character strings in \glosskey{lists} and \glosskey{key/value} structures. The acronym stands for "JavaScript Object Notation"; unlike better-defined standards like \glosskey{XML}, it is unencumbered by a syntax for comments or ways to define a \glosskey{schema}.


\noindent \textbf{\glosskey{join (tables)}}: 
An operation that combines two \glosskey{tables}, typically by matching \glosskey{keys} from one with keys from another.


\noindent \textbf{\glosskey{key}}: 
A \glosskey{field} or combination of fields whose value(s) uniquely identify a \glosskey{record} within a \glosskey{table} or dataset. Keys are often used to select specific records and in \glosskey{joins}.


\noindent \textbf{\glosskey{label (address in memory)}}: 
A human-readable name given to a particular location in memory when writing programs in \glosskey{assembly code}.


\noindent \textbf{\glosskey{layout engine}}: 
A piece of software that decides where to place text, images, and other elements on a page.


\noindent \textbf{\glosskey{lazy evaluation}}: 
Evaluating expressions only when absolutely necessary.


\noindent \textbf{\glosskey{lazy matching}}: 
Matching as little as possible while still finding a valid match.


\noindent \textbf{\glosskey{lexical scoping}}: 
To look up the value associated with a name according to the textual structure of a program. Most programming languages use lexical scoping instead of \glosskey{dynamic scoping} because the latter is less predictable.


\noindent \textbf{\glosskey{library}}: 
An installable collection of software, also often called a \glosskey{module} or \glosskey{package}.


\noindent \textbf{\glosskey{link (a program)}}: 
To combine separately \glosskey{compiled} modules into a single runnable program.


\noindent \textbf{\glosskey{Liskov Substitution Principle}}: 
A design rule stating that it should be possible to replace objects in a program with objects of derived classes without breaking the program. \glosskey{Design by contract} is intended to enforce this rule.


\noindent \textbf{\glosskey{list}}: 
A \glosskey{vector} that can contain values of many different (\glosskey{heterogeneous}) types.


\noindent \textbf{\glosskey{list comprehension}}: 
A single expression that constructs a list by looping over its items.


\noindent \textbf{\glosskey{literal}}: 
A representation of a fixed value in a program, such as the digits \texttt{123} for the number 123 or the characters \texttt{"abc"} for the string containing those three letters.


\noindent \textbf{\glosskey{little endian}}: 
A storage scheme in which the most significant part of a number is stored in the byte with the highest address. For example, the 16-bit big-endian representation of 258 stores 0x02 in the lower byte and 0x01 in the higher byte.


\noindent \textbf{\glosskey{Markdown}}: 
A \glosskey{markup language} with a simple syntax intended as a replacement for \glosskey{HTML}.


\noindent \textbf{\glosskey{markup language}}: 
A set of rules for annotating text to define its meaning or how it should be displayed. The markup is usually not displayed, but instead controls how the underlying text is interpreted or shown. \glosskey{Markdown} and \glosskey{HTML} are widely-used markup languages for web pages.


\noindent \textbf{\glosskey{method}}: 
An implementation of a \glosskey{generic function} that handles objects of a specific class.


\noindent \textbf{\glosskey{mixin}}: 
A class that is not meant to be instantiated itself, but which contains methods to be added to other classes (typically via \glosskey{multiple inheritance}).


\noindent \textbf{\glosskey{mock object}}: 
A simplified replacement for part of a program whose behavior is easy to control and predict. Mock objects are used in \glosskey{unit tests} to simulate databases, web services, and other complex systems.


\noindent \textbf{\glosskey{module}}: 
A reusable software \glosskey{package}, also often called a \glosskey{library}.


\noindent \textbf{\glosskey{multiple inheritance}}: 
In a programming language, the ability to derive a \glosskey{child class} from two or more \glosskey{parent classes}.


\noindent \textbf{\glosskey{node}}: 
An element of a \glosskey{graph} that is connected to other nodes by \glosskey{edges}. Nodes typically have data associated with them, such as names or weights.


\noindent \textbf{\glosskey{object}}: 
In \glosskey{object-oriented programming}, a structure that contains the data for a specific instance of a \glosskey{class}. The operations the object is capable of are defined by the class's \glosskey{methods}.


\noindent \textbf{\glosskey{object-oriented programming} (OOP)}: 
A style of programming in which functions and data are bound together in \glosskey{objects} that only interact with each other through well-defined interfaces.


\noindent \textbf{\glosskey{off-by-one error}}: 
A common error in programming in which the program refers to element \texttt{i} of a structure when it should refer to element \texttt{i-1} or \texttt{i+1}, or processes \texttt{N} elements when it should process \texttt{N-1} or \texttt{N+1}.


\noindent \textbf{\glosskey{op code}}: 
The numerical code for a particular instruction that a processor can execute.


\noindent \textbf{\glosskey{Open-Closed Principle}}: 
A design rule stating that software should be open for extension but closed for modification, i.e., it should be possible to extend functionality without having to rewrite existing code.


\noindent \textbf{\glosskey{overlay configuration}}: 
A technique for configuring programs in which several layers of configuration are used, each overriding settings in the ones before.


\noindent \textbf{\glosskey{package}}: 
A collection of code, data, and documentation that can be distributed and re-used. Also referred to in some languages as a \glosskey{library} or \glosskey{module}.


\noindent \textbf{\glosskey{parameter}}: 
The name that a function gives to one of the values passed to it when it is called.


\noindent \textbf{\glosskey{parent (in a tree)}}: 
A \glosskey{node} in a \glosskey{tree} that is above another node (called a \glosskey{child}). Every node in a tree except the \glosskey{root node} has a single parent.


\noindent \textbf{\glosskey{parent class}}: 
In \glosskey{object-oriented programming}, the \glosskey{class} from which a sub class (called the \glosskey{child class}) is derived.


\noindent \textbf{\glosskey{parser}}: 
A function or program that reads data in a particular format and converts it to a data structure in memory. Every programming language has a parser that reads programs written in that language; parsers also exist for various data formats.


\noindent \textbf{\glosskey{pass (result of test)}}: 
A test passes if the \glosskey{actual result} matches the \glosskey{expected result}.


\noindent \textbf{\glosskey{pattern rule (in build)}}: 
A generic \glosskey{build rule} that describes how to update any file whose name matches a pattern. Pattern rules often use \glosskey{automatic variables} to represent the actual filenames.


\noindent \textbf{\glosskey{polymorphism}}: 
Having many different implementations of the same interface. If a set of functions or objects are polymorphic, they can be called interchangeably.


\noindent \textbf{\glosskey{post-condition}}: 
Something that is guaranteed to be true after a function runs successfully. Post-conditions are often expressed as \glosskey{assertions} that are guaranteed to be be true of a function's results.


\noindent \textbf{\glosskey{pre-condition}}: 
Something that must be true before a function runs in order for it to work correctly. Pre-conditions are often expressed as as \glosskey{assertions} that must be true of a function's inputs in order for it to run successfully.


\noindent \textbf{\glosskey{precedence}}: 
The priority of an operation. For example, multiplication has a higher precedence than addition, so \texttt{a+b*c} is read as "the sum of \texttt{a} with the product of \texttt{b} and \texttt{c}".


\noindent \textbf{\glosskey{prerequisite}}: 
Something that a \glosskey{build target} depends on.


\noindent \textbf{\glosskey{provenance}}: 
The sequence of steps that led to a particular result.


\noindent \textbf{\glosskey{race condition}}: 
A situation in which a result depends on the order in which two or more concurrent operations are carried out.


\noindent \textbf{\glosskey{raise (an exception)}}: 
To signal that something unexpected or unusual has happened in a program by creating an \glosskey{exception} and handing it to the \glosskey{error-handling} system, which then tries to find a point in the program that will \glosskey{catch} it.


\noindent \textbf{\glosskey{record}}: 
A group of related values that are stored together. A record may be represented as a \glosskey{tuple} or as a row in a \glosskey{table}; in the latter case, every record in the table has the same \glosskey{fields}.


\noindent \textbf{\glosskey{reflection}}: 
To inspect the properties of a running program in a generic way. Reflection relies on the fact that a program is just another data structure.


\noindent \textbf{\glosskey{register}}: 
A small piece of memory (typically one \glosskey{word} long) built into a processor that operations can refer to directly.


\noindent \textbf{\glosskey{regular expression}}: 
A pattern for matching text, written as text itself. Regular expressions are sometimes called "regexp", "regex", or "RE", and are powerful tools for working with text.


\noindent \textbf{\glosskey{relational database}}: 
A database that organizes information into \glosskey{tables}, each of which has a fixed set of named \glosskey{fields} (shown as columns) and a variable number of \glosskey{records} (shown as rows).


\noindent \textbf{\glosskey{relative error}}: 
The absolute value of the difference between the actual and correct value divided by the correct value. For example, if the actual value is 9 and the correct value is 10, the relative error is 0.1. Relative error is usually more useful than \glosskey{absolute error}.


\noindent \textbf{\glosskey{root (in a tree)}}: 
The \glosskey{node} in a \glosskey{tree} of which all other nodes are direct or indirect \glosskey{children}, or equivalently the only node in the tree that has no \glosskey{parent}.


\noindent \textbf{\glosskey{row-wise storage}}: 
To organize the memory of a two-dimensional table so that the values in each row are laid out in contiguous blocks.


\noindent \textbf{\glosskey{runtime}}: 
A program that implements the basic operations used in a programming language.


\noindent \textbf{\glosskey{schema}}: 
A specification of the format of a dataset, including the name, format, and content of each \glosskey{table}.


\noindent \textbf{\glosskey{SHA-256 hash code}}: 
A \glosskey{cryptographic hash function} that produces a 256-bit output.


\noindent \textbf{\glosskey{sign and magnitude}}: 
A binary representation of integers in which one bit indicates whether the value is positive or negative and the remaining bits indicate its magnitude.


\noindent \textbf{\glosskey{signature}}: 
The ordered list of parameters and return values that specifies how a function must be called and what it returns.


\noindent \textbf{\glosskey{singleton}}: 
A set with only one element, or a \glosskey{class} with only one \glosskey{instance}.


\noindent \textbf{\glosskey{Singleton pattern}}: 
A \glosskey{design pattern} that creates a \glosskey{singleton} \glosskey{object} to manage some resource or service, such as a database or \glosskey{cache}. In \glosskey{object-oriented programming}, the pattern is usually implemented by hiding the \glosskey{constructor} of the \glosskey{class} in some way so that it can only be called once.


\noindent \textbf{\glosskey{SQL}}: 
The language used for writing queries for a \glosskey{relational database}. The term was originally an acronym for Structured Query Language.


\noindent \textbf{\glosskey{stack frame}}: 
A section of the \glosskey{call stack} that records details of a single call to a specific function.


\noindent \textbf{\glosskey{stale (in build)}}: 
To be out-of-date compared to a \glosskey{prerequisite}. A \glosskey{build manager}'s job is to find and update things that are stale.


\noindent \textbf{\glosskey{static site generator}}: 
A software tool that creates HTML pages from templates and content.


\noindent \textbf{\glosskey{streaming API}}: 
An \glosskey{API} that processes data in chunks rather than needing to have all of it in memory at once. Streaming APIs usually require handlers for events such as "start of data", "next block", and "end of data".


\noindent \textbf{\glosskey{string}}: 
A block of text in a program. The term is short for "character string".


\noindent \textbf{\glosskey{table}}: 
A set of \glosskey{records} in a \glosskey{relational database} or \glosskey{dataframe}.


\noindent \textbf{\glosskey{target (in build)}}: 
The file(s) that a \glosskey{build rule} will update if they are out-of-date compared to their \glosskey{dependencies}.


\noindent \textbf{\glosskey{Template Method pattern}}: 
A \glosskey{design pattern} in which a \glosskey{parent class} defines an overall sequence of operations by calling \glosskey{abstract methods} that \glosskey{child classes} must then implement. Each child class then behaves in the same general way, but implements the steps differently.


\noindent \textbf{\glosskey{text mode}}: 
An option for reading or writing files in which bytes are translated to or from characters and end-of-line markers are normalized. The term is used in contrast with \glosskey{binary mode}.


\noindent \textbf{\glosskey{throw exception}}: 
Another term for \glosskey{raising} an exception.


\noindent \textbf{\glosskey{time of check - time of use}}: 
A \glosskey{race condition} in which a process checks the state of something and then operates on it, but some other process might alter that state between the check and the operation.


\noindent \textbf{\glosskey{timestamp}}: 
A digital identifier showing the time at which something was created or accessed. Timestamps should use \glosskey{ISO date format} for portability.


\noindent \textbf{\glosskey{token}}: 
An indivisible unit of text for a parser, such as a variable name or a number. Exactly what constitutes a token depends on the language.


\noindent \textbf{\glosskey{topological order}}: 
Any ordering of the \glosskey{nodes} in a \glosskey{graph} that respects the direction of its \glosskey{edges}, i.e., if there is an edge from node A to node B, A comes before B in the ordering. There may be many topological orderings of a particular graph.


\noindent \textbf{\glosskey{tree}}: 
A \glosskey{graph} in which every node except the \glosskey{root} has exactly one \glosskey{parent}.


\noindent \textbf{\glosskey{tuple}}: 
A value that has a fixed number of parts, such as the three color components of a red-green-blue color specification.


\noindent \textbf{\glosskey{Turing Machine}}: 
A theoretical model of computation that manipulates symbols on an infinite tape according to a fixed table of rules. Any computation that can be expressed as an algorithm can be done by a Turing Machine.


\noindent \textbf{\glosskey{two hard problems in computer science}}: 
Refers to a quote by Phil Karlton: "There are only two hard problems in computer science—cache invalidation and naming things." Many variations add a third problem as a joke, such as \glosskey{off-by-one errors}.


\noindent \textbf{\glosskey{two's complement}}: 
A binary representation of integers that "rolls over" like an odometer to represent negative values.


\noindent \textbf{\glosskey{Unicode}}: 
A standard that defines numeric codes for many thousands of characters and symbols. Unicode does not define how those numbers are stored; that is done by standards like \glosskey{UTF-8}.


\noindent \textbf{\glosskey{unit test}}: 
A test that exercises one function or feature of a piece of software and produces \glosskey{pass}, \glosskey{fail}, or \glosskey{error}.


\noindent \textbf{\glosskey{UTF-32}}: 
A way to store the numeric codes representing \glosskey{Unicode} characters in which every character is stored as a 32-bit integer.


\noindent \textbf{\glosskey{UTF-8}}: 
A way to store the numeric codes representing \glosskey{Unicode} characters that is \glosskey{backward-compatible} with the older \glosskey{ASCII} standard.


\noindent \textbf{\glosskey{variable-length encoding}}: 
Any technique for representing data in which a single logical unit of data may be represented by a variable number of bits or bytes.


\noindent \textbf{\glosskey{vector}}: 
A sequence of values, usually of \glosskey{homogeneous} type.


\noindent \textbf{\glosskey{version control system}}: 
A system for managing changes made to software during its development.


\noindent \textbf{\glosskey{virtual machine}}: 
A program that pretends to be a computer. This may seem a bit redundant, but VMs are quick to create and start up, and changes made inside the virtual machine are contained within that VM so we can install new \glosskey{packages} or run a completely different operating system without affecting the underlying computer.


\noindent \textbf{\glosskey{Visitor pattern}}: 
A \glosskey{design pattern} in which the operation to be done is taken to each element of a data structure in turn. It is usually implemented by having a generator "visitor" that knows how to reach the structure's elements, which is given a function or method to call for each in turn, and that carries out the specific operation.


\noindent \textbf{\glosskey{well-formed}}: 
A piece of text that obeys the rules of a formal grammar is said to be well formed.


\noindent \textbf{\glosskey{word (of memory)}}: 
The unit of memory that a particular processor most naturally works with. While a byte is a fixed size (8 bits), a word may be 16, 32, or 64 bits long depending on the processor.


\noindent \textbf{\glosskey{XML}}: 
A set of rules for defining \glosskey{HTML}-like tags and using them to format documents (typically data). XML was popular in the early 2000s, but its complexity led many programmers to adopt \glosskey{JSON}, instead.


\noindent \textbf{\glosskey{Yet Another Markup Language} (YAML)}: 
Short for "YAML Ain't Markup Language", a way to represent nested data using indentation rather than the parentheses and commas of \glosskey{JSON}. YAML is often used in configuration files and to define \glosskey{parameters} for various flavors of \glosskey{Markdown} documents.


\noindent \textbf{\glosskey{z-buffering}}: 
A drawing method that keeps track of the depth of what lies "under" each pixel so that it displays whatever is nearest to the observer.




\chapter{Appendix G: Links}\label{links}

\begin{itemize}

\item ANTLR: https://www.antlr.org/

\item The Architecture of Open Source Applications: https://aosabook.org/

\item The Birthday Problem on Wikipedia: https://en.wikipedia.org/wiki/Birthday\_problem

\item Black: https://black.readthedocs.io/

\item Bob Nystrom: http://journal.stuffwithstuff.com/

\item CC-BY-NC License: https://creativecommons.org/licenses/by-nc/4.0/

\item Chris Harrelson: https://twitter.com/chrishtr

\item A clever use of the `Fraction` package: https://www.textualize.io/blog/posts/7-things-about-terminals

\item Connor Stack: https://connorstack.com/

\item Crafting Interpreters: https://craftinginterpreters.com/

\item Django: https://www.djangoproject.com/

\item Embedded JavaScript Templating: https://ejs.co/

\item The ENIAC Programmers Project: http://eniacprogrammers.org/

\item Flake8: https://flake8.pycqa.org/

\item Git: https://git-scm.com/

\item Git man page generator: https://git-man-page-generator.lokaltog.net/

\item Gitlet: http://gitlet.maryrosecook.com/

\item Glosario: https://github.com/carpentries/glosario

\item GNU Make: https://www.gnu.org/software/make/

\item Hippocratic License: https://firstdonoharm.dev/

\item Human Resource Machine: https://tomorrowcorporation.com/humanresourcemachine

\item Isort: https://pycqa.github.io/isort/

\item Ivy: https://www.dmulholl.com/docs/ivy/main/

\item Jekyll: https://jekyllrb.com/

\item Julia Evans: https://jvns.ca/

\item kilo text editor: https://viewsourcecode.org/snaptoken/kilo/index.html

\item LaTeX: https://www.latex-project.org/

\item Learner personas: https://teachtogether.tech/en/index.html\#s:process-personas

\item Let's build a browser engine!: https://limpet.net/mbrubeck/2014/08/08/toy-layout-engine-1.html

\item Let's Build a Simple Database: https://cstack.github.io/db\_tutorial/

\item Let's Dev: A Package Manager: https://classic.yarnpkg.com/blog/2017/07/11/lets-dev-a-package-manager/

\item Mary Rose Cook: https://maryrosecook.com/

\item Matt Brubeck: https://limpet.net/mbrubeck/

\item Maël Nison: https://arcanis.github.io/

\item Mike Hoye: http://exple.tive.org/blarg/

\item networkx module: https://networkx.org/

\item Paige Ruten: https://viewsourcecode.org/

\item Pavel Panchekha: https://pavpanchekha.com/

\item PHP: https://www.php.net/

\item pip: https://pip.pypa.io/

\item Punching Holes: http://exple.tive.org/blarg/2020/11/26/punching-holes/

\item pyfakefs module: https://jmcgeheeiv.github.io/pyfakefs/

\item pytest approximate equality: https://docs.pytest.org/en/4.6.x/reference.html\#pytest-approx

\item pytest testing framework: https://docs.pytest.org/

\item Python: https://www.python.org/

\item Python array module: https://docs.python.org/3/library/array.html

\item Python fractions module: https://docs.python.org/3/library/fractions.html

\item Python glob module: https://docs.python.org/3/library/glob.html

\item Python hashlib module: https://docs.python.org/3/library/hashlib.html

\item Python io module: https://docs.python.org/3/library/io.html

\item Python JSON module: https://docs.python.org/3/library/json.html

\item Python pickle module: https://docs.python.org/3/library/pickle.html

\item Python profile module: https://docs.python.org/3/library/profile.html

\item Python textwrap module: https://docs.python.org/3/library/textwrap.html

\item requests package: https://requests.readthedocs.io/

\item Shunting-yard algorithm: https://en.wikipedia.org/wiki/Shunting-yard\_algorithm

\item Software Design in Python repository: https://github.com/gvwilson/sdpy

\item Software Design in Python website: https://gvwilson.github.io/sdpy

\item SVG Screenshot: https://chrome.google.com/webstore/detail/svg-screenshot/nfakpcpmhhilkdpphcjgnokknpbpdllg

\item WAVE: https://wave.webaim.org/

\item Web Browser Engineering: https://browser.engineering/

\item Wikipedia article on programming tools: https://en.wikipedia.org/wiki/Programming\_tool

\item Wizard Zines: https://wizardzines.com/

\item You can't parse [X]HTML with regex: https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454\#1732454

\end{itemize}

\chapter{Appendix H: Credits}\label{credits}


\textbf{Greg Wilson} has worked in industry and academia for 35 years, and is the author, co-author, or editor of several books, including \emph{Beautiful Code}, \emph{The Architecture of Open Source Applications}, \emph{JavaScript for Data Science}, \emph{Teaching Tech Together}, and \emph{Research Software Engineering with Python}. He was the co-founder and first Executive Director of Software Carpentry and received ACM SIGSOFT's Influential Educator Award in 2020.

\printindex

\end{document}

